[toc]

## 神经网络的学习

- 这里所说的“学习”是指从训练数据中自动获取最优权重参数的过程.
- 为了使神经网络能进行学习，导入**损失函数**这一指标。
- 而学习的目的就是以该损失函数为**基准**，找出能使它的值**达到最小的权重参数**。
- 为了找出尽可能小的损失函数的值，这里介绍函数斜率的梯度法。

- 神经网络的特征就是可以从数据中学习。所谓“从数据中学习”，是指可以由数据自动决定权重参数的值。这是非常了不起的事情!
  - 在简单感知机中,可以对照着真值表，人工设定了参数的值。而在实际的神经网络中，参数的数量成千上万，在层数更深的深度学习中，参数的数量甚至可以上亿，想要人工决定这些参数的值是不可能的。

### 数据驱动

- 数据是机器学习的核心。这种数据驱动的方法，也可以说脱离了过往以人为中心的方法
- 机器学习的方法则极力避免人为介入，尝试从收集到的数据中发现答案（模式）。
- 神经网络或深度学习则比以往的机器学习方法更能避免人为介入。

- 比如如何实现手写数字“5”图像的识别。如果让我们自己来设计一个能将 5正确分类的程序，就会意外地发现这
  是一个很难的问题。人可以简单地识别出 5，但却很难明确说出是基于何种规律而识别出了 5。

- 每个人都有不同的写字习惯，要发现其中的规律是一件非常难的工作。因此，与其绞尽脑汁，从零开始想出一个可以识别 5的算法，不如考虑通过有效利用数据来解决这个问题。

- 一种方案是，先从图像中提取特征量，再用机器学习技术学习这些特征量的模式。这里所说的“特征量”是指可以从输入数据（输入图像）中准确地提取本质数据（重要的数据）的转换器。图像的特征量通常表示为向量的形式。在计算机视觉领域，常用的特征量包括<u>SIFT、SURF和 HOG等</u>。使用这些特征量将图像数据转换为向量，然后对转换后的向量使用机器学习中的 <u>SVM、KNN等分类器进行学习</u>。

- 机器学习的方法中，由机器从收集到的数据中找出规律性。与从零开始想出算法相比，这种方法可以更高效地解决问题，也能减轻人的负担。但是需要注意的是，将图像转换为向量时使用的特征量仍是由人设计的。对于不同的问题，必须使用合适的特征量（必须设计专门的特征量），才能得到好的结果。

- 比如，为了区分狗的脸部，人们需要考虑与用于识别 5的特征量不同的其他特征量。<u>也就是说，即使使用特征量和机器学习的方法，也需要针对不同的问题人工考虑合适的特征量。</u>

- 两种针对机器学习任务的方法:

  - ```mermaid
    flowchart LR	
    	input-->a[人想到的算法] & b["人想到的特征量&lpar;SIFT、SURF和 HOG等&rpar;"] & c["神经网络&lpar;深度学习&rpar;"] 
    	b-->bb["机器学习&lpar;eg.SVM,KNN&rpar;"]
    	a & bb & c --> output(answer)
    	
    	
    ```

    

  - 图中还展示了神经网络（深度学习）的方法，可以看出该方法不存在人为介入。

  - 如果input是图像,则神经网络直接学习图像本身。

  - 而在第 2个方法，即利用特征量和机器学习的方法中，特征量仍是由人工设计的

  - 而在神经网络中，连图像中包含的重要特征量也都是由机器来学习的。

- 深度学习有时也称为端到端机器学习（end-to-end machine learning）。

  - 指从是从原始数据（输入）中获得目标结果（输出）的意思。

- 神经网络的优点是对所有的问题都可以用同样的流程来解决。

  - 比如，不管要求解的问题是识别 5，还是识别狗，抑或是识别人脸，神经网络都是通过不断地学习所提供的数据，尝试发现待求解的问题的模式。
  - 也就是说，<u>与待处理的问题无关</u>，神经网络可以将原始数据直接作为输入，进行“端对端”的学习。

## 训练数据和测试数据

- 机器学习中，一般将数据分为训练数据和测试数据两部分来进行学习和实验等。
  - 首先，使用训练数据进行学习，寻找最优的参数；
  - 然后，使用测试数据评价训练得到的模型的实际能力。
- 为什么需要将数据分为训练数据和测试数据呢？
  - 因为我们追求的是**模型的泛化能力**。
  - 为了正确评价模型的泛化能力，就必须划分训练数据和测试数据。
  - 另外，**训练数据**也可以称为**监督数据**。
  - **泛化能力**是指处理**未被观察过的数据**（不包含在训练数据中的数据）的能力。
  - 获得泛化能力是机器学习的**最终目标**。
    - 比如，在识别手写数字的问题中，泛化能力可能会被用在自动读取明信片的邮政编码的系统上。
    - 此时，手写数字识别就必须具备较高的识别而是“任意一个人写的**任意文字**”。
    - 如果系统只能正确识别已有的训练数据，那有可能是只学习到了训练数据中的<u>个人的习惯写法</u>。因此，仅仅用一个数据集去学习和评价参数，是无法进行正确评价的。这样会导致可以顺利地处理某个数据集，但无法处理其他数据集的情况。

### 过拟合

- 只对某个数据集<u>过度拟合</u>的状态称为**过拟合**（over fitting）。
- 避免过拟合也是机器学习的一个重要课题。

## 损失函数

- 神经网络的学习通过某个指标表示现在的状态。神经网络**以某个指标为线索**寻找**最优权重参数**。
- 神经网络的学习中所用的**指标**称为**损失函数**（loss function）。这个损失函数可以使用**任意函数**，<u>但一般用均方误差和交叉熵误差等。</u>
- 损失函数是表示神经网络性能的“恶劣程度”的指标，即描述(刻画)当前的神经网络对**监督数据**在多大程度上不拟合，在多大程度上不一致。
- 以“性能的恶劣程度”为指标可能会使人感到不太自然，但是如果给损失函数<u>乘上一个负值，就可以解释为“在多大程度上不坏”，即“性能有多好”。</u>
- 并且，“使性能的恶劣程度达到最小”和“使性能的优良程度达到最大”是等价的，不管是用“恶劣程度”还是“优良程度”，做的事情本质上都是一样的。

### 均方误差

- 可以用作损失函数的函数有很多，其中最有名的是均方误差（mean squared error）。

  - 均方误差如下式所示
    $$
    E=\frac{1}{2}\sum_{k=1}^{n}(y_k-t_k)^2
    $$

  - $y_k$是神经网络的输出

  - $t_k$表示监督数据(training set)

  - $n$表示数据的维数

- 均方误差会计算神经网络的输出和正确解监督数据的各个元素之差的平方，再求总和

  - ```python
    def mean_squared_error(y, t):
        return 0.5 * np.sum((y-t)**2)
    ```

    

- 假设MNIST手写数据集识别手写数字的输出向量包含10个元素

  - ```
    >>> y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
    >>> t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
    ```

  - 设神经网络的输出y是softmax函数的输出

  - softmax的输出可以理解为概率

    - 假设将某张图片x输入神经网络中
    - y中的各个分量分别表示x是(被识别为)数字0,1,...,9的概率大小
      - “0”的概率是 0.1，“1”的概率是 0.05，“2”的概率是 0.6,...等

  - t是监督数据,将正确的标签设为1,其他设为0(这种表示方法是one-hot表示法)

    - 本例t向量的t[2]元素为1,其余都为0,表示输入的图片x实际上是2.如果上述识别结果y[2]是向量y中的最大值,那么认为此次识别结论是正确的
    - 使用one-hot表示法有许多优点(某些场合下会考虑缺点)

### 交叉熵误差

- 除了均方误差之外，**交叉熵误差**（cross entropy error）也经常被用作损失函数。交叉熵误差如下式所示

  - $$
    E=-sum(t\cdot{\ln y})=-\sum_{k=1}^{n}t_k\ln{y_k}
    $$

  - $y$表示输入x被神经网络识别为n个类别对应的n个概率值构成的向量,$i=1,2,\cdots,n$)

    - 通常y包含的n个概率值最大的一个,(对应的类别)作为预测结果

  - $y_k$表示神经网络的输出向量y的第k个分量(也就是概率值)

  - $t$表示输入x的正确结果(标签,人工识别结果)<u>以one-hot形式表示的向量</u>,例如

    - $t$向量中只有正确解标签的对应的分量为1,其他为0(one-hot表示)

    - 输入的手写图片x人工识别为2,那么对于一个长度n=10的one-hot格式向量为$[0,0,1,0,\cdots,0]$

    - | 输入x的实际标签的one-hot向量形式                             | 0    | 0    | 1    | 0    | ...  | 0    |
      | ------------------------------------------------------------ | ---- | ---- | ---- | ---- | ---- | ---- |
      | 分量表示的类别(手写0~9的10个数字)                            | 0    | 1    | 2    | 3    | ...  | 9    |
      | 分量的含义(输入x是否属于第i分量表示的类别,"N"表示不属于该类,"T"表示属于该类) | N    | N    | T    | N    | ...  | N    |

  - $t_k$表示向量$t$的第k个分量

  - 基于one-hot表示法的这个特点,容易知道上述的交叉熵误差公式可以简化形式为

    - $$
      \\设向量t中第r个分量为1,其余分量为0,即t_r=1
      \\
      E=-\sum_{k=1}^{n}t_k\ln{y_k}=-t_r\ln{y_r}=-1\times{\ln{y_r}}=-\ln{y_r}
      $$

    - 换句话说,上述公式在实际运算的时候,相当于只计算了**正确标签**(类别$r$)对应的输出向量的分量$y_r$的自然对数(再取负)

      - $r$是从向量t读出的索引(向量t中的唯一个非零元素的索引作为x的正确类别)
      - $y_r$表明神经网络预测输入x属于正确类别的概率值($y_r$)越高,则说明对于x的预测做的越好,约有可能做出正确判断(如果向量y满足$y_r$=max(y),那么神经网络对于x的类别识别就是对的,反之则认为神经网络将x错误的识别为其他类别,比如r=2时,$y_2$不是最大的,且有$y_3=max(y)$,那么神经网络就将x识别为2以外的数字3)

#### 特点

- 函数$\pm\ln{x}$的在$x\in(0,1]$范围内的图像
  - $-\ln{x}$和$\ln{x}$关于x轴对称

| ![](https://img-blog.csdnimg.cn/070f25955d884a51b601bfec6abc7918.png) | ![在这里插入图片描述](https://img-blog.csdnimg.cn/bbfdb4121a614ce7a0441720431b38f4.png) |      |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ---- |
| $\ln{x}$                                                     | $-\ln{x}$                                                    |      |



- x等于1时，y为 0；
- 随着 x向 0靠近，y逐渐变小。
- 因此正确解标签对应的输出越大，E的值越接近 0；当输出为 1时，交叉熵误差为 0。
- 此外，如果正确解标签对应的输出较小，则E的值较大。



#### 关于numpy中的自然对数:

- [numpy.log — NumPy v1.24 Manual](https://numpy.org/doc/stable/reference/generated/numpy.log.html)
  - `np.log()`就是(默认)自然对数
    - The natural logarithm [`log`](https://numpy.org/doc/stable/reference/generated/numpy.log.html#numpy.log) is the inverse of the exponential function, so that $log(exp(x)) = x$. The natural logarithm is logarithm in base [`e`](https://numpy.org/doc/stable/reference/constants.html#numpy.e).
- [numpy.emath.log — NumPy v1.24 Manual](https://numpy.org/doc/stable/reference/generated/numpy.emath.log.html#numpy.emath.log)
  - 具有自动定义域的数学函数
  - Return the “principal value” (for a description of this, see [`numpy.log`](https://numpy.org/doc/stable/reference/generated/numpy.log.html#numpy.log)) of $\log_{e}{x}$ 
  - For real *x > 0*, this is a real number (`log(0)` returns `-inf` and `log(np.inf)` returns `inf`). 
  - Otherwise, the complex principle value is returned.

#### 实现交叉熵🎈

- `cross_entropy_error(y, t)`函数(numpy实现)

  ```python
  def cross_entropy_error(y, t):
      delta = 1e-7#微小值防止溢出
      return -np.sum(t * np.log(y + delta))
  ```

  - 这里，参数 y和 t是 NumPy数组。

  - 函数内部在计算 np.log时，加上了一个微小值 delta。这是因为，当出现 np.log(0)时，np.log(0)会变为负无限大的 -inf，这样一来就会导致后续计算无法进行。添加一个微小值可以**防止负无限大的发生**。

  - 使用cross_entropy_error(y, t)进行一些简单的计算:

    - ```python
      >>> t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
      >>> y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
      >>> cross_entropy_error(np.array(y), np.array(t))
      0.51082545709933802
      >>>
      >>> y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
      >>> cross_entropy_error(np.array(y), np.array(t))
      2.3025840929945458
      
      ```

    - 第一个例子中，正确解标签对应的输出为 0.6，此时的交叉熵误差大约为 0.51。第二个例子中，正确解标签对应的输出为 0.1的低值，此时的交叉熵误差大约为 2.3。由此可以看出，这些结果与我们前面讨论的内容是一致的。

### one-hot表示法的优缺点

- 机器学习主要考虑器优点,下面还列出one-hot在门电路领域的优缺点

##### Advantages

- Determining the state has a low and constant cost of accessing one [flip-flop](https://en.wikipedia.org/wiki/Flip-flop_(electronics))
- Changing the state has the constant cost of accessing two flip-flops
- Easy to design and modify
- Easy to detect illegal states
- Takes advantage of an [FPGA](https://en.wikipedia.org/wiki/Field-programmable_gate_array)'s abundant flip-flops
- Using a one-hot implementation typically allows a state machine to run at a faster clock rate than any other encoding of that state machine 

##### Disadvantages

- Requires more flip-flops than other encodings, making it impractical for [PAL](https://en.wikipedia.org/wiki/Programmable_Array_Logic) devices
- Many of the states are illegal 

## mini-batch 学习

- 机器学习使用训练数据进行学习。使用训练数据进行学习，严格来说，就是针对训练数据计算损失函数的值，找出使该值尽可能小的参数。

  - 因此，计算损失函数时必须将所有的训练数据作为对象。

  - $$
    E=-\frac{1}{N}\sum_{j}^{N}\sum_{k}^{n}t_{jk}\ln{y_{jk}}
    $$

    - <<deep-learning-from-scratch>>将公式写作

      - $$
        E=-\frac{1}{N}\sum_{n}\sum_{k}{t_{nk}\ln{y_{nk}}}
        $$

      - 这种写法比较简练,$\displaystyle{\sum_{n}\sum_{k}}$同时将n和k作为求和边界上界,并且在$t_{nk},\ln{y_{nk}}$中表示索引变量(index),是向量中的元素,用来遍历需要运算的元素
      
      - This may read as: sum of $t_{nk}\ln{y_{nk}}$,from $n=1$ to $n$ and k=1 to k

  - 也就是说，如果训练数据有 100个的话，我们就要把这 100个损失函数的总和作为学习的指标

  - 上式其实只是把求单个数据的损失函数扩大到了 N份数据，不过最后还要除以 N进行**正规化**。

    - 通过除以 N，可以求单个数据的**“平均损失函数**”。
    - 通过这样的平均化，可以获得<u>和训练数据的数量无关的统一指标</u>。
      - 比如，即便训练数据有 1000个或 10000个，也可以求得单个数据的平均损失函数。
      - 例如,MNIST数据集的训练数据有 60000个，如果以全部数据为对象求损失函数的和，则计算过程需要花费较长的时间。再者，如果遇到大数据，数据量会有几百万、几千万之多，这种情况下以全部数据为对象计算损失函数是不现实的。

  - 从全部数据中选出一部分，作为全部数据的“近似”。

  - 神经网络的学习也是从训练数据中选出一批数据（称为 mini-batch,小批量），然后<u>对每个 mini-batch进行学习。</u>

    - 比如，从 60000个训练数据中**随机选择** 100笔，再用这 100笔数据进行学习。<u>这种学习方式称为 mini-batch学习。</u>	

    - 使用 np.random.choice()可以从指定的数字中随机选择想要的数字。比如，np.random.choice(60000, 10)会从 0到 59999之间随机选择 10个数字。如下面的实际代码所示，我们可以得到一个包含被选数据的索引的数组。

      - ```python
        batch_mask = np.random.choice(train_size, batch_size)
        x_batch = x_train[batch_mask]
        t_batch = t_train[batch_mask]
        ```

### mini-batch交叉熵误差

- ```python
  def cross_entropy_error(y, t):
      #如果输入时一维向量shape=(size(..),)则将其转换为二维向量[[...]]的形式,即shape=(1,size(...))
      if y.ndim == 1:
          t = t.reshape(1, t.size)
          y = y.reshape(1, y.size)
  
      batch_size = y.shape[0]#矩阵y的第一维的形状(shape)值就是矩阵y的行数,对应y这个mini-batch包含多少个基础输入(一维的行向量)
      #矩阵t和y的规格时一样的,且要求以one_hot规则编码
      #cees=t * np.log(y + 1e-7)
      return -np.sum(t * np.log(y + 1e-7)) / batch_size
  ```

## 为何要设定损失函数

- 在神经网络的学习中，寻找最优参数（权重和偏置）时，要寻找使损失函数的值尽可能小的参数。
- 为了找到使损失函数的值尽可能小的地方，需要计算参数的导数（确切地讲是梯度），然后以这个导数为指引，逐步更新参数的值。
  - 假设有一个神经网络，现在我们来关注这个神经网络中的某一个权重参数。
  - 此时，对该权重参数的损失函数求导，表示的是“如果稍微改变这个权重参数的值，损失函数的值会如何变化”。
  - 如果导数的值为负，通过使该权重参数向正方向改变，可以减小损失函数的值；反过来，如果导数的值为正，则通过使该权重参数向负方向改变，可以减小损失函数的值。
- 不过，当导数的值为 0时，无论权重参数向哪个方向变化，损失函数的值都不会改变，此时该权重参数的更新会停在此处。
- 之所以不能用识别精度作为指标，是因为这样一来绝大多数地方的导数都会变为 0，导致<u>参数无法更新</u>。
  - 假设某个神经网络正确识别出了 100笔训练数据中的32笔，此时识别精度为32 %。
  - 如果以识别精度为指标，即使稍微改变权重参数的值，识别精度也仍将保持在 32 %，不会出现变化。
  - 也就是说，仅仅微调参数，是无法改善识别精度的。
    - 即便识别精度有所改善，它的值也不会像 32.0123 . . . %这样连续变化，而是变为 33 %、34 %这样的不连续的、离散的值。
  - 而如果把损失函数作为指标，则当前损失函数的值可以表示为 0.92543 . . . 这样的值。
    - 并且，如果稍微改变一下参数的值，对应的损失函数也会像 0.93432 . . . 这样发生连续性的变化。
  - 识别精度对<u>微小的参数变化</u>基本上没有什么反应，即便有反应，它的值也是不连续地、突然地变化。
  - 作为激活函数的阶跃函数也有同样的情况。
    - 出于相同的原因，如果使用阶跃函数作为激活函数，神经网络的学习将无法进行。
    - 阶跃函数的导数在绝大多数地方（除了 0以外的地方）均为 0。也就是说，如果使用了阶跃函数，那么即便将损失函数作为指标，<u>参数的微小变化也会被阶跃函数抹杀</u>，导致损失函数的值不会产生任何变化。
    - 阶跃函数只在某个瞬间产生变化。
    - 而 sigmoid函数不仅函数的输出（竖轴的值）是连续变化的，曲线的斜率（导数）也是连续变化的。
    - 也就是说，sigmoid函数的导数在任何地方都不为 0。这对神经网络的学习非常重要。得益于这个<u>斜率不会为 0的性质</u>，神经网络的学习得以正确进行。

## 数值微分(导数)

- **数值微分**就是用数值方法**近似求解函数的导数**的过程
  - 如上所示，利用<u>微小的差分</u>求导数的过程称为**数值微分**（numerical differentiation）。
  - 而基于<u>数学式的推导</u>求导数的过程，则用“**解析性**”（analytic）一词形容，称为“**解析性求解**”或者“**解析性求导**”。
  - 比如，$y = x^2$ 的导数，可以通过 解析性地求解出来($y'=2x)$。
    - 因此，当x = 2时，y的导数为4。
- **解析性求导**得到的导数是**不含误差的“真的导数”**。

- 梯度法使用梯度的信息决定前进的方向

- 导数的微商(极限)定义式:

  - $$
    f^{'}(x)=\frac{\mathrm d{f(x)}}{\mathrm d{x}}
    =\lim_{h\to{0}}\frac{f(x+h)-f(x)}{h}
    \\
    f^{'}(x)=\lim_{h\to{0}}\frac{f(x+h)-f(x-h)}{2h}
    $$

  - $f'(x)$表示了$f(x)$相对于$x$(附近)的变化程度

    - 第一种形式表示**前向差分**形式
    - 第二种形式式**中心差分**形式,有利于计算机估算产生的**误差**(比前向差分更好)

  - 导数的含义是，$x$的“微小变化”将导致函数 $f(x)$的值在多大程度上发生变化。其中，表示微小变化的h无限趋近 0

- ```python
  # 不好的实现示例
  def numerical_diff(f, x):
      h = 10e-50
  	return (f(x+h) - f(x)) / h
  #改进
  def numerical_diff(f, x):
      h = 1e-4 # 0.0001
      return (f(x+h) - f(x-h)) / (2*h)
  
  ```

- 函数 `numerical_diff(f, x)`的名称来源于数值微分 A 的英文 numerical differentiation。

  - 这个函数`numerical_diff`有两个参数，即“函数f”和“传给函数 f的参数 x”。

  - 在上面的实现中，因为想把尽可能小的值赋给 h（可以话，想让 h无限接近 0），所以 h使用了 `1e-50`(也就是$1\times{10^{-50}}$这个微小值。

  - 但是国小的h，导致产生**舍入误差**（rounding error）。

    - 所谓舍入误差，是指因<u>省略小数的精细部分的数值</u>（比如，小数点<u>第8位以后的数值</u>）而造成最终的计算结果上的误差。

    - 比如，在 Python中，舍入误差可如下表示。

      - ```python
        ##
        delta=1e-50
        a=float(delta)
        b=np.float32(delta)
        c=np.float64(delta)
        print(f'{a=}\n{b=}\n{c=}\n')
        
        ```

      - output

        - ```
          a=1e-50
          b=0.0
          c=1e-50
          
          ```

          

    - 如果用 float32类型（32位的浮点数）来表示 `1e-50`，就会变成0.0，无法正确表示出来。也就是说，使用过小的值会造成计算机出现计算上的问题。

    - 这是第一个需要改进的地方，即将微小值 h改为 `1e−4`。使用 `1e−4`就可以得到正确的结果。

    - 第二个需要改进的地方与函数 f的差分有关。虽然上述实现中计算了函数 f在 x+h和 x之间的差分，但是必须注意到，这个计算从一开始就有误差。

      - “真的导数”对应函数在 x处的斜率（称为切线），但上述实现中计算的导数对应的是 (x + h)和 x之间的斜率。
      - 因此，真的导数（真的切线）和上述实现中得到的导数的值在严格意义上并不一致。这个差异的出现是因为 h不可能无限接近 0。
      - 数值微分含有误差。为了减小这个误差，我们可以计算函数 f在 (x + h)和 (x −h)之间的差分。
        - 因为这种计算方法以 x为中心，计算它左右两边的差分，所以也称为**中心差分**（而 (x + h)和 x之间的差分称为**前向差分**）。

### Tips:使用numpy@sympy计算导数

- [python - How do I compute derivative using Numpy? - Stack Overflow](https://stackoverflow.com/questions/9876290/how-do-i-compute-derivative-using-numpy)
- [Installation - SymPy 1.11 documentation](https://docs.sympy.org/latest/install.html#installation)
  - conda方式安装:激活某个conda环境,然后执行`conda install sympy`即可

## 偏导数

- 和高数中的偏导数一样,计算的时候,将多元函数转化为一元函数的导数计算问题

- $$
  f(x_1,\cdots,x_n)
  \\
  对于变量x_i的偏导可以记为:\frac{\partial{f}}{\partial{x_i}},(i=1,2,\cdots,n)
  \\计算的时候x_j(j\neq{i})都视为常数
  $$

- 设二元函数$f(x_0,x_1)=x_0^2+x_1^2$,其中$x_0,x_1$表示两个变量

  - ```python
    
    def function_2(x):
        return x[0]**2 + x[1]**2
        # 或者 return np.sum(x**2)
    
    ```

### 偏导数的估算

- 参考[math_@多元函数求导@偏导数的定义和计算机估算@高阶偏导@混合偏导_xuchaoxin1375的博客-CSDN博客](https://blog.csdn.net/xuchaoxin1375/article/details/128380947)


### 偏导数的几何含义🎈

- 以二元函数(空间曲面)$z=f(x,y)$为例,描述某个点$M(x_0,y_0,z_0)$处的偏导数

  - $\frac{\partial{z}}{\part{x}}|_M$表示:曲面z和(垂直于y轴的)截面$y=y_0$的**交线**,该交线$l$在点M处的切线$T_x$对于x轴的斜率

    - $T_x,l\in{y=y_0}$(即切线$T_x$和交线$l$上的所有点都在截面$y=y_0$上)
    - 不妨将该交线正投影到平面$XOZ$,就转换为一元函数$y=g(x)$的导数(变换率)问题

  - $\frac{\partial{z}}{\part{y}}|_M$表示曲面z和截面$x=x_0$构成的交线$m$在点M处的切线$T_y$对于y轴的斜率

    - 类似的,可以将交线m投影到$YOZ$上

    

## 梯度（梯度向量）

- 计算多元函数的每个变量的偏导数$\frac{\partial{f}}{\partial{x_i}}$ 其中$(i=1,2,\cdots,n)$,他们构成的向量称为函数f的**梯度**(和高等数学中的含义一致)
  
  - 也就是说,对于一个n元函数$f$,向量$\nabla{f}$=$(\frac{\partial{f}}{\partial{x_1}},\frac{\partial{f}}{\partial{x_2}},\cdots,\frac{\partial{f}}{\partial{x_n}})$
  - 对于某个点P处的梯度,$\nabla{f}|_{P}$=$(\left.\frac{\partial{f}}{\partial{x_1}}\right|_P,\left.\frac{\partial{f}}{\partial{x_2}}\right|_P,\cdots,\left.\frac{\partial{f}}{\partial{x_n}}\right|_P)$
  - 即,全部变量的偏导数汇总而成的向量称为**梯度**（gradient）
  
- 梯度向量的方向会指向各点(例如某个点P)处的函数值降低的方向。

  - 更严格地讲，梯度指示的方向是<u>各点处的函数值减小最多的方向</u> 。这是一个非常重要的性质.

  - ```python
    
    import numpy as np
    # from gradient_1d import numerical_diff
    def function_2(x):
        return x[0]**2 + x[1]**2
        # 或者 return np.sum(x**2)
    
    def numerical_gradient(f, x):
        """计算给定多元函数f在点x的梯度向量
        f的变量数和向量x的维数一致
    
        Parameters
        ----------
        f : func
            多元函数
        x : array
            需要求梯度的位置(使用一个高维数组来表示各个分量的取值)
            可以将x理解为多维点坐标
        """ 
        h = 1e-4 # 0.0001微小值用于数值微分
        grad = np.zeros_like(x)
        #np.zeros_like(x)会生成一个形状和 x相同、所有元素都为0的数组。(该矩阵用来存储梯度的计算结果)
        for idx in range(x.size):
            tmp_val = x[idx]#为例防止左右差分的混淆,备份分量
            #采用中心差分法计算(偏)导数
            # f(x+h)的计算
            x[idx] = tmp_val + h
            fxh1 = f(x)
            # f(x-h)的计算
            x[idx] = tmp_val - h
            fxh2 = f(x)
            #计算梯度
            grad[idx] = (fxh1 - fxh2) / (2*h)
            x[idx] = tmp_val # 还原值
        return grad
    
    a=numerical_gradient(function_2, np.array([3.0, 4.0]))
    b=numerical_gradient(function_2, np.array([0.0, 2.0]))
    c=numerical_gradient(function_2, np.array([3.0, 0.0]))
    a,b,c
    # array([ 6.,  8.])
    # array([ 0.,  4.])
    # array([ 6.,  0.]) """
    
    ```

### 计算机计算梯度

- 数值微分法虽然简单，也容易实现，但缺点是计算上比较费时间。
- 误差反向传播法可以快速求解该问题

### 梯度和偏导数的关系

- 从上面的分析以及高等数学的知识可知，（多元函数的）梯度是建立在偏导数的基础上:

  - $$
    \nabla{f}=(\frac{\partial{f}}{\partial{x_1}},\frac{\partial{f}}{\partial{x_2}},\cdots,\frac{\partial{f}}{\partial{x_n}})
    $$

    - $\nabla{f}$表示梯度向量(而不是标量)

  - 另一方面,偏导数也可以展开成梯度和单位方向导数的数量积

    - 以三元函数$f(x,y,z)$为例

    - $$
      \frac{\partial{u}}{\partial{l}}
      =(\frac{\partial{u}}{\partial{x}}
      ,\frac{\partial{u}}{\partial{y}}
      ,\frac{\partial{u}}{\partial{z}})
      (\cos{\alpha},\cos{\beta},\cos{\gamma})
      $$

    - 其中$l$是方向导数的方向,$(\cos{\alpha},\cos{\beta},\cos{\gamma})$是方向$l$的单位方向向量

## 梯度法

### 梯度和最小值

- 机器学习的主要任务是在学习时寻找最优参数。同样地，神经网络也必须在学习时找到最优参数（权重和偏置）。
- 这里所说的<u>最优参数是指损失函数取**最小值**时的参数</u>。🎈
- 但是，一般而言，损失函数很复杂，参数空间庞大，我们不知道它在何处能取得最小值。而通过巧妙地使用梯度来寻找函数最小值（或者尽可能小的值）的方法就是梯度法。
- 这里需要注意的是，梯度表示的是各点处的函数值减小最多的方向。因此，无法保证梯度所指的方向就是函数的最小值或者真正应该前进的方向
- 实际上，在复杂的函数中，梯度指示的方向基本上都**不是**函数值最小处。
- 函数的<u>极小值、最小值</u>以及被称为<u>鞍点</u>（saddle point）的地方，**梯度为 0**。
  - 极小值是局部最小值，也就是限定在某个范围内的最小值。
  - 鞍点是从某个方向上看是极大值，从另一个方向上看则是极小值的点。
  - 可见:虽然梯度法是要寻找**梯度为 0** 的地方，但是那个地方不一定就是最小值（也有可能是极小值或者鞍点）。
- 此外，当函数很复杂且呈扁平状时，学习可能会进入一个（几乎）平坦的地区，陷入被称为“学习高原”的无法前进的停滞期。
- 虽然梯度的方向并不一定指向最小值，但<u>沿着它的方向能够最大限度地减小函数的值</u>。
- 因此，在寻找函数的最小值（或者尽可能小的值）的位置的任务中，要<u>以梯度的信息为线索</u>，决定前进的方向。

### 优化问题

- 大多数深度学习算法都涉及某种形式的优化。优化指的是改变 x 以最小化或最大化某个函数 $f(x)$ 的任务。
- 我们通常以最小化 $f(x)$ 指代大多数最优化问题。最大化可经由最小化算法最小化 $−f(x)$ 来实现。
- 我们把要最小化或最大化的函数称为 **目标函数**（objective function）或 **准则**（criterion）。
  - 当我们对其进行最小化时，我们也把它称为 **代价函数**（cost function）、**损失函数**（loss function）或 **误差函数**（error function）。

### 梯度法

- 在梯度法中，函数的取值从当前位置**沿着梯度**方向**前进一定距离**，然后在**新的地方重新求梯度**，再沿着新梯度方向前进，<u>如此反复，不断地沿梯度方向前进</u>。
  - 像这样，通过不断地<u>沿梯度方向</u>前进，<u>逐渐减小函数值的过程</u>就是**梯度法**（gradient method）。
  - 当梯度随着迭代减小到接近0的时候,目标函数
  - 梯度法是解决机器学习中最优化问题的**常用方法**，特别是在神经网络的学习中经常被使用。
- 根据目的是寻找<u>最小值还是最大值</u>，梯度法的<u>叫法有所不同</u>。
  - 严格地讲，寻找最小值的梯度法称为**梯度下降法**（gradient descent method），寻找最大值的梯度法称为**梯度上升法**（gradient ascent method）。
  - 但是通过<u>反转损失函数的符号</u>，求最小值的问题和求最大值的问题<u>会变成相同的问题</u>，因此“下降”还是“上升”的差异本质上并不重要。
  - 一般来说，神经网络（深度学习）中，**梯度法主要是指梯度下降法**。

#### 其他参考

- [Gradient Descent Algorithm — a deep dive | by Robert Kwiatkowski | Towards Data Science](https://towardsdatascience.com/gradient-descent-algorithm-a-deep-dive-cf04e8115f21)

- **梯度下降法**（英语：Gradient descent）是一个一阶[最优化](https://zh.wikipedia.org/wiki/最优化)[算法](https://zh.wikipedia.org/wiki/算法)，通常也称为**最陡下降法**
  - 但是不该与近似积分的<u>最陡下降法</u>（英语：Method of steepest descent）混淆。 
- 要使用梯度下降法找到一个函数的[局部极小值](https://zh.wikipedia.org/wiki/最值)，必须向函数上当前点对应[梯度](https://zh.wikipedia.org/wiki/梯度)（或者是近似梯度）的***反方向***的规定步长距离点进行[迭代](https://zh.wikipedia.org/wiki/迭代)搜索。
- 如果相反地向梯度*正方向*迭代进行搜索，则会接近函数的[局部极大值](https://zh.wikipedia.org/wiki/最值)点；这个过程则被称为**梯度上升法**。

- 梯度下降方法基于以下的观察：

  - 如果实值函数$F(x)$在点$a$ 处可微且有定义，那么函数$F(x)$在$a$ 点沿着梯度相反的方向$-\nabla F(a)$ 下降最多。

  - $$
    因而，如果{\mathbf  {b}}={\mathbf  {a}}-\gamma \nabla F({\mathbf  {a}})
    对于一个足够小数值\gamma >0时成立，那么F({\mathbf  {a}})\geq F({\mathbf  {b}})。\\
    考虑到这一点，我们可以从函数F的局部极小值的初始估计
    {\mathbf  {x}}_{0}出发，并考虑如下序列 \\
    {\mathbf  {x}}_{0},{\mathbf  {x}}_{1},{\mathbf  {x}}_{2},\dots 使得\\
    \mathbf{x}_{n+1}=\mathbf{x}_n-\gamma_n \nabla F(\mathbf{x}_n),\ n \ge 0。\\
    因此可得到
    F({\mathbf  {x}}_{0})\geq F({\mathbf  {x}}_{1})\geq F({\mathbf  {x}}_{2})\geq \cdots ,
    $$

  - 如果顺利的话序列$x_n$收敛到期望的局部极小值。注意每次迭代*步长*$\gamma$可以改变。

    右侧的图片示例了这一过程，这里假设$F$定义在平面上，并且函数图像是一个[碗](https://zh.wikipedia.org/wiki/碗)形。
    
    蓝色的曲线是[等高线](https://zh.wikipedia.org/wiki/等高线)（[水平集](https://zh.wikipedia.org/wiki/水平集)），即函数$F$为常数的集合构成的曲线。红色的箭头指向该点梯度的反方向。（一点处的梯度方向与通过该点的[等高线](https://zh.wikipedia.org/wiki/等高线)垂直）。
    
    沿着梯度*下降*方向，将最终到达碗底，即函数$F$局部极小值的点。

### 梯度下降法的数学表示

- $$
  x_0=x_0-\eta\frac{\partial{f}}{\partial{x_0}}
  \\
  x_1=x_1-\eta\frac{\partial{f}}{\partial{x_1}}
  \\
  \vdots
  \\
  x_n=x_n-\eta\frac{\partial{f}}{\partial{x_n}}
  $$

  - 上述公式表明各个分量是如何更新的.

    - $x_i=x_i-\eta\frac{\partial{f}}{\partial{x_i}}$

    - $$
      x_i=x_i-\eta\frac{\partial{f}}{\partial{x_i}}
      ,(i=1,2,\cdots,n)
      \\
      用向量的方式描述,设向量x=(x_1,x_2,\cdots,x_n)
      \\
      x=x-\eta{\frac{\partial{f}}{\partial{x}}}
      \\
      用迭代(地推的方式描述):
      \\
      x_{n+1}=x_n-\eta_{n}\nabla{f(x_n)}
      $$
      
    - 程序设计可以写作:$x_i$`-=`$\eta\frac{\partial{f}}{\partial{x_i}}$

  - $\eta$表示更新量,在神经网络中称为**学习率**

    - 学习率决定一次学习中,应该学习多少,在多大程度上更新参数

  - 上述迭代公式的有效性证明比较复杂,需要而外参考资料TODO

  - 上述公式是一次更新执行的计算内容,为例反复更新,上述公式反复执行

    - 每一步更新都按照相同的公式计算,逐渐减小(损失)函数的值

  - 学习率需要事先确定某个值,一般而言这个值不易过大或过小(否则不容易使得参数$x_i$到达一个好位置)

    - 神经网络的学习一般会边改边学习率的值,一般确认学习<u>是否正确进行了</u>

- ```python
  def gradient_descent(f, init_x, lr=0.01, step_num=100):
      x = init_x
  
      for i in range(step_num):
          grad = numerical_gradient(f, x)
          x -= lr * grad
  
      return x
  ```

- ```python
  # coding: utf-8
  ##
  import numpy as np
  import matplotlib.pylab as plt
  from gradient_2d import numerical_gradient
  
  def function_2(x):
      return x[0]**2 + x[1]**2
  
  def gradient_descent0(f, init_x, lr=0.01, step_num=100):
      x = init_x
  
      for i in range(step_num):
          grad = numerical_gradient(f, x)
          x -= lr * grad
  
      return x
  
  def gradient_descent(f, init_x, lr=0.01, step_num=100):
      x = init_x
      x_history = []#收集点坐标(x0,x1)的历史数据,用于绘图
  
      for i in range(step_num):
          # 每次更新x前先备份值到x_history中
          x_history.append( x.copy() )
          # 计算梯度向量
          grad = numerical_gradient(f, x)
          #根据梯度下降法公式:x_new=x-lr*grad
          x -= lr * grad
  
      # 返回更新了step_num此的位置x,以及100个x的历史更新位置
      return x, np.array(x_history)
  
  init_x = np.array([-3.0, 4.0])    #设置初始位置
  x,x_history=gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)
  print(f'{x=}\n')
  
  ##
  #画图
  plt.plot( [-5, 5], [0,0], '--b')
  plt.plot( [0,0], [-5, 5], '--b')
  plt.plot(x_history[:,0], x_history[:,1], 'o')
  
  plt.xlim(-3.5, 3.5)
  plt.ylim(-4.5, 4.5)
  plt.xlabel("X0")
  plt.ylabel("X1")
  plt.show()
  
  ```

  

### 学习率和超参数

- 学习率过大的话，会发散成一个很大的值；反过来，学习率过小的话，基本上没怎么更新就结束了。也就是说，设定合适的学习率是一个很重要的问题。
- 像学习率这样的参数称为**超参数**。这是一种和神经网络的参数（权重和偏置）性质不同的参数。
- 相对于神经网络的权重参数是通过训练数据和学习算法自动获得的，学习率这样的<u>超参数则是人工设定的</u>。
- 一般来说，超参数需要尝试多个值，以便找到一种可以使学习顺利进行的设定。

## 神经网络的梯度

- 神经网络的学习需要求梯度,此处梯度指<u>损失函数关于权重参数的**梯度**</u>

- 设

  - $$
    W=
    \begin{pmatrix}
        w_{11}&w_{12}&w_{13}\\
        w_{21}&w_{22}&w_{23}
    \end{pmatrix}
    \\
    \frac{\partial{L}}{\partial{W}}=
    \begin{pmatrix}
        \frac{\partial{L}}{\partial{w_{11}}}&\frac{\partial{L}}{\partial{w_{12}}}&\frac{\partial{L}}{\partial{w_{13}}}\\
        \frac{\partial{L}}{\partial{w_{21}}}&\frac{\partial{L}}{\partial{w_{22}}}&\frac{\partial{L}}{\partial{w_{23}}}
    \end{pmatrix}\\
    $$

    - 用$L$表示损失函数
    - $\frac{\partial{L}}{\partial{W}}$中的各个元素$\frac{\partial{L}}{\partial{w_{ij}}},i=1,2;j=1,2,3$由关于W的偏导数构成
      - 在这个$2\times{3}$的矩阵中含有6个元素(权重值),如果将他们视为6个变量,损失函数L对于W矩阵中的6个变量分别求偏导数构成的矩阵$\frac{\partial{L}}{\partial{W}}$
    - $\frac{\partial{L}}{\partial{w_{11}}}$表示(描述)当权重$w_{11}$稍微变换时,损失函数L发生多大的变化

  - 例如,设$\frac{\partial{L}}{\partial{w_{11}}}$的值大约是0.2,则$w_{11}$增加$h$,那么损失函数的值增加约0.2h.

    - 如果$\frac{\partial{L}}{\partial{w_{23}}}\approx{-0.5}$这表示如果$w_{23}$增加h,损失函数的值将减小约0.5h

    - 参考:

      - 对于一元微分的应用知识:$f(x+h)\approx f(x)+f'(x)h,(h\to{0})$
      - 类比到多元函数偏导数:$L(x_1,x_2,\cdots,x_i+h,\cdots,x_n)\approx L(x_1,x_2,\cdots,x_i,\cdots,x_n)+\frac{\partial{L}}{\partial{x_{i}}}h$

      

## 神经网络学习算法

- 神经网络存在合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为“学习”。

- 神经网络的学习分成下面4个步骤。
  - 步骤 1（mini-batch）
    - 从训练数据中随机选出一部分数据，这部分数据称为mini-batch。
    - 我们的目标是减小mini-batch的损失函数的值。
  - 步骤 2（计算梯度）
    - 为了减小mini-batch的损失函数的值，需要求出各个权重参数的梯度(梯度表示损失函数的值减小最多的方向。)
  - 步骤 3（更新参数）
    将权重参数沿梯度方向进行<u>微小更新</u>。
  - 步骤 4（重复）
    重复步骤1、步骤2、步骤3。

- 这个方法通过梯度下降法更新参数，使用的数据是<u>随机选择的 mini batch数据</u>，称为**随机梯度下降法**（stochastic gradient descent）。
  - “随机”指的是“随机选择的”的意思，因此，随机梯度下降法是“对<u>随机选择的数据</u>进行的梯度下降法”。
  - 深度学习的很多框架中，随机梯度下降法一般由一个名为SGD的函数来实现。
  - SGD来源于随机梯度下降法的英文名称的首字母。
  - 这里以 2层神经网络（隐藏层为 1层的网络）为对象，使用 MNIST数据集进行学习。

## 基于测试数据的评价

- 通过反复学习可以使损失函数的值逐渐减小。
- 不过这个损失函数的值，严格地讲是“对训练数据的某个 mini-batch的损失函数”的值。训练数据的损失函数值减小，虽说是神经网络的学习正常进行的一个信号，但光看这个结果还不能说明该神经网络在其他数据集上也一定能有同等程度的表现。
- 神经网络的学习中，必须确认是否能够正确识别训练数据以外的其他数据，即确认是否会发生过拟合。过拟合是指，虽然训练数据中的数字图像能被正确辨别，但是不在训练数据中的数字图像却无法被识别的现象。
- 神经网络学习的最初目标是掌握泛化能力，因此，要评价神经网络的泛化能力，就<u>必须使用不包含在训练数据中的数据。</u>
- 可以在进行学习的过程中，会<u>定期地对训练数据和测试数据记录识别精度</u>。
- 例如,每经过一个epoch，我们都会记录下训练数据和测试数据的识别精度

### epoch

- epoch是一个单位,一个 epoch 表示学习中<u>所有训练数据均被使用过一次时的**更新次数**</u>。
  - 比如，对于 10000 笔训练数据，用大小为 100笔数据的 mini-batch 进行学习时，重复随机梯度下降法 100 次，<u>所有的训练数据就都被“看过”了</u>。此时，100 次就是一个 epoch。
- 一般做法是事先将所有<u>训练数据随机打乱</u>，然后按指定的批次大小，按序生成 mini-batch。这样每个 mini-batch均有一个索引号，比如此例可以是 0, 1, 2, . . . , 99，然后用索引号可以遍历所有的 mini-batch。
- 遍历一次所有数据，就称为一个 epoch。
- 若mini-batch每次都是随机选择的，则不一定每个数据都会被看到。

- 如果随着 epoch的前进（学习的进行），我们发现使用训练数据和测试数据评价的识别精度都提高了，并且，这两个识别精度基本上没有差异（两条线基本重叠在一起）。因此，可以说这次的学习中没有发生过拟合的现象















