[toc]

# 摘要

- 近年来，语音情感识别(Speech Emotion Recognition，SER)在人机交互过程中发挥着越来越重要的作用，相关领域的研究也越来越受到重视。SER的主要目的是对语音信号按照不同的情感进行分类，比如“生气”、“恐惧”、“厌恶”、“高兴”等。在过去的几年里，该领域研究者已经提出了许多有效的方法来解决SER中出现的问题，这些方法大部分是集中在一个单一的语音数据库上进行的。大多数的语音情感识别方法主要在单一语音数据库上进行训练和测试。这些训练数据和测试数据具有相同的声学条件，在这种条件下，现有的语音情感识别技术已经实现了较高的性能。然而，在实际应用中训练和测试集可能来自不同的语音情感数据库，用于训练的语料库与测试语料库之间往往存在非常大的差异。由于噪声环境、说话方式、说话人的特性等因素，使得语音数据库特征空间分布不均匀，因此传统的SER系统存在跨库识别率较低等问题。为了提高SER系统的跨库识别性能，近年来许多新的跨库技术和方法被提出。本文献综述系统性综述了近年来跨库语音情感识别方法的研究现状与进展，分析了新兴的深度学习方法在跨库语音情感识别中的应用情况。主要从跨库语音情感识别的意义、语音数据库、语音特征、传统机器学习分类器和深度学习方法等对语音情感识别的相关文献进行分析，并对未来的研究方向进行了展望。

## 关键词

- 语音情感识别; 跨库;  语音情感；深度学习; 手工特征; 深度特征;

# Abstract

In recent years, Speech Emotion Recognition ( SER ) plays an increasingly important role in the process of human-computer interaction, and research in related fields has received more and more attention. The main purpose of  SER is to classify speech signals according to different emotions, such as ' angry ', ' fear ', ' disgusted ', ' happy ', etc. In the past few years, researchers in this field have proposed many effective methods to solve the problems in SER, most of which are concentrated on a single speech database. Most speech emotion recognition methods are mainly trained and tested on a single speech database. These training data and test data have the same acoustic conditions. Under this condition, the existing speech emotion recognition technology has achieved high performance. However, in practical applications, training and test sets may come from different speech emotion databases, and there are often very large differences between the training corpus and the test corpus. Due to factors such as noise environment, speech style, and speaker characteristics, the feature space distribution of the speech database is uneven, so the traditional SER system has a low cross-database recognition rate. In order to improve the cross-database identification performance of SER system, many new cross-database technologies and methods have been proposed in recent years. This literature review systematically summarizes the research status and progress of cross-database speech emotion recognition methods in recent years, and analyzes the application of emerging deep learning methods in cross-database speech emotion recognition. Mainly from the cross-database speech emotion recognition.

## Keywords

Speech emotion recognition ; cross-library ; voice emotion ; deep learning ; handcrafted features ; depth features ;

# 跨库语音情感识别的研究背景和意义



情感是人类现实生活中的一个重要因素。人类情感可以被诸如手势，面部表情，身体姿态言语交流。许多物理属性也被用于人类情感识别，例如体温，心率，血压，肌肉活动，皮肤电阻[1]。 人类情感也可以通过语音交流被良好的识别。从语音中识别出人类情感的一个基础问题是从给定的说话者的一段语音信号中识别情感状态。过去的几十年里，从语音中识别情感是研究人员不断增长的研究领域，因为它在许多现实生活中的问题，例如呼叫中心对话自动回复系统，在线辅导，对话系统，疼痛识别，抑郁诊断等。

利用计算机从语音信号中自动识别出说话人的情感状态，是实现自然人机交互界面的关键前提。借助语音情感识别系统，可以提升用户操作计算机的体验。由于跨库语音情感识别涉及多个学科，因此对该领域的研究还有助于促进相关学科的共同进步，具有重要的使用价值和理论价值。跨库语音情感识别系统考虑的条件相比于单数据库训练出来的SER系统更加接近实际生活，具有更好的鲁棒性和实用性，更加适应实际生活的识别要求，跨库技术的研究使SER系统在使用的时候更加有效。

-  Paidi G， Kadiri S， Yegnanarayana B: Analysis of Emotional Speech—A Review， 2016: 205-238.

### 传统语音情感识别的一般流程

跨库语音情感识别作为特殊的语音情感识别任务，两者遵循类似的总体处理流程.

1. 情感语音信号采集
2. 情感语料库
3. 预料预处理
4. 特征提取
5. 特征选择(优选/降维)
6. 情感识别模型
7. 情感识别(分类)

相邻两个编号的环节中，前一个环节是后一个环节的基础和输入[]

- 罗德虎，冉启武，杨超等.语音情感识别研究综述[J].计算机工程与应用，2022，58(21):40-52.

# 情感语音数据库

语音情感语料库的分类有多种，可按语种、情感描述模型、自然度来分类。对于按情感描述模型为分类标准，可将语料库分为维度情感语料库以及离散情感语料库。下面主要介绍基于自然度的类别划分，包括表演型（基于演员型），诱导型和自然型，它们的自然度依次提升。

## 基于演员型

基于演员的情感语音数据库也称为模拟情感语音数据库。这些类型的数据库是由训练有素的专业演员创建的，如广播艺术家、戏剧艺术家，或者来自一个可以用不同情感说话的人。录音是由说话人在不同的情绪下讲同一文本所做的。记录可能在早上、下午、晚上和晚上等不同的时段进行，以考虑人类的物理言语和表现力的变化。这是完整情感范围内记录情感语音数据库的可靠方式之一。超过60 %的情感语音数据库是模拟数据库。一般来说，模拟的情感数据库比真实的情感数据库更具表现力[  ]。

-  Williams C E， Stevens K N. Emotions and Speech: Some Acoustical Correlates[J]. The Journal of the Acoustical Society of America， 1972， 52(4B): 1238-1250.

## 诱导型

由于诱导情感语音数据库更接近自然数据库，因此也被称为半自然情感数据库。这些类型的数据库是在行为人不知情的情况下，在人工情感情境下录制的。在人工情感情境创设之后，演员与说话人一起参与情感转换。这种类型的数据库比模拟数据库更加自然。但如果说话人知情，录音可能不具有表现力，那么这就是一种人为的情感情境。

## 自然型

自然情感数据库是真实的数据，有时很难识别情感。这类数据库可以从客服与客户的谈话、电视广播、医患对话、法庭、非正常情况下的驾驶舱录音等方面进行记录。在这些情况下，很难找到完整的情感范围，也存在一些版权和安全问题[ ]。

-  El Ayadi M， Kamel M S， Karray F. Survey on speech emotion recognition: Features， classification schemes， and databases[J]. Pattern Recognition， 2011， 44(3): 572-587.



## 常用情感语音数据库

- EMO-DB

这个数据集包含10个德语句子，10个人分别模仿了7种情感，其中7种情感分别是中性的，愤怒的，恐惧的，快乐的，悲伤的，厌恶的和厌烦的，总共采集了536个样本，总共耗时0.38小时。录音是在柏林技术学院技术声部的消音室进行的，取样频率为48 kHz，随后向下取样为16 kHz，每一次取样都用16比特的数字来表示。该数据库被大量的语音情感识别实验和研究所采用，是许多研究的基础。

- SAVEE

该数据集是一组多模式情绪数据集，以英国英语为基础进行录制。这个语料库中一共有480个语音和7种情绪，分别是：中性，愤怒，惊讶，快乐，悲伤，恐惧和厌恶。这些语音的录制是由4名男演员组成的。为了保证高品质的情绪表现，所有的记录都是通过10个不同的评估人员在音频、视觉和视听环境下进行的。这段录音的剧本是从传统的 TIMIT语料库中选出的。

- RAVDESS
- 这是一个多模态的关于情绪语音和歌曲的数据集。这一数据集具有性别平衡性，邀请了24位专业人员，他们用北美语调进行了语音和歌曲的创作。这些情绪语音中包括平静，恐惧，惊讶，快乐，悲伤，愤怒，厌恶。在情感方面，它包括平静，恐惧，惊讶，快乐，悲伤，愤怒，厌恶和恐惧。每一种表情都来自于两个不同的情绪强度，再加上一种中性的表达。

1.  Burkhardt F，Paeschke A，Rolfes M，et al.A database of German emotional speech.Proceedings of the 9th European Conference on Speech Communication and Technology.Lisbon:ISCA，2005.1517-1520.
2. Sugan N，Srinivas NSS，Kar N，et al.Performance comparison of different cepstral features for speech emotion recognition.Proceedings of 2018 International CET Conference on Control，Communication，and Computing(IC4).Thiruvananthapuram:IEEE，2018.266-271.
3. Ali Z，Talha M.Innovative method for unsupervised voice activity detection and classification of audio segments.IEEEAccess，2018，6:15494-15504.
4. Livingstone SR，Russo FA.The ryerson audio-visual database of emotional speech and song(RAVDESS):A dynamic，multimodal set of facial and vocal expressions in North American English.PLoS One，2018，13(5):e0196391.

# 语音情感特征

特征提取是所有模式识别系统的重要部分。在跨库语音情感识别中，提取域不变性的情绪特征是非常关键的，域不变性特征domain invariant )的提取将直接影响到跨库语音的情感识别效果。按照所使用的样本，可以分为三种：监督、半监督和无监督。

监督学习是用标记的训练数据来训练网络,使其可以根据新的输入数据得到相应的预测输出[]。与监督学习不同，无监督学习采用完全没有标记的训练资料进行训练[]，半监督式学习是一种介于二者之间的学习方法，它的训练数据包括有标签的数据和没有标签的数据，一般认为所有的训练数据都来源于同一或类似的分布[].

OpenSMILE[]是最常用的语音特征提取工具包，常用于Interspeech副语言信息提取挑战赛，可提取多种语音特征,包括能量信号、语音质量、共振峰等特征



1. Caruana R ,  Niculescu-Mizil A . An Empirical Comparison of Supervised Learning Algorithms Using Different Performance Metrics[C]// International Conference on Machine Learning. ACM, 2006.
2. Barlow HB.Unsupervised learning.Neural Computation，1989，1(3):295-311.
3. Zhu XJ，Goldberg AB.Introduction to Semi-supervised Learning.Synthesis Lectures on Artificial Intelligence and Machine Learning.San Rafael:Morgan&amp;Clay pool Publishers.1-84.
4. Zhu XJ.Semi-supervised learning literature survey.Technical Report，Madison:University of WisconsinMadison，2005.
5. EYBEN F,WENINGER F,GROSS F,et al.Recent developments in opensmile,the munich open-source multimedia feature extractor[C]//Proceedings of the 21st ACM International Conference on Multimedia,2013:835-838.

## 谱特征

谱特征可以通过傅里叶变换获得，傅里叶变换是将频域变换到时域。最先进的谱特征是梅尔频率倒谱系数( MFCC )。

此外还有线性预测倒谱系数（LPCC）和感知线性预测系数（PLP）等特征。

在语音识别中，语音模型与语句样本的不一致将导致语音情感识别能力大幅降低。从图象的视角分析，语谱图的特点可以很好地弥补已有的传统语音情感特征的不足。

## 韵律特征

音高、时长、能量等韵律特征被认为和情绪具有良好的相关[ 44  ]。最大值、最小值、音高相似特征、方差、极差、平均值和标准差等作为良好的韵律信息源，用于识别在片段级别提取的情绪[ 47 ]。

- Lee C M ，  Narayanan S S . Toward detecting emotions in spoken dialogs[J]. IEEE， 2005(2).
- Schröder M. Emotional speech synthesis: a review[M].  2001: 561-564.

## 音质特征

音质特征被定义为个体的语音特征。音质特征是许多语音处理、说话人识别、情感识别等的中心。格式频率、带宽、声门参数、谐波噪声比、抖动和微光等特征被称为音质特征。音质与情感内容之间存在着截然不同的相互关系[ 52 ]。

-  Guidi A ，  Gentili C ，  Scilingo E P ， et al. Analysis of speech features and personality traits[J]. Biomedical Signal Processing and Control， 2019， 51(MAY):1-7.

# 跨库SER分类模型

对于SER来说，分类器与语音特征同等重要。可分为两类：( i )传统机器学习分类器；( ii )深度学习分类器。混合方法，即传统分类器和深度学习分类器的结合也被一些研究者使用。大量的分类器已经被用于SER的研究，但是到目前为止，很难确定哪个分类器表现最好。

## 传统的机器学习方法

传统的用于SER的ML方法是在从语音信号中提取所需特征后应用的。许多分类器已经被研究人员对SER进行了评估，以达到更好的精度。常用的传统分类器有支持向量机、高斯混合模型、隐马尔可夫模型、人工神经网络、k近邻等[ 5 ]。

## 深度学习方法

深度学习概念被描述为机器学习的子集，它从多个层面进行学习。深度学习在2015年前后对语音情感识别影响较大。近年来，深度学习方法为语音情感识别提供了令人鼓舞的结果。与传统的机器学习方法相比，深度学习方法具有功能灵活、特征自动学习范围广、可扩展性强、识别率高等优点，被认为是最适合情感识别的分类器。另一方面，传统的ML方法需要较少的数据进行训练。研究者们使用了多种深度学习方法进行情感识别。目前的研究者正在使用深度学习方法进行情绪识别并提高准确率。使用深度学习方法的语音情感识别模型的主要类别是使用自动学习相关特征的方法、使用手工特征和使用语谱图。总体而言在2013年之后，使用深度学方法进行情绪识别的研究逐渐增多。目前，研究者们提出的SER模型大多采用深度学习方法，并在平均精度和计算成本方面取得了较好的结果。

# 近年来跨库语音情感识别的研究

2014年金赟[]等针对训练样本与测试样本来自不同语料库造成特征分布不匹配的问题，使用半监督判别分析来缩减二者在分布上的差异。并寻找了在有标签的训练样本和来自另一个库的一部分无标签训练样本之间的可能的最优投影方向。根据相似性假定，相似点的类型更接近，采用 P-近邻图建立了相似度模型，得到了非标记样本的分布信息。在保证不带标记的样本之间的流形结构的前提下，将训练样本类间的离散程度与类内离散程度之比最大化，以获得最佳的投影方向。该研究的第一组使用 eNTERFACE库对 Berlin库进行了识别，其识别率为51.41%，而对 eNTERFACE库的识别率为45.76%；通过对试验前后的数据进行可视化分析，表明采用半监督判别方法可以有效地减少各库间的特征向量空间分布不一致性，提高了跨库语音的情感识别性能。

2016年，张昕然[]基于跨库语音情感特征的降维选优，作者建立了具有无限组元数目的t分布混合模型。与传统的 GMM方法比较，基于t分布的语音情感识别模型能够较好地解决样本特征空间中出现的异常值。在情绪模型中引入了全局隐空间来解决高维空间中数据的高复杂性，以及缺乏训练样本的问题。该方法使得样本空间中的组分数目是无穷大的，从而构成了一个被称为 iSMM的情感模型。该模型在满足较低的复杂度的前提下，能够自动地判断出最优组分的数目，从而实现对各种情绪特征的分类。为验证iSMM模型对不同情感特征分布空间的识别效果，作者在3个数据库上进行仿真实验(表演型语料库DES、EMO-DB和自发型语料库FAU)。结果显示iSMM相比其它又有的对比模型，整体上保持了更稳定的识别性能。此外，作者还提出了一种基于听觉的注意机制，使得模型能够从多个不同的语料库中抽取具有显著性的特征，从而增强了对情绪的识别。实验表明，与国际上常用的标准方法比较，语音信号的识别率提高了9%左右，表明了该方法在处理来自不同数据库的各种数据时具有的鲁棒性。在深度学习的基础上，结合深度信念模型，建立了一种新的特征层融合算法，该算法把声音频谱图中所蕴涵的感情信息作为图象的特征，并与传统的声学情感特性相结合。

2017年，张昕然[]通过 STB/Itti模型，从颜色、亮度、方向等方面提取了语音信号的新特征；其次，对改进后的 DBN网络模型进行了分析，并将其与新的语音信号进行了特征层次的融合，从而提高了其特征子集的规模，提高了其情绪表达的能力。在 ABC和多个中文数据库上进行了试验，发现新的特征子集与传统的语音情绪特征相比，具有显著的提高。

2019年，陈颖	[]由于特征映射的迁移学习忽视了特殊信息，因此，在此基础上，提出了一种将共性和特性相结合的双子空间迁移学习框架，并在此基础上对只使用共性的特征映射进行了改进，从而改善了对情绪识别的效果。

钟琪[]利用 IEMOCAP英语情感数据库， CASIA汉语情感数据库，EMO-BD德语情感数据库，以单语语料库、混合语料库、跨语料库、德语情感数据库为主要研究对象。利用支持向量机（SVM）、卷积神经网络（CNN，Convolutional Neural Networks）以及长短期记忆 （LSTM，Long-Short TermMemory）作为分类器，进行情绪识别。通过对实验数据的分析，发现不同语料库中的语音情感特征和识别模型具有相似性。实验结果表明，英文和中文的哀伤情绪在模式上表现出了很好的概括性，而英文的哀伤情绪和中文的中庸情绪则表现出了很好的适应性。

2020年，陈秀珍[]研究了基于子空间学习模式的跨库语音情感识别算法。该算法利用投影子空间将声音特征映射到标签空间中，从而建立了源域和目标域的联系，从而降低了特征分布的差异性。用l1和l2，1范数作为该模式的正则项，以得到更加高效的投影矩阵。最后，从 INTERSPEECH情感挑战赛中抽取IS09和IS10两个不同的特征集，通过三个开放和广泛被使用的语音数据库（EmoDB、 eNTERFACE、 AFEW4.0）对该模型进行了试验，并与已有的跨库语音情绪识别方法进行了比较，结果显示该算法是行之有效的，IS09特征集明显优于IS10。

刘雨柔[]利用变分模态算法可以很好地处理非线性的非平稳信号，将不同的频率合并，然后合成不同频率的信号，再通过伽马通滤波器，求对数运算，并计算离散余弦变换后的统计参数，获得新的情绪语音特征；针对单个特征不能充分反映情绪信息，从混沌的角度对其特征进行分析，并将其与新的频谱特征相结合，从而获得整体性的全局特征。作者设计了一种复合网络栈式稀疏自编码网络的核函数极限学习机，首先对原始特征通过栈式稀疏自编码网络进行无监督预训练，再结合数据标记，采用反向传播算法监督学习对其进行调整，通过对神经网络模型的重新构造，获得了更加接近于人脑稀疏度和更能分辨情绪信息的深度特征，并利用人工蜂群优化的核函数极限学习机进行情绪识别和分类。

2021年汪洋[]等提出一种基于决策边界优化域自适应（decision boundary optimized domain adaptation， DBODA）的跨库语音情感识别方法。首先利用卷积神经网络(CNN)进行初步的特征处理，再将特征送入最大化核范数及均值差异（maximum n-norm and mean discrepancy， MNMD）模块中优化，使得模型在减小域间差异的同时，还能够最大化目标域情感预测概率矩阵的核范数，最终达到提升目标域样本的识别率，实现优化决策边界目标。在以Berlin， eNTERFACE， CASIA三个常用的语音库为基准库设立的六组跨库语音情感识别实验中，所提方法的平均识别率领先已有的其他识别算法1.28%~11.01%，该结果说明此模型能狗有效降低决策边界的样本密度，优化了识别模型预测的准确性。

钟颖[]采用了Focal 损失函数，一方面，该函数能够使网络模型对较难样本的学习获得更多的关注，另一方面也达到了减少对简单样本学习的关注度。通过结合注意力机制，使得识别网络可以增强模型获取情感显著信息的部分。在验证实验中，该模型在IEMOCAP 和EmoDB 两个单一语料库上的识别任务中分别达到了71.72%和90.1%的未加权精度。通过与目前已知参数量最少的模型做比较，该模型的参数量大幅度降低;为了提高跨库语音情感识别模型泛化能力，作者采用增量学习的方法对模型进行改进训练，经过验证，使用了增量学习的办法在IEMOCAP、MSP-IMPROV、MES-P 三个语音数据库的跨库语音情感识别性能上取得了显著的改进。 

李晓坤[]等基础上现有语音情感识别方法的扩充性研究中，为了改进跨库语音情感识别性能，提出一个深度迁移网络基于注意力机制的长短时动态对抗适配网络(LSTM-TF-at-DAAN)。 eNTERFACE 和 EMO-DB是两个在语音情感识别领域被广泛使用的语音数据库。新模型基于上述语音数据库上进行实验，并将实验结果与采用一般迁移学习方法的实验结果进行对比，结果表明 LSTM-TF-at-DAAN网络相较于一般迁移学习方法的识别准确率提升了 5.37% ，证明了深度迁移学习应用于跨库语音情感识别的有效性。

郑婉璐[]提出一种全局局部尺度对抗网络。该网络为了获得具有更强判别性和泛化性的语音情感特征，针对语音信号的时序特征进行网络模型的构建。针对语音信号的时序性，提出了一种融合了手工特征、深度特征等多尺度特征的优势，在局部、全局、融合尺度上的语音情感特征的提取方法，具备同时表征不同语音数据库中的情感信息的能力。同时，还提出一种基于注意力的时序信息建模网络，该网络能够筛选出与情感相关的语音片段，以获得高判别性的语音情感特征。此外，为了降低源域数据库和目标数据库的情感特征分布不同导致语音情感识别模型性能低的负面影响，作者提出了一种分层级的差异对抗网络，在多尺度特征层面上同时地改进域间差异问题，得到强泛化性的语音情感特征

## 结论和展望

从语音情绪辨识的概念提出至今，已有大量的语音、情绪资料，但是由于自然环境中采集的声音样本会受到噪声干扰，目前已有的资料很少。在自然条件下，如何采集到干净的声音信号，并对噪声信号进行有效地处理，是今后的一个重要课题。另外，由于对数据集的注释比较困难，因此，利用已有的资料来标注未知数据集合中的情绪，这是一个有待深入研究的问题。

在跨库语音的情绪识别中，以前的工作是将人工特性与标准化方法、 MMD标准相结合，以消除数据集之间的差异性，并对齐数据集之间的特性进行研究。在此基础上，进一步证实了基于深度学习的方法能够有效地改善语音识别的准确性。近年来，大量的基于深度学习的跨库语音情感识别方法别提出，并成功改善了跨库识别性能。其中最重要的是通过使用自编码器、基于对抗学习等域的自适应算法获得域不变性特征，从而提高分类器在目标区域内的识别率。

目前的深度学习方法多是通过对高层特征的学习，而忽视了低层次的情感特征，今后的研究工作可以将面向跨库的低级和高级特性结合起来，从而提高语音的情绪识别效果。随着深度学习技术的不断发展，目前深度学习技术已经越来越成熟，但仍然存在着许多问题，例如：网络参数多、运算量大、样本数量要求高、训练时间长等问题，所以深入网络的压缩将成为今后深入研究的重点[]。

# 参考文献

1. P. Gangamohan， S.R. Kadiri， B. Yegnanarayana， Analysis of emotional speech-a review， Toward Robotic Socially Believable Behaving Systems-Volume I， (2016) 205-238. 



