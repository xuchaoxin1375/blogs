[toc]

# 概率基础不等式

## 马尔可夫不等式

- [Markov's inequality - Wikipedia](https://en.wikipedia.org/wiki/Markov's_inequality)
- 在[概率论](https://zh.wikipedia.org/wiki/概率论)中，**马尔可夫不等式**(英语：Markov's inequality)
  - 给出了[随机变量](https://zh.wikipedia.org/wiki/随机变量)的函数**大于等于某正数**的**概率**的**上界**。
  - X为一**非负**随机变量
    - $P(X\geqslant{a})\leqslant{\frac{E(X)}{a}}$
- 马尔可夫不等式把**概率**关联到[数学期望](https://zh.wikipedia.org/wiki/数学期望)，给出了**随机变量**的[累积分布函数](https://zh.wikipedia.org/wiki/累积分布函数)一个宽泛但仍有用的**界**。
  - 马尔可夫不等式的一个应用是，不超过1/5的人口会有超过5倍于人均收入的收入

### 推导

- 连续型随机变量(X为一非负随机变量)为例

- $$
  {
  \begin{aligned}{\textrm {E}}(X)
      		&=\int _{-\infty }^{\infty }xf(x)dx
     		 \\&=\int _{0}^{\infty }xf(x)dx
      \\[6pt]&\geqslant \int _{a}^{\infty }xf(x)dx
      \\[6pt]&\geqslant \int _{a}^{\infty }af(x)dx
      \\[6pt]&=a\int _{a}^{\infty }f(x)dx
      \\[6pt]&=a{\textrm {P}}(X\geqslant a).
  \end{aligned}
  }
  \\
  \therefore
  \boxed{P(X\geqslant{a})\leqslant{\frac{E(X)}{a}}}
  $$

  

  

#### 注

- 随机变量的由于非负性$(X\geqslant{0})$

  - 也就是说,随机变量X的取值(观测值)落在概率密度函数f(x)的负半轴区间的概率为0

  - 因此

    - $$
      f(x)=0,x\leqslant{0}
      \\
      \int_{-\infin}^{0}f(x)dx=0
      $$

- $$
  F(a)=\int_{-\infin}^{a}f(x)dx
  \\F(+\infin)=\int_{-\infin}^{+\infin}f(x)dx=1
  \\1-F(a)=\int_{a}^{+\infin}f(x)dx
  \\
  F(a)=P(X\leqslant{a})
  $$

- $$
  1-F(a)=1-P(X\leqslant{a})=P(X>a)
  \\
  P(X=a)=0
  \\
  \therefore
  P(X\geqslant{a})=P(X>a)+P(X=a)=1-F(a)=\int_{a}^{+\infin}f(x)dx
  \\\int_{a}^{+\infin}f(x)dx=P(X\geqslant{a})
  $$

  

## 切比雪夫不等式

- **chebyshev's inequality**
  
  - ref:[Chebychev's inequality and weak law of large numbers (CS 2800, Spring 2017) (cornell.edu)](https://www.cs.cornell.edu/courses/cs2800/2017sp/lectures/lec17-chebychev.html)
  - [Chebyshev's inequality - Wikipedia](https://en.wikipedia.org/wiki/Chebyshev's_inequality)
  
- 通过**方差**来估计:

    - <u>随机变量的取值和期望</u>之间的**偏差**大于某个正数$\varepsilon$的**概率**
        - $P(|X-E(X)|\geqslant\varepsilon)$

- chebyshev不等式给出了这个**概率的上界**$U(\varepsilon)$

    - 记$D(X)=\sigma^2$

    - $U(\varepsilon)=\frac{D(X)}{\varepsilon^2}=\frac{\sigma^2}{\varepsilon^2}$

- 设随机变量X的方差存在:($D(X)存在是chebyshev不等式作用的前提$)

  - 根据概率的规范性,可写出chebyshev不等式的两种形式:

  - $$
    P(|X-E(X)|\geqslant\varepsilon)
    \leqslant\frac{D(X)}{\varepsilon^2}
    $$

    - $$
      其中,\varepsilon>0
      \\根据概率的规范性:P({|X-E(X)|}<\varepsilon)=1-P({|X-E(X)|\geqslant{\varepsilon}}),
      \\可以写出另一形式:
      $$

- $$
  P( |X-E(X)|<\varepsilon)\geqslant{1-\frac{D(X)}{
  \varepsilon^2}}
  $$

### 推导

- 推导(连续型情况)

  - chebyshev inequality的推导

    - 在于利用事件$|X-E(X)|\geqslant\varepsilon>0$

      - $$
        \frac{|X-E(X)|}{\varepsilon}\geqslant{1}
        $$
  
    - 也可以借助Markov不等式推导
  
    - 从而:$|X-E(X)|{\varepsilon}\geqslant{1}$,利用这个不等式进行放缩被积函数
  
    - $$
      \frac{|X-E(X)|}{\varepsilon}\geqslant{1}
      \\\Rightarrow \frac{{|X-E(X)|^2}}{\varepsilon^2}\geqslant{1}
      \\
      记u=\frac{{|X-E(X)|^2}}{\varepsilon^2}
      \\
      uf(x)
      \geqslant{f(x)}
      \\
      将E(X)看作一个常数,\mu_X=E(X)
      \\(\sigma^2_{X}=D(X)=E(X^2)-E^2(X)相对于自变量x可以视为常数);
      \\(E(X)\&D(X)与随机变量X是函数关系)
      \\积分变量设为x
      \\积分区间用不等式表示:|x-E(X)|>\varepsilon
      $$
  
  
      $$
      P({|X-E(X)|}\geqslant\varepsilon)
      =\int\limits_{|x-E(X)|>\varepsilon}
      f(x)\mathrm{d}x
      \\\LARGE\leqslant{}
      \normalsize
      \int\limits_{|x-E(X)|>\varepsilon}
      \frac{{|x-E(X)|^2}}{\varepsilon^2}f(x)\mathrm{d}x
      \\\LARGE\leqslant{}
      \normalsize
      \int\limits_{x\in{R}}
      \frac{{(x-E(X))^2}}{\varepsilon^2}f(x)\mathrm{d}x
      \\Note:(此处,\int\limits_{x\in{R}}\Leftrightarrow \int_{-\infin}^{+\infin};\int\limits_{x\in{R}}
      f(x)\mathrm{d}x=1)
      \\=\frac{1}{\varepsilon^2}\int\limits_{x\in{R}}
      {(x-E(X))^2}f(x)\mathrm{d}x
      \\=\frac{1}{\varepsilon^2}D(X)
      \\上述证明中,对被积函数和被积分区域都进行了放缩
      $$
  
    - $$
      记g(X)=(X-E(X))^2=(X-\mu_X)^2,\mu_X=E(X)是常数
      \\则g(x)=(x-\mu_X)^2
      \\根据方差的展开公式(连续型)
      \\D(X)=E(g(X))
      =\int_{-\infin}^{+\infin}g(x)f(x)\mathrm{d}x
      \\=\int_{-\infin}^{+\infin}(x-E(X))^2f(x)\mathrm{d}x
      \\=\int_{-\infin}^{+\infin}(x-\mu_{X})^2f(x)\mathrm{d}x
      \\=\int_{x\in{R}}(x-\mu_{X})^2f(x)\mathrm{d}x
      $$
  
      

### 例

- 设随机变量$X_1,\cdots,X_n$独立同分布,且$X_i$的4阶矩存在

  - 设$\mu_k=E(X^k_i)(k=1,2,\cdots,4)$

  - 则由chebyshev不等式,对于$\forall \epsilon>0$,$Q=P(|\frac{1}{n}\sum\limits_{i=1}^{n}X_i^2-\mu_2)\geqslant{\epsilon}\leqslant{S=?}$

  - $$
    chebyshev不等式形式:
    \\P(|X-E(X)|\geqslant{\epsilon})\leqslant{\frac{D(X)}{\epsilon^2}}
    \\尝试令X=\overline{X_i^2}=\frac{1}{n}\sum\limits_{i=1}^{n}X_i^2
    \\E(X)=E(\overline{X_i^2})=\frac{1}{n}\sum\limits_{i=1}^{n}E(X_i^2)
    =\frac{1}{n}\sum\limits_{i=1}^{n}\mu_2=\frac{1}{n}(n\times{\mu_2})=\mu_{2}
    \\这恰好和Q的形式对应上了
    \\D(X)=D(\overline{X_i^2})=\frac{1}{n^2}\sum\limits_{i=1}^{n}D(X_i^2)
    \\D(X_i^2)=E((X_i^2)^2)-E^2(X_i^2)=\mu_4-\mu_2^2
    \\(或者另一条路:
    D(X)=E(X^2)-E^2(X)=E((\overline{X_i}^2)^2)\cdots,并不顺利)
    \\D(X)=\frac{1}{n^2}n(\mu_4-\mu_2^2)
    \\S=\frac{D(X)}{\epsilon^2}=\frac{\mu_4-\mu_2^2}{n\epsilon^2}
    $$

    

# 依概率收敛

## 定义

- $随机变量序列\set{X_i},i=1,2,\cdots$

  - A是一个常数

  - $\forall{\epsilon}>0$

  - $$
    \lim\limits_{n\to{\infin}}P(|X_n-A|<\epsilon)=1
    \\或:\lim\limits_{n\to{\infin}}P(|X_n-A|\geqslant\epsilon)=0
    \\则称\set{X_i}依赖概率收敛于常数A
    \\记为:X_n{\xrightarrow{P}}{A}
    \\或(X_n-A{\xrightarrow{P}}{0})
    $$
    
    
    
  - $$
    特别的,当A=0的时候
    \\\lim\limits_{n\to{\infin}}P(|X_n|<\epsilon)=1
    或\\
    \lim\limits_{n\to{\infin}}P(|X_n|\geqslant\epsilon)=0
    则称\set{X_i}依赖概率收敛于0:
    \\X_n\xrightarrow{P}0
    $$
  
    - 从极限的角度,也就是说符号$\xrightarrow{P}{A}表示依概率收敛于A;P表示概率Probability$
    
      

### 直观解释

- 以概率收敛的直观解释:
  - $\forall{\epsilon}>0,n充分大的时候,X_n与X的偏差小于\epsilon$
  - 描述的是在概率意义下的收敛性
    - $当n很大的时候,我们有很大的把握可以保证X_n与X很接近(要多接近有多接近)$


### 特点

- 从形式上看,依概率收敛的定义中,被求极限的概率表达式部分:$P(|X_n-A|\geqslant\epsilon)$很符合chebyshev不等式中的形式

  - $$
    P(|X-E(X)|\geqslant\varepsilon)
    \leqslant\frac{D(X)}{\varepsilon^2}
    \\
    \lim\limits_{n\to{\infin}}P(|X_n-A|\geqslant\epsilon)=0
    $$

    

## 服从大数定律

- $如果\set{X_n|n=1,2,\cdots}是一列随机变量序列$

  - $\set{a_n}是一列实数序列$

  - $如果存在某个\set{a_n},使得:$

    - $$
      S=S(n,\set{X_n})=\frac{1}{n}(\sum\limits_{i=1}^{n}X_i)-a_n
      \xrightarrow{P}0
      \\
      \lim\limits_{n\to{\infin}}P({S})=\lim\limits_{n\to{\infin}}P(|(\frac{1}{n}\sum\limits_{i=1}^{n}X_i)-a_n|\geqslant\epsilon)=0
      $$

      

  - 则称$\set{X_n}$服从大数定律

    

    

# 大数定律

- Law of large numbers(LLN)

- 事件A发生的频率具有稳定性:

  - 当试验次数n增大,频率将稳定于某一个常数(这个常数就A发生的概率:P(A))

  - 例如:做测量的时候,重复测量n次,得到的数值分别记为$X_1,X_2,\cdots,X_n$

    - $可以将\set{X_i},i=1,2,\cdots,n视为n个独立同分布的随机变量$

      - 一般测量试验的随机变量服从正态分布:设为$X\sim{N(\mu,\sigma^2)}$
      - $X_i之间的的数学期望和方差都是一致的,分别为\mu,\sigma$
      - $E(\overline{X})=E(X_i)=\mu$
      - $D(\overline{X})=\frac{1}{n}\sigma^2$
        - 从这个角度上看,当n充分大的时候,方差趋近于0
        - $并且\overline{X}会稳定于它的数学期望\mu$,体现的是大量试验中平均结果的**稳定性**

    - 下面是推导过程:

      - $将这n个随机变量的算数平均值记为\overline{X}=\frac{1}{n}\sum\limits_{i=1}^{n}X_i$

      - $\overline{X}也视为一个随机变量$

      - $$
        E(\overline{X})
        =\frac{1}{n}E(\sum\limits_{i=1}^{n}X_i)
        =\frac{1}{n}\sum_{i=1}^{n}E(X_i)=\frac{1}{n}n\mu
        =\mu
        \\
        D(\overline{X})=E(\overline{X}^2)-E^2(\overline{X})
        \\=E((\frac{1}{n}\sum\limits_{i=1}^{n}X_i)^2)-\mu^2
        \\=\frac{1}{n^2}E((\sum\limits_{i=1}^{n}X_i)^2)-\mu^2
        $$

        

      - $$
        (\sum\limits_{i=1}^{n}X_i)^2
        =\sum\limits_{i=1}^{n}\sum\limits_{j=1}^{n}X_iX_j
        \\=\sum\limits_{i=1}^{n}\sum\limits_{
        \begin{aligned}
        j=1
        \\
        j\neq{i}
        \end{aligned}
        }^{n}X_iX_j
        +\sum\limits_{i=1}^{n}X_i^2
        $$

        - $$
          D(X_i)=\sigma^2=E(X_i^2)-E^2(X_i)=E(X_i^2)-\mu^2
          \\E(X_i^2)=\sigma^2+\mu^2
          $$

        - $$
          E(\sum\limits_{i=1}^{n}\sum\limits_{
          \begin{aligned}
          j=1
          \\
          j\neq{i}
          \end{aligned}
          }^{n}X_iX_j
          +\sum\limits_{i=1}^{n}X_i^2)
          \\=\sum\limits_{i=1}^{n}\sum\limits_{
          \begin{aligned}
          j=1
          \\
          j\neq{i}
          \end{aligned}
          }^{n}E(X_iX_j)
          +\sum\limits_{i=1}^{n}E(X_i^2)
          \\=\sum\limits_{i=1}^{n}\sum\limits_{
          \begin{aligned}
          j=1
          \\
          j\neq{i}
          \end{aligned}
          }^{n}E(X_i)E(X_j)
          +\sum\limits_{i=1}^{n}E(X_i^2)
          \\=\sum\limits_{i=1}^{n}\sum\limits_{
          \begin{aligned}
          j=1
          \\
          j\neq{i}
          \end{aligned}
          }^{n}\mu^2
          +n(\mu^2+\sigma^2)
          \\=n^2(\mu^2)+n\sigma^2
          $$
        
      - $$
        E(\overline{X})
        =\frac{1}{n^2}E((\sum\limits_{i=1}^{n}X_i)^2)-\mu^2
        \\=\frac{1}{n^2}(n^2\mu^2+n\sigma^2)-\mu^2
        \\=\mu^2+\frac{1}{n}\sigma-\mu^2
        \\=\frac{1}{n}\sigma^2
        $$
    
  - 前面说到的大量试验中平均结果的稳定性,

    - 用大数定律,以严格的数学语言表达了随机现象在大量试验中所呈现出的统计规律性
      - 频率的稳定性
      - 平均结果的稳定性

## chebyshev LLN

- $$
  记\overline{X}=\frac{1}{n}\sum_{i=1}^{n}X_i;
  \\E(\overline{X})=E(\frac{1}{n}\sum_{i=1}^{n}X_i)
  =\frac{1}{n}E(\sum_{i=1}^{n}X_i)=\frac{1}{n}\sum_{i=1}^{n}E(X_i)
  \\
  \overline{E(X)}=\frac{1}{n}\sum_{i=1}^{n}E(X_i)
  \\E(\overline{X})=\overline{E(X)}
  $$

- chebyshevLLN可以描述为:

  - 
    $$
    \set{X_n|n=1,2,\cdots},是一列\underline{相互独立}的随机变量序列
    \\\text{if }\exist{C>0},\text{s.t.}\forall{i},D(X_i)\leqslant{C},
    \\即(max(D(X_i))\leqslant{C})
    \\\Rightarrow\forall \epsilon>0:
    \\
    $$


  - $$
    \lim_{n\to{\infin}}P(|\overline{X}-\overline{E(X)}|\geqslant{\epsilon})=0
    \\即:\overline{X}-\overline{E(X)}
    =\overline{X}-E(\overline{X})
    \geqslant{\epsilon}\xrightarrow{P}{0}
    \\
    \overline{X}\xrightarrow{P}\overline{E(X)}
    $$


- 推导:(可以由chebyshev不等式进行推导)

  - $$
    由独立性可知:
    \\D(\overline{X})=\frac{1}{n^2}D(\sum_{i=1}^nX_i)
    =\frac{1}{n^2}\sum_{i=1}^nD(X_i)
    \leqslant{\frac{1}{n^2}nC=\frac{C}{n}}
    $$

    

  - $$
    由chebyshev不等式:
    \\P(|X-E(X)|\geqslant\varepsilon)
    \leqslant\frac{D(X)}{\varepsilon^2}
    \\P(|\overline{X}-E(\overline{X})|\geqslant\varepsilon)
    \leqslant\frac{D(\overline{X})}{\varepsilon^2}
    \leqslant\frac{1}{\epsilon^2}\frac{C}{n}
    \xrightarrow{n\to{\infin}}{0}
    \\经过上面的放缩从而得到证明
    \\
    \lim_{n\to{\infin}}P(|\overline{X}-E(\overline{X})|\geqslant\varepsilon)
    =0
    $$

    

### 应用

- 回到前面提到的多次测量取平均值的期望和方差问题:

  - $$
    X_i独立同分布
    \\
    E(X_i)=\mu;
    \\D(X_i)=\sigma^2
    \\\overline{E(X)}=\frac{1}{n}\sum_{i=1}^{n}E(X_i)=\frac{1}{n}n(\mu)=\mu
    $$
    
    - $$
      n个观察值的算数平均值:\overline{X}=\frac{1}{n}\sum\limits_{i=1}^{n}X_i
      \\\mu作为被测物的指标真值,每次测量(X_i)的最理想值是同一个真值
      $$
      

- 当试验次数n趋于无穷大的时候,实际测量值的算数平均值$\overline{X}$依概率收敛于真值$\mu$

  - 揭示了平均结果具有稳定性
  - 测量中,常用多次重复测量所得到的观测值的算数平均值作为被测量值的近似值


## bernoulli LLN

- 和chebyshevLLN的区别
  - bernoulliLLN主要针对n重bernoulli试验的
  - chebyshevLLN的使用范围更广泛,但是精度有限
  - khinchinLLN试图进一步削弱定理运用条件

- 🎈$设X_n是n次独立试验中事件A发生的次数$

  - 在每次试验中,事件A发生的概率是$p,p\in(0,1)$

    - $则:X_n\sim{B(n,p)}$

    - $E(X_n)=np,D(X_n)=np(1-q)$

  - $\forall \epsilon>0:$

    - $$
      \lim_{n\to{\infin}}P(|\frac{X_n}{n}-p|\geqslant{\epsilon})=0
      \\即\frac{1}{n}X_n\to{p}
      $$

  - 推导:

    - $$
      由chebyshev 不等式:
      \\P(|X-E(X)|\geqslant{\epsilon})\leqslant{\frac{1}{\epsilon^2}{D(X)}}
      \\P(|\frac{1}{n}X_n-E(\frac{1}{n}X_n)|\geqslant{\epsilon})\leqslant{\frac{1}{\epsilon^2}{D(\frac{1}{n}X_n)}}
      \\P(|\frac{1}{n}X_n-\frac{1}{n}E(X_n)|\geqslant{\epsilon})\leqslant{\frac{1}{\epsilon^2}{\frac{1}{n^2}D(X_n)}}
      \\P(|\frac{1}{n}X_n-\frac{1}{n}np|\geqslant{\epsilon})\leqslant{\frac{1}{\epsilon^2}{\frac{1}{n^2}np(1-p)}}
      \\P(|\frac{1}{n}X_n- p|\geqslant{\epsilon})\leqslant{\frac{1}{\epsilon^2}{\frac{1}{n}p(1-p)}}
      \xrightarrow{n\to{\infin}}{0}
      \\即\lim_{n\to{\infin}}P(|\frac{1}{n}X_n- p|\geqslant{\epsilon})=0
      \\\frac{1}{n}X_n\xrightarrow{P}p\quad (n\to{\infin})
      $$

### 意义

- bernoulli LLN揭示了**频率与概率**之间的关系
  - 推导过程中的$X_n$就表示的频率
  - 当试验条件不变的时候,多次重复试验中,**随机事件出现的频率$\frac{1}{n}{X_n}$**将依概率收敛于**随机事件的概率p**
  - 从而,以频率估计(接近)概率的这一直观经验有了严格的数学意义
  - 也就是频率的稳定性在理论上得到证明
  - 是实践中,用频率估计概率的依据

## Khinchin LLN

- 辛钦大数定律告诉我们,chebyshev LLN中要求的随机变量序列$\set{X_n|n=1,2,\cdots}$相互独立这条件在某些情况下是多余的

- 样本数量越多，则其[算术平均值](https://zh.wikipedia.org/wiki/算术平均值)就有越高的概率接近[期望值](https://zh.wikipedia.org/wiki/期望值)。

- 相比于chebyshev大数定律,具有更广的使用范围(证明需要专业知识)

  - 设$\set{X_n|n=1,2,\cdots}$是独立同分布的随机变量序列

    - $记:\overline{X}=\frac{1}{n}\sum\limits_{i=1}^{n}X_i$

    - $E(X_n)=\mu存在$

    - $\forall{\epsilon}>0:$

    - $$
      \lim_{n\to{\infin}}P(|\frac{1}n\sum_{i=1}^{n}X_i-\mu|\geqslant{\epsilon})=0
      \\
      \lim_{n\to{\infin}}
      P(|\overline{X}
      -\mu|\geqslant{\epsilon})=0
      \\\overline{X}\xrightarrow{P}\mu\quad(n\to\infin)
      $$

- 揭示了n足够大的时候,可以用各次试验结果对应的随机变量$X_n,n=1,2,\cdots$的均值$\overline{X}$估计期望$\mu$



# 总结

- $$
  弱大数定律
  \\
  弱大数定律(WLLN) 也称为辛钦定理，陈述为：样本均值依概率收敛于期望值
  \\
   {\displaystyle {\overline {X}}_{n}\ {\xrightarrow {P}}\ \mu \quad {\textrm {as}}\quad n\to \infty }
  \\也就是说对于任意正数 ε,
  \\
  {\displaystyle \lim _{n\to \infty }P\left(\,|{\overline {X}}_{n}-\mu |>\varepsilon \,\right)=0}
  $$

  

- $$
  强大数定律
  \\
  强大数定律(SLLN)指出，样本均值以概率1收敛于期望值。
  \\
   {\displaystyle {\overline {X}}_{n}\ {\xrightarrow {\text{a.s.}}}\ \mu \quad {\textrm {as}}\quad n\to \infty }
  即
  \\
  {\displaystyle P\left(\lim _{n\to \infty }{\overline {X}}_{n}=\mu \right)=1 }
  $$

  



- $$
  切比雪夫定理的特殊情况
  设  {\displaystyle a_{1},\ a_{2},\ \dots \ ,\ a_{n},\ \dots } 为相互独立的随机变量，
  \\其数学期望为： {\displaystyle \operatorname {E} (a_{i})=\mu \quad (i=1,\ 2,\ \dots )}，
  \\方差为： {\displaystyle \operatorname {Var} (a_{i})=\sigma ^{2}\quad (i=1,\ 2,\ \dots )}
  \\
  则序列{\displaystyle {\overline {a}}={\frac {1}{n}}\sum _{i=1}^{n}a_{i}} 依概率收敛于 \mu 
  \\（即收敛于此数列的数学期望E(a_{i})。
  \\
  换言之，在定理条件下，当 n无限变大时， n个随机变量的算术平均将变成一个常数。
  \\
  $$

  



- $$
  伯努利大数定律
  设在{\displaystyle n}次独立重复伯努利试验中，
  \\事件{\displaystyle X}发生的次数为 n_{x}，事件{\displaystyle X}在每次试验中发生的总体概率为 p，
  \\{\displaystyle {\frac {n_{x}}{n}}} 代表样本发生事件 X的频率。
  \\
  则对任意正数 \varepsilon >0，伯努利大数定律表明：
  \\
  {\displaystyle \lim _{n\to \infty }{P{\left\{\left|{\frac {n_{x}}{n}}-p\right|<\varepsilon \right\}}}=1}
  $$

  

  - 换言之，事件发生的频率依概率收敛于事件的总体概率。
    该定理以严格的数学形式表达了频率的稳定性，也就是说当${\displaystyle n}$很大时，事件发生的频率于总体概率有较大偏差的可能性很小



