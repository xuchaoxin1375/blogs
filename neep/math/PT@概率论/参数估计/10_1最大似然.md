[toc]

## 最大似然估计

- **likelihood**(似然)

- $设样本X_1,X_2,\cdots,X_n来自总体X, x_1,x_2,\cdots,x_n是样本值,\theta是待估计值$

### 最大似然思想

- 例:
  - A,B箱子均有100个球,A有99个白球,B只有1个白球
  - 现在随机从A,B重抽取一个,发现是白球,称为白球$\alpha$
  - 这个白球$\alpha$更可能来自那个箱子?
    - 从直观上,应该来自于A箱子
      - 应为从A中抽取白球的概率比从B中抽出白球的概率要大
    - 白球来自于A更好的解释了抽中白球的事实
    - 设
      - $\theta_1$表示白球$\alpha$来自于A
      - $\theta_2$表示白球$\alpha$来自于B
      - 如果$\theta_1,\theta_2$都是概率分布函数参数的估计,那么我们认为$\theta_1$更加合适
- 最大似然是要从给定的事实出发,寻找一个能最好解释该事实的参数
  - 通过观察样本值$x_1,x_2,\cdots,x_n$,在参数$\theta$所有可能取值中寻找一个看起来最好解释了该事实的那一个



#### 似然函数

- 从下面的似然函数(似然方程)的定义中可以看到,它们是函数连乘的形式

  - 另一方面,由于自然对数$\ln{x}$单调递增函数,所以$L(\theta)与\ln{L(\theta)}$在同一个地方取得最大值

  - 意味着,求$\ln{L(\theta)}$可以被分解加法的形式,使得求解计算过程更加容易

    - $$
      例如:
      \\
      \ln{L(\theta)}=\ln{\prod_{i=1}^{n}f(x_i;\theta)}=\sum\limits_{i=1}^{n}\ln{f(x_i;\theta)}
      \\
      \frac{\mathrm{d}}{\mathrm{d}\theta}\ln(L(\theta))
      =\sum\limits_{i=1}^{n}\frac{1}{f(x_i;\theta)}(\frac{\mathrm{d}}{\mathrm{d}x}f(x_i;\theta))
      $$

      

      - 上面这个导数(求和形式)形式的似然函数可以直接使用

        - 注意符合函数的求导

        - 注意,似然函数的自变量是参数($\theta$),而不是$x_i$

        - 虽然$x_i$在最大似然估计中不是自变量,但是由于和累乘/累加($\sum,\prod$)相挂钩,不可以视为一般的常数提取出($\sum,\prod$)

          - 可以称$x_i$等带有遍历变量的表达式称为遍历表达式(通项)

          - 而且建议使用字母A,B,C,来简化累乘/累积部分书写

            

        

      

- 离散型总体

  - $设总体为X,其概率分布为P(X=a_i)=p(a_i;\theta),i=1,2,\cdots$

    - $$
      L(\theta)=L( x_1,x_2,\cdots,x_n;\theta)=\prod_{i=1}^{n}p(x_i;\theta)
      \\为参数\theta的似然函数
      $$

      

- 连续型总体

  - 设总体为X,其概率密度为$f(x;\theta)$

    - $$
      L(\theta)=L( x_1,x_2,\cdots,x_n;\theta)=\prod_{i=1}^{n}f(x_i;\theta)
      \\为参数\theta的似然函数
      $$

- $对于似然函数L(\theta)=L(\theta;x)$

  - 如果
    $$
    对于已有观测值(事实):s_0=x_1,x_2,\cdots,x_n
    \\
    L(x_1,x_2,\cdots,x_n;\theta_1)>L(x_1,x_2,\cdots,x_n;\theta_2)
    \\则认为\theta_1比\theta_2(看上去)能够更好的解释给定事实s_0
    $$
    

  - $似然函数L(\theta)刻画了:当样本观察值(事实)为s_0时,参数值取\theta的可能性大小$

- 当试验结果为$x_1,x_2,\cdots,x_n$的时候,导致该结果发生的最大似然函数值应该是$L(\theta)取最值L_{Max}$

#### 似然方程

- 似然方程一侧为似然函数(或者似然函数对数)的导数,另一侧为0
  - 是为了求的驻点!

- $$
  \frac{\mathrm{d}}{\mathrm{d}\theta}L(\theta)=0
  或
  \\
  \frac{\mathrm{d}}{\mathrm{d}\theta}\ln{L(\theta)}=0
  $$

  

- $$
  双参数方程
  \\\frac{\partial}{\partial\theta_i}L(\theta_i)=0;i=1,2
  \\
  或
  \\
  \frac{\partial}{\partial\theta_i }\ln{L(\theta_i)}=0;i=1,2
  $$

  



### 最大似然估计法

- 对于给定的样本值$ x_1,x_2,\cdots,x_n$,使得极大似然函数$L(\theta)=L( x_1,x_2,\cdots,x_n;\theta)$达到最大值的参数值$\hat{\theta}=\hat{\theta}(x_1,x_2,\cdots,x_n)$,称为未知参数$\theta$的**最大似然估计值**;

  - $$
    L(\hat\theta)=max(L(\theta));\theta\in{\Theta}
    \\\Theta为所有的\theta可能取值
    $$

    

  - 相应的,$\hat{\theta}=\hat{\theta}(X_1,X_2,\cdots,X_n)$称为**最大似然估计量**



#### 步骤

- 确定分布律或者概率密度
- 建立似然方程

- 如果$L(\theta)或者\ln{L(\theta)}$关于$\theta$可微,值$\hat{\theta}$往往可以从**似然方程**中求解:

  - 似然方程不总是有效的:

    - 使得$L(\theta)或\ln{L(\theta)}$达到最大值的$\hat{\theta}$不一定是驻点,

      - 也就是说,驻点值未必是满足最大似然条件的值

        这种情况下,需要另寻它法求解最大似然估计

### 案例(最大似然法)

#### 离散型实例

- $$
  设总体X\sim{P(\lambda)};\lambda>0是位置参数
  \\(X_1,X_2,\cdots,X_n)是X的样本
  \\x_1,x_2,\cdots,x_2是样本的观察值
  \\求\lambda的最大似然估计\hat\lambda
  $$

  

  - $$
    由X\sim{P(\lambda)}可知:
    \\分布律:p(x;\lambda)=P(X=x)=\frac{\lambda^x}{x!}e^{-\lambda};x=0,1,2\cdots
    $$

    

  - 从似然函数取对数到对数似然方程

    - $$
      \\似然函数:
      \\L(\lambda)=\prod_{i=1}^{n}p(x_i;\theta)
      =\prod_{i=1}^{n}\frac{1}{x_i!}\lambda^{x_i}e^{-\lambda}
      =\frac{1}{\prod\limits_{i=1}^{n}x_i!}\Large\lambda^{\scriptsize\sum\limits_{i=1}^{n}x_i}e^{-n\lambda}
      \\= ({\prod\limits_{i=1}^{n}(x_i!)^{-1}})\Large\lambda^{\scriptsize\sum\limits_{i=1}^{n}x_i}e^{-n\lambda}
      \\记A=\sum\limits_{i=1}^{n}x_i;B=\prod\limits_{i=1}^{n}x_i!
      \\则L(\lambda)=B^{-1}\lambda^Ae^{-n\lambda}
      \\\\
      \ln{L(\lambda)}=-\ln{B}+A\ln{\lambda}-n\lambda
      \\
      \frac{\mathrm{d}}{\mathrm{d}\lambda}\ln{L(\lambda)}
      =A\lambda^{-1}-n
      $$

    

  - 建立方程:

    - $$
      \frac{\mathrm{d}}{\mathrm{d}\lambda}\ln{L(\lambda)}=0
      \\A\lambda^{-1}-n=0
      \\
      \lambda=(nA^{-1})^{-1}=n^{-1}A=n^{-1}\sum\limits_{i=1}^{n}x_i=\overline{X}
      $$

    - 直接建立方程:

      - $$
        \frac{\mathrm{d}}{\mathrm{d}\lambda}\ln{L(\lambda)}=0
        \\
        =\sum\limits_{i=1}^{n}\frac{1}{f(x_i;\theta)}(\frac{\mathrm{d}}{\mathrm{d}x}f(x_i;\theta))
        \\=\sum\limits_{i=1}^{n}(\frac{1}{x_i!}\lambda^{x_i}e^{-\lambda})^{-1}
        \frac{1}{x_i!}(x_i\lambda^{x_i-1}e^{-\lambda}+\lambda^{x_i}e^{-\lambda}(-1))
        \\=\sum\limits_{i=1}^{n}\lambda^{-x_i}(x_i\lambda^{x_i-1}-\lambda^{x_i})
        =\sum\limits_{i=1}^{n}(x_i\lambda^{-1}-1)
        =0
        \\
        \lambda^{-1}\sum_{i=1}^{n}x_i-n=0
        \\
        \lambda=n^{-1}\sum_{i=1}^{n}x_i=\overline{X}
        $$

        

  - 结果:

    - $\\极大似然估计为\hat{\lambda}=\overline{X}$

    

#### 连续型实例

- $$
  设总体X\sim{N(\mu,\sigma^2)},(X_1,X_2,\cdots,X_n)来自总体X的样本
  \\(x_1,x_2,\cdots,x_n)是样本观察值
  \\求未知参数\mu,\sigma^2的最大似然
  $$

  

  - $$
    此处未知参数有两个,令向量:\theta=(\mu,\sigma^2)
    \\记f(x_i;\theta)=f(x_i;(\mu,\sigma^2))
    =(\sqrt{2\pi}\sigma)^{-1}e^{-\frac{1}{2}\frac{(x_i-\mu)^2}{\sigma^2}}
    \\
    \ln{L(\theta)}=L(\mu,\sigma^2)=\prod_{i=1}^{n}f(x_i;\theta)
    =(\sqrt{2\pi}\sigma)^{-n}\prod_{i=1}^{n}e^{-\frac{1}{2}\frac{(x_i-\mu)^2}{\sigma^2}}
    \\=(\sqrt{2\pi}\sigma)^{-n}  e^{-\frac{1}{2}(\sum\limits_{i=1}^{n}\frac{(x_i-\mu)^2}{\sigma^2})}
    \\取对数
    \\
    \ln{L(\theta)}
    =-n\ln({\sqrt{2\pi}\sigma})-\frac{1}{2}(\sum\limits_{i=1}^{n}\frac{(x_i-\mu)^2}{\sigma^2})
    \\=-n\ln({\sqrt{2\pi}})-n\ln{\sigma}-\frac{1}{2}(\sum\limits_{i=1}^{n}\frac{(x_i-\mu)^2}{\sigma^2})
    \\=-n\ln({\sqrt{2\pi}})-\frac{1}{2}n\ln{\sigma^2}-\frac{1}{2\sigma^2}(\sum\limits_{i=1}^{n}(x_i-\mu)^2)
    $$

    

  - 求出驻点(求导,多个未知量求偏导)

    - $$
      \frac{\partial}{\partial\theta_i }\ln{L(\theta_i)}=0;i=1,2
      \\\\
      \frac{\partial}{\partial\mu }\ln{L(\theta)}=0;i=1,2
      \\\frac{\partial}{\partial\mu }\ln{L(\theta)}=0+0-\frac{1}{2\sigma^2}(\sum\limits_{i=1}^{n}2(x_i-\mu)(-1))
      \\=\frac{1}{\sigma^2}\sum\limits_{i=1}^{n}(x_i-\mu)=0
      \\
      \sum\limits_{i=1}^{n}(x_i-\mu)=0
      \\
      \sum\limits_{i=1}^{n}x_i-\sum\limits_{i=1}^{n}\mu=0
      \\
      \mu=\frac{1}{n}\sum\limits_{i=1}^{n}x_i=\overline{X}
      $$

      

    - $$
      记A=\sum\limits_{i=1}^{n}(x_i-\mu)^2
      \\
      \frac{\partial}{\partial\sigma^2 }\ln{L(\theta)}
      =A\sigma^{-3}-\frac{1}{\sigma}n=0
      \\\sigma^2=\frac{1}{n}A
      =\frac{1}{n}\sum\limits_{i=1}^{n}(x_i-\mu)^2
      =\frac{1}{n}\sum\limits_{i=1}^{n}(x_i-\overline{X})^2
      $$

      

    

  

  - 结果:
    $$
    \hat{\mu}=\overline{X}
    \\
    \hat{\sigma^2}=\frac{1}{n}\sum\limits_{i=1}^{n}(x_i-\overline{X})^2
    $$
    

