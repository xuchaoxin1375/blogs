

[toc]

## ref

- `<<DeepLearning>>`:Deep Learning (Adaptive Computation and Machine Learning series)
- [Principal component analysis - Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis)
- [ä¸»æˆåˆ†åˆ†æ -(wikipedia.org)](https://zh.wikipedia.org/wiki/ä¸»æˆåˆ†åˆ†æ)

## ä¸»æˆåˆ†åˆ†æPCA

PCAï¼ˆPrincipal Component Analysisï¼‰æ˜¯ä¸€ç§å¸¸ç”¨çš„æ•°æ®é™ç»´ç®—æ³•ï¼Œå®ƒå¯ä»¥å°†é«˜ç»´æ•°æ®é™ä½åˆ°ä½ç»´ï¼ŒåŒæ—¶ä¿ç•™æ•°æ®çš„ä¸»è¦ç‰¹å¾ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬ç»å¸¸ä¼šé‡åˆ°æ•°æ®ç»´åº¦å¾ˆé«˜çš„æƒ…å†µï¼Œè¿™æ—¶å€™æˆ‘ä»¬éœ€è¦é™ç»´æ¥å‡å°‘è®¡ç®—é‡ï¼Œæé«˜ç®—æ³•æ•ˆç‡ã€‚

PCAçš„åŸºæœ¬æ€æƒ³æ˜¯<u>æŠŠåŸå§‹æ•°æ®æŠ•å½±åˆ°ä¸€ä¸ªæ–°çš„åæ ‡ç³»ä¸­ï¼Œä½¿å¾—æŠ•å½±åçš„æ•°æ®æ–¹å·®æœ€å¤§</u>ã€‚

- åœ¨æ–°çš„åæ ‡ç³»ä¸­ï¼Œ**ç¬¬ä¸€ä¸ªåæ ‡è½´**è¢«ç§°ä¸ºç¬¬ä¸€ä¸»æˆåˆ†ï¼Œæ˜¯åŸå§‹æ•°æ®ä¸­æ–¹å·®æœ€å¤§çš„æ–¹å‘ï¼›
- **ç¬¬äºŒä¸ªåæ ‡è½´**è¢«ç§°ä¸ºç¬¬äºŒä¸»æˆåˆ†ï¼Œæ˜¯ä¸ç¬¬ä¸€ä¸»æˆåˆ†å‚ç›´çš„æ–¹å‘ä¸­æ–¹å·®æœ€å¤§çš„ã€‚

é€šè¿‡ä¸æ–­æ‰¾åˆ°æ–¹å·®æœ€å¤§çš„åæ ‡è½´ï¼Œæˆ‘ä»¬å°±å¯ä»¥å¾—åˆ°ä¸€ç»„**æ–°çš„ç‰¹å¾**ï¼Œç”¨äºæè¿°æ•°æ®ã€‚

PCAç®—æ³•é€šè¿‡ä»¥ä¸‹æ­¥éª¤å®ç°æ•°æ®é™ç»´ï¼š

1. å¯¹æ•°æ®è¿›è¡Œä¸­å¿ƒåŒ–ï¼Œå³å°†æ¯ä¸ªç‰¹å¾çš„**å‡å€¼**ç§»åŠ¨åˆ°åŸç‚¹ã€‚
2. è®¡ç®—æ•°æ®çš„**åæ–¹å·®çŸ©é˜µ**ã€‚
3. å¯¹åæ–¹å·®çŸ©é˜µè¿›è¡Œ**ç‰¹å¾å€¼åˆ†è§£**ï¼Œå¾—åˆ°ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ã€‚
4. å°†**ç‰¹å¾å‘é‡**æŒ‰ç…§å¯¹åº”**ç‰¹å¾å€¼å¤§å°**é™åºæ’åˆ—ï¼Œé€‰æ‹©**å‰kä¸ªç‰¹å¾å‘é‡ä½œä¸ºæ–°çš„åæ ‡ç³»**ã€‚
5. å°†æ•°æ®æŠ•å½±åˆ°æ–°çš„åæ ‡ç³»ä¸­ï¼Œå¾—åˆ°é™ç»´åçš„æ•°æ®ã€‚

- PCAç®—æ³•çš„è¾“å‡ºåŒ…æ‹¬**é™ç»´åçš„æ•°æ®**å’Œ**æ¯ä¸ªä¸»æˆåˆ†è§£é‡Šçš„æ–¹å·®æ¯”ä¾‹**ã€‚
- é€šè¿‡æ–¹å·®æ¯”ä¾‹ï¼Œæˆ‘ä»¬å¯ä»¥**è¯„ä¼°æ¯ä¸ªä¸»æˆåˆ†çš„é‡è¦æ€§**ï¼Œç¡®å®šåº”è¯¥ä¿ç•™çš„**ä¸»æˆåˆ†æ•°é‡**ã€‚

æ€»ä¹‹ï¼ŒPCAç®—æ³•æ˜¯ä¸€ç§éå¸¸æœ‰ç”¨çš„æ•°æ®é™ç»´æ–¹æ³•ï¼Œå¯ä»¥å¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°ç†è§£å’Œåˆ©ç”¨é«˜ç»´æ•°æ®ã€‚

### æ•°å­¦ä¸­çš„PCAæ–¹æ³•

- åœ¨å¤šå…ƒç»Ÿè®¡åˆ†æä¸­ï¼Œ**ä¸»æˆåˆ†åˆ†æ**ï¼ˆè‹±è¯­ï¼š**Principal components analysis**ï¼Œ**PCA**ï¼‰æ˜¯ä¸€ç§ç»Ÿè®¡åˆ†æã€ç®€åŒ–æ•°æ®é›†çš„æ–¹æ³•ã€‚
- å®ƒåˆ©ç”¨[æ­£äº¤å˜æ¢](https://zh.wikipedia.org/wiki/æ­£äº¤å˜æ¢)æ¥å¯¹ä¸€ç³»åˆ—å¯èƒ½ç›¸å…³çš„å˜é‡çš„è§‚æµ‹å€¼è¿›è¡Œçº¿æ€§å˜æ¢ï¼Œä»è€ŒæŠ•å½±ä¸ºä¸€ç³»åˆ—**çº¿æ€§ä¸ç›¸å…³å˜é‡çš„å€¼**ï¼Œè¿™äº›ä¸ç›¸å…³å˜é‡ç§°ä¸º**ä¸»æˆåˆ†**ï¼ˆPrincipal Componentsï¼‰ã€‚
- å…·ä½“åœ°ï¼Œ<u>ä¸»æˆåˆ†å¯ä»¥çœ‹åšä¸€ä¸ªçº¿æ€§æ–¹ç¨‹</u>ï¼Œå…¶åŒ…å«ä¸€ç³»åˆ—**çº¿æ€§ç³»æ•°**æ¥æŒ‡ç¤º**æŠ•å½±æ–¹å‘**ã€‚
- PCAå¯¹åŸå§‹æ•°æ®çš„æ­£åˆ™åŒ–æˆ–é¢„å¤„ç†æ•æ„Ÿï¼ˆç›¸å¯¹ç¼©æ”¾ï¼‰ã€‚

- **åŸºæœ¬æ€æƒ³ï¼š**

  - å°†åæ ‡è½´ä¸­å¿ƒç§»åˆ°æ•°æ®çš„ä¸­å¿ƒï¼Œç„¶åæ—‹è½¬åæ ‡è½´ï¼Œä½¿å¾—<u>æ•°æ®åœ¨C1è½´ä¸Šçš„æ–¹å·®æœ€å¤§</u>ï¼Œå³å…¨éƒ¨nä¸ªæ•°æ®ä¸ªä½“åœ¨è¯¥æ–¹å‘ä¸Šçš„**æŠ•å½±æœ€ä¸ºåˆ†æ•£**ã€‚æ„å‘³ç€æ›´å¤šçš„ä¿¡æ¯è¢«ä¿ç•™ä¸‹æ¥ã€‚C1æˆä¸º**ç¬¬ä¸€ä¸»æˆåˆ†**ã€‚

  - C2**ç¬¬äºŒä¸»æˆåˆ†**ï¼šæ‰¾ä¸€ä¸ªC2ï¼Œä½¿å¾—C2ä¸C1çš„åæ–¹å·®ï¼ˆç›¸å…³ç³»æ•°ï¼‰ä¸º0ï¼Œä»¥å…ä¸C1**ä¿¡æ¯é‡å **ï¼Œå¹¶ä¸”ä½¿æ•°æ®åœ¨è¯¥æ–¹å‘çš„**æ–¹å·®å°½é‡æœ€å¤§**ã€‚

  - ä»¥æ­¤ç±»æ¨ï¼Œæ‰¾åˆ°ç¬¬ä¸‰ä¸»æˆåˆ†ï¼Œç¬¬å››ä¸»æˆåˆ†â€¦â€¦ç¬¬pä¸ªä¸»æˆåˆ†ã€‚
  - pä¸ªéšæœºå˜é‡å¯ä»¥æœ‰pä¸ªä¸»æˆåˆ†.

- ä¸»æˆåˆ†åˆ†æç»å¸¸ç”¨äºå‡å°‘æ•°æ®é›†çš„[ç»´æ•°](https://zh.wikipedia.org/wiki/ç»´æ•°)ï¼ŒåŒæ—¶ä¿ç•™æ•°æ®é›†å½“ä¸­**å¯¹æ–¹å·®è´¡çŒ®æœ€å¤§çš„ç‰¹å¾**ã€‚
  - è¿™æ˜¯é€šè¿‡<u>ä¿ç•™ä½ç»´ä¸»æˆåˆ†</u>ï¼Œ<u>å¿½ç•¥é«˜ç»´ä¸»æˆåˆ†</u>åšåˆ°çš„ã€‚
  - è¿™æ ·ä½ç»´æˆåˆ†å¾€å¾€èƒ½å¤Ÿä¿ç•™ä½æ•°æ®çš„æœ€é‡è¦éƒ¨åˆ†ã€‚ä½†æ˜¯ï¼Œ<u>è¿™ä¹Ÿä¸æ˜¯ä¸€å®šçš„ï¼Œè¦è§†å…·ä½“åº”ç”¨è€Œå®š</u>ã€‚ç”±äºä¸»æˆåˆ†åˆ†æä¾èµ–æ‰€ç»™æ•°æ®ï¼Œæ‰€ä»¥<u>æ•°æ®çš„å‡†ç¡®æ€§å¯¹åˆ†æç»“æœå½±å“å¾ˆå¤§</u>ã€‚
- ä¸»æˆåˆ†åˆ†æç”±[å¡å°”Â·çš®å°”é€Š](https://zh.wikipedia.org/wiki/å¡å°”Â·çš®å°”é€Š)äº1901å¹´å‘æ˜ï¼Œç”¨äºåˆ†ææ•°æ®åŠå»ºç«‹æ•°ç†æ¨¡å‹ï¼Œåœ¨åŸç†ä¸Šä¸[ä¸»è½´å®šç†](https://zh.wikipedia.org/w/index.php?title=ä¸»è½´å®šç†&action=edit&redlink=1)ç›¸ä¼¼ã€‚
- ä¹‹ååœ¨1930å¹´å·¦å³ç”±[å“ˆç½—å¾·Â·éœç‰¹æ—](https://zh.wikipedia.org/wiki/å“ˆç¾…å¾·Â·éœç‰¹æ—)ç‹¬ç«‹å‘å±•å¹¶å‘½åã€‚
- ä¾æ®åº”ç”¨é¢†åŸŸçš„ä¸åŒï¼Œåœ¨ä¿¡å·å¤„ç†ä¸­å®ƒä¹Ÿå«åšç¦»æ•£[K-L è½¬æ¢](https://zh.wikipedia.org/wiki/K-L_è½‰æ›)ï¼ˆdiscrete Karhunenâ€“LoÃ¨ve transform (KLT)ï¼‰ã€‚
- å…¶æ–¹æ³•ä¸»è¦æ˜¯é€šè¿‡å¯¹[åæ–¹å·®çŸ©é˜µ](https://zh.wikipedia.org/wiki/å…±è®Šç•°æ•¸çŸ©é™£)è¿›è¡Œç‰¹å¾åˆ†è§£ï¼Œä»¥å¾—å‡ºæ•°æ®çš„ä¸»æˆåˆ†ï¼ˆå³[ç‰¹å¾å‘é‡](https://zh.wikipedia.org/wiki/ç‰¹å¾å‘é‡)ï¼‰ä¸å®ƒä»¬çš„æƒå€¼ï¼ˆå³[ç‰¹å¾å€¼](https://zh.wikipedia.org/wiki/ç‰¹å¾å€¼)ã€‚
- PCAæ˜¯æœ€ç®€å•çš„ä»¥ç‰¹å¾é‡åˆ†æå¤šå…ƒç»Ÿè®¡åˆ†å¸ƒçš„æ–¹æ³•ã€‚
  - å…¶ç»“æœå¯ä»¥ç†è§£ä¸ºå¯¹åŸæ•°æ®ä¸­çš„[æ–¹å·®](https://zh.wikipedia.org/wiki/æ–¹å·®)åšå‡ºè§£é‡Šï¼šå“ªä¸€ä¸ªæ–¹å‘ä¸Šçš„æ•°æ®å€¼å¯¹æ–¹å·®çš„å½±å“æœ€å¤§ï¼Ÿ
  - æ¢è€Œè¨€ä¹‹ï¼ŒPCAæä¾›äº†ä¸€ç§é™ä½æ•°æ®[ç»´åº¦](https://zh.wikipedia.org/wiki/ç¶­åº¦)çš„æœ‰æ•ˆåŠæ³•ï¼›
  - å¦‚æœåˆ†æè€…åœ¨åŸæ•°æ®ä¸­é™¤æ‰æœ€å°çš„[ç‰¹å¾å€¼](https://zh.wikipedia.org/wiki/ç‰¹å¾å€¼)æ‰€å¯¹åº”çš„æˆåˆ†ï¼Œé‚£ä¹ˆæ‰€å¾—çš„ä½ç»´åº¦æ•°æ®å¿…å®šæ˜¯æœ€ä¼˜åŒ–çš„ï¼ˆè¿™æ ·é™ä½ç»´åº¦å¿…å®šæ˜¯å¤±å»ä¿¡æ¯æœ€å°‘çš„æ–¹æ³•ï¼‰ã€‚
  - ä¸»æˆåˆ†åˆ†æåœ¨åˆ†æå¤æ‚æ•°æ®æ—¶å°¤ä¸ºæœ‰ç”¨ï¼Œæ¯”å¦‚[äººè„¸è¯†åˆ«](https://zh.wikipedia.org/wiki/äººè„¸è¯†åˆ«)ã€‚
  - é€šå¸¸ï¼Œè¿™ç§è¿ç®—å¯ä»¥è¢«çœ‹ä½œæ˜¯<u>æ­éœ²æ•°æ®çš„å†…éƒ¨ç»“æ„</u>ï¼Œä»è€Œæ›´å¥½åœ°å±•ç°æ•°æ®çš„å˜å¼‚åº¦ã€‚
  - å¦‚æœä¸€ä¸ªå¤šå…ƒæ•°æ®é›†æ˜¯ç”¨é«˜ç»´æ•°æ®ç©ºé—´ä¹‹åæ ‡ç³»æ¥è¡¨ç¤ºçš„ï¼Œé‚£ä¹ˆ<u>PCAèƒ½æä¾›ä¸€å¹…è¾ƒä½ç»´åº¦çš„å›¾åƒï¼Œç›¸å½“äºæ•°æ®é›†åœ¨è®¯æ¯é‡æœ€å¤šä¹‹è§’åº¦ä¸Šçš„ä¸€ä¸ªæŠ•å½±</u>ã€‚è¿™æ ·å°±å¯ä»¥åˆ©ç”¨å°‘é‡çš„ä¸»æˆåˆ†è®©æ•°æ®çš„ç»´åº¦é™ä½äº†ã€‚
- PCA è·Ÿ[å› å­åˆ†æ](https://zh.wikipedia.org/wiki/å› å­åˆ†æ)å¯†åˆ‡ç›¸å…³ã€‚å› å­åˆ†æé€šå¸¸åŒ…å«æ›´å¤šç‰¹å®šé¢†åŸŸåº•å±‚ç»“æ„çš„å‡è®¾ï¼Œå¹¶ä¸”æ±‚è§£ç¨å¾®ä¸åŒçŸ©é˜µçš„ç‰¹å¾å‘é‡ã€‚
- PCA ä¹Ÿè·Ÿ[å…¸å‹ç›¸å…³åˆ†æ](https://zh.wikipedia.org/wiki/å…¸å‹ç›¸å…³åˆ†æ)ï¼ˆCCAï¼‰æœ‰å…³ã€‚
  - CCAå®šä¹‰çš„åæ ‡ç³»å¯ä»¥æœ€ä½³åœ°æè¿°<u>ä¸¤ä¸ªæ•°æ®é›†ä¹‹é—´</u>çš„[äº’åæ–¹å·®](https://zh.wikipedia.org/wiki/äº’åæ–¹å·®)ï¼Œ
  - è€ŒPCAå®šä¹‰äº†æ–°çš„æ­£äº¤åæ ‡ç³»ï¼Œèƒ½æœ€ä½³åœ°æè¿°<u>å•ä¸ªæ•°æ®é›†å½“ä¸­çš„æ–¹å·®</u>ã€‚

### PCAçš„æ•°å­¦å®šä¹‰

- ä¸€ä¸ªæ­£äº¤åŒ–çº¿æ€§å˜æ¢ï¼ŒæŠŠæ•°æ®å˜æ¢åˆ°ä¸€ä¸ªæ–°çš„åæ ‡ç³»ç»Ÿä¸­ï¼Œä½¿å¾—è¿™ä¸€æ•°æ®çš„ä»»ä½•æŠ•å½±çš„ç¬¬ä¸€å¤§æ–¹å·®åœ¨ç¬¬ä¸€ä¸ªåæ ‡ï¼ˆç§°ä¸ºç¬¬ä¸€ä¸»æˆåˆ†ï¼‰ä¸Šï¼Œç¬¬äºŒå¤§æ–¹å·®åœ¨ç¬¬äºŒä¸ªåæ ‡ï¼ˆç¬¬äºŒä¸»æˆåˆ†ï¼‰ä¸Šï¼Œä¾æ¬¡ç±»æ¨ã€‚

- å®šä¹‰ä¸€ä¸ª$n\times m$çš„çŸ©é˜µ, ${\displaystyle X^{T}}$ä¸ºå»å¹³å‡å€¼ï¼ˆä»¥å¹³å‡å€¼ä¸ºä¸­å¿ƒç§»åŠ¨è‡³åŸç‚¹ï¼‰çš„æ•°æ®ï¼Œå…¶è¡Œä¸ºæ•°æ®æ ·æœ¬ï¼Œåˆ—ä¸ºæ•°æ®ç±»åˆ«ï¼ˆæ³¨æ„ï¼Œè¿™é‡Œå®šä¹‰çš„æ˜¯${\displaystyle X^{T}}$ è€Œä¸æ˜¯$X$ï¼‰ã€‚åˆ™$X$çš„å¥‡å¼‚å€¼åˆ†è§£ä¸º${\displaystyle X=W\Sigma V^{T}}$ï¼Œå…¶ä¸­${\displaystyle W\in \mathbf {R} ^{m\times m}}$æ˜¯${\displaystyle XX^{T}}$çš„ç‰¹å¾å‘é‡çŸ©é˜µï¼Œ ${\displaystyle \Sigma \in \mathbf {R} ^{m\times n}}$æ˜¯å¥‡å¼‚å€¼çŸ©é˜µï¼Œ${\displaystyle V\in \mathbf {R} ^{n\times n}}$æ˜¯${\displaystyle X^{T}X}$çš„ç‰¹å¾å‘é‡çŸ©é˜µã€‚æ®æ­¤ï¼Œ
  $$
  {\displaystyle {\begin{aligned}{\boldsymbol {Y}}^{\top }&={\boldsymbol {X}}^{\top }{\boldsymbol {W}}\\&={\boldsymbol {V}}{\boldsymbol {\Sigma }}^{\top }{\boldsymbol {W}}^{\top }{\boldsymbol {W}}\\&={\boldsymbol {V}}{\boldsymbol {\Sigma }}^{\top }\end{aligned}}}
  $$

  - å½“ m < n âˆ’ 1æ—¶ï¼ŒV åœ¨é€šå¸¸æƒ…å†µä¸‹ä¸æ˜¯å”¯ä¸€å®šä¹‰çš„ï¼Œè€ŒY åˆ™æ˜¯å”¯ä¸€å®šä¹‰çš„ã€‚W æ˜¯ä¸€ä¸ªæ­£äº¤çŸ©é˜µï¼ŒYTWT=XTï¼Œä¸”YTçš„ç¬¬ä¸€åˆ—ç”±ç¬¬ä¸€ä¸»æˆåˆ†ç»„æˆï¼Œç¬¬äºŒåˆ—ç”±ç¬¬äºŒä¸»æˆåˆ†ç»„æˆï¼Œä¾æ­¤ç±»æ¨ã€‚
  - ä¸ºäº†å¾—åˆ°ä¸€ç§é™ä½æ•°æ®ç»´åº¦çš„æœ‰æ•ˆåŠæ³•ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨WLæŠŠ X æ˜ å°„åˆ°ä¸€ä¸ªåªåº”ç”¨å‰é¢Lä¸ªå‘é‡çš„ä½ç»´ç©ºé—´ä¸­å»ï¼š

    - $\mathbf{Y}=\mathbf{W_L}^\top\mathbf{X} = \mathbf{\Sigma_L}\mathbf{V}^\top$

    - å…¶ä¸­$\mathbf{\Sigma_L}=\mathbf{I}*_{L\times m}\mathbf{\Sigma}$**ï¼Œä¸”**$\mathbf{I}_*{L\times m}$ä¸º$L\times m$çš„å•ä½çŸ©é˜µã€‚

    - X çš„å•å‘é‡çŸ©é˜µWç›¸å½“äºåæ–¹å·®çŸ©é˜µçš„ç‰¹å¾å‘é‡ C = X XT,

    - $\mathbf{X}\mathbf{X}^\top = \mathbf{W}\mathbf{\Sigma}\mathbf{\Sigma}^\top\mathbf{W}^\top$
  - åœ¨æ¬§å‡ é‡Œå¾—ç©ºé—´ç»™å®šä¸€ç»„ç‚¹æ•°ï¼Œç¬¬ä¸€ä¸»æˆåˆ†å¯¹åº”äºé€šè¿‡å¤šç»´ç©ºé—´å¹³å‡ç‚¹çš„ä¸€æ¡çº¿ï¼ŒåŒæ—¶ä¿è¯å„ä¸ªç‚¹åˆ°è¿™æ¡ç›´çº¿è·ç¦»çš„å¹³æ–¹å’Œæœ€å°ã€‚å»é™¤æ‰ç¬¬ä¸€ä¸»æˆåˆ†åï¼Œç”¨åŒæ ·çš„æ–¹æ³•å¾—åˆ°ç¬¬äºŒä¸»æˆåˆ†ã€‚
  - ä¾æ­¤ç±»æ¨ã€‚åœ¨Î£ä¸­çš„å¥‡å¼‚å€¼å‡ä¸ºçŸ©é˜µ $XX^T$çš„ç‰¹å¾å€¼çš„å¹³æ–¹æ ¹ã€‚
  - æ¯ä¸€ä¸ªç‰¹å¾å€¼éƒ½ä¸è·Ÿå®ƒä»¬ç›¸å…³çš„æ–¹å·®æ˜¯æˆæ­£æ¯”çš„ï¼Œè€Œä¸”æ‰€æœ‰ç‰¹å¾å€¼çš„æ€»å’Œç­‰äºæ‰€æœ‰ç‚¹åˆ°å®ƒä»¬çš„å¤šç»´ç©ºé—´å¹³å‡ç‚¹è·ç¦»çš„å¹³æ–¹å’Œã€‚

## æœºå™¨å­¦ä¹ ä¸­çš„PCAç®—æ³•

- ä¸»æˆåˆ†åˆ†æï¼ˆprincipal components analysis, PCAï¼‰æ˜¯ä¸€ä¸ªç®€å•çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå¯ä»¥é€šè¿‡åŸºç¡€çš„çº¿æ€§ä»£æ•°çŸ¥è¯†æ¨å¯¼ã€‚

- å‡è®¾åœ¨ $R^n$ ç©ºé—´ä¸­æˆ‘ä»¬æœ‰ m ä¸ªç‚¹ $\{x^{(1)}, . . . , x^{(m)}\}$ï¼Œæˆ‘ä»¬å¸Œæœ›å¯¹è¿™äº›ç‚¹è¿›è¡Œæœ‰æŸå‹ç¼©ã€‚

  - æœ‰æŸå‹ç¼©è¡¨ç¤ºæˆ‘ä»¬å¯ä»¥æŸå¤±ä¸€äº›ç²¾åº¦ä½†ä½¿ç”¨æ›´å°‘çš„å†…å­˜å»å­˜å‚¨è¿™äº›ç‚¹ã€‚
  - æˆ‘ä»¬å¸Œæœ›æŸå¤±çš„ç²¾åº¦å°½å¯èƒ½å°‘ã€‚

### ç¼–ç å‘é‡c

- ä¸€ç§ç¼–ç è¿™äº›ç‚¹çš„æ–¹å¼æ˜¯ç”¨**ä½ç»´**è¡¨ç¤ºã€‚
  - å¯¹äºæ¯ä¸ªç‚¹ $x^{(i)}\in R^n$ï¼Œä¼šæœ‰ä¸€ä¸ªå¯¹åº”çš„ç¼–ç å‘é‡ $c^{(i)} \in \mathbb{R}^l$ã€‚
  - å¦‚æœ $l$ æ¯” $n$ å°ï¼Œé‚£ä¹ˆæˆ‘ä»¬ä¾¿ä½¿ç”¨äº†<u>æ›´å°‘çš„å†…å­˜æ¥**å­˜å‚¨**åŸæ¥çš„æ•°æ®</u>ã€‚

#### ç¼–ç å‡½æ•°

- æˆ‘ä»¬å¸Œæœ›æ‰¾åˆ°ä¸€ä¸ª**ç¼–ç å‡½æ•°**ï¼Œæ ¹æ®è¾“å…¥è¿”å›ç¼–ç $f(x) = c$ï¼›

#### è§£ç å‡½æ•°

- æˆ‘ä»¬ä¹Ÿå¸Œæœ›æ‰¾åˆ°ä¸€ä¸ª**è§£ç å‡½æ•°**ï¼Œç»™å®šç¼–ç é‡æ„è¾“å…¥$x\approx g(f(x))$(å› ä¸ºå­˜å‚¨çš„æ—¶å€™æ—¶æœ‰æŸçš„,æ‰€ä»¥åªèƒ½è¿‘ä¼¼è¿˜åŸ)
  - è¿™é‡Œçš„$\approx$åº”è¯¥è¡¨ç¤ºå®ƒä»¬çš„ä¸»è¦ä¿¡æ¯ç›¸è¿‘,å®ƒä»¬çš„ç»´æ•°å¯èƒ½æ˜¯ä¸åŒçš„,å¾€å¾€$g(f(x))$çš„ç»´æ•°$l$è¦å°äº$x$çš„ç»´æ•°$n$
  - è®°$r(x)=g(f(x))$
  - è¡¡é‡å‘é‡$x$å’Œ$r(x)$çš„åå·®:$x-r(x)$

#### ç¡®å®šç¼–ç å‡½æ•°

- PCA ç”±æˆ‘ä»¬é€‰æ‹©çš„**è§£ç å‡½æ•°**è€Œå®š
  - ä¸ºäº†ç®€åŒ–è§£ç å™¨$c=f(x)$ï¼Œæˆ‘ä»¬ä½¿ç”¨**çŸ©é˜µä¹˜æ³•**å°†ç¼–ç $c$æ˜ å°„å› $R^n$ï¼Œå³ 
    - $x=g(c)= Dc$,å³$x=g(f(x))=Df(x)$ï¼Œ
    - å…¶ä¸­ $D\in R^{nÃ—l}$ æ˜¯å®šä¹‰**è§£ç **çš„çŸ©é˜µã€‚

#### çº¦æŸæ¡ä»¶

- ç›®å‰ä¸ºæ­¢æ‰€æè¿°çš„é—®é¢˜ï¼Œå¯èƒ½ä¼šæœ‰å¤šä¸ªè§£ã€‚

  - å› ä¸ºå¦‚æœæˆ‘ä»¬æŒ‰æ¯”ä¾‹åœ°ç¼©å°æ‰€æœ‰ç‚¹å¯¹åº”çš„<u>ç¼–ç å‘é‡</u>$c_i$,é‚£ä¹ˆæˆ‘ä»¬åªéœ€æŒ‰æ¯”ä¾‹æ”¾å¤§ç¬¬iåˆ—çš„åˆ—å‘é‡$D_{:,i}$ï¼Œå³å¯<u>ä¿æŒç»“æœä¸å˜ã€‚</u>
    - $\frac{1}{k}Dkc_i=Dc_i$,$k\in{R}$
  - è®¡ç®—è§£ç å™¨çš„**æœ€ä¼˜ç¼–ç **å¯èƒ½æ˜¯ä¸€ä¸ªå›°éš¾çš„é—®é¢˜ã€‚

- ä¸ºäº†ä½¿é—®é¢˜æœ‰**å”¯ä¸€è§£**ï¼Œæˆ‘ä»¬é™åˆ¶ D ä¸­æ‰€æœ‰**åˆ—å‘é‡**éƒ½æœ‰**å•ä½èŒƒæ•°**ğŸˆ

- ä¸ºäº†ä½¿ç¼–ç é—®é¢˜ç®€å•ä¸€äº›ï¼ŒPCA é™åˆ¶ D çš„**åˆ—å‘é‡å½¼æ­¤æ­£äº¤**ğŸˆ

  - æ³¨æ„ä¸Šè¿°çš„çº¦æŸæ¡ä»¶ä½¿å¾—Dæ¥è¿‘**æ­£äº¤çŸ©é˜µ**çš„æ€§è´¨ï¼Œä½†é™¤é $l = n$ï¼Œå¦åˆ™ä¸¥æ ¼æ„ä¹‰ä¸Š $D\in R^{nÃ—l}$ ä¸æ˜¯ä¸€ä¸ªæ­£äº¤çŸ©é˜µ

- é¦–å…ˆæˆ‘ä»¬éœ€è¦æ˜ç¡®å¦‚ä½•<u>æ ¹æ®æ¯ä¸€ä¸ªè¾“å…¥ $x$ å¾—åˆ°ä¸€ä¸ª**æœ€ä¼˜ç¼–ç ** $c^âˆ—$</u>

  - ä¸€ç§æ–¹æ³•æ˜¯**æœ€å°åŒ–**[åŸå§‹è¾“å…¥å‘é‡ $x$] å’Œ[é‡æ„å‘é‡$g(c^âˆ—)$ ]ä¹‹é—´çš„**è·ç¦»**ã€‚

    - ä½¿ç”¨**èŒƒæ•°**æ¥è¡¡é‡å®ƒä»¬ä¹‹é—´çš„**è·ç¦»**ã€‚

  - åœ¨ PCA ç®—æ³•ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ $L^2$ èŒƒæ•°ï¼š

    - $c^âˆ— = \underset{c}{\arg min}(||x-g(c))||_2)$

  - è®°å·å¯ä»¥å‚è€ƒæ–‡æœ«è¡¥å……

  - å¯ä»¥ç”¨$(L^2)^2$ä»£æ›¿$L^2$èŒƒæ•°,å› ä¸º<u>ä¸¤è€…åœ¨ç›¸åŒçš„$c$å€¼å–å¾—æœ€å°å€¼</u>ğŸˆ

  - è¿™æ˜¯å› ä¸º $L^2$ èŒƒæ•°æ˜¯éè´Ÿçš„ï¼Œå¹¶ä¸”å¹³æ–¹è¿ç®—åœ¨éè´Ÿå€¼ä¸Šæ˜¯å•è°ƒé€’å¢çš„ã€‚

    - $c^âˆ— = \underset{c}{\arg min}(||x-g(c)||_2^2)$
      - $T=\sum_{i=1}^{n}(x-g(c))^2$
      - $c^*=\underset{c}{\arg{min}(T)}$

  - $L^2$èŒƒæ•°çš„å®šä¹‰ï¼Œè¯¥æœ€å°åŒ–å‡½æ•°å¯ä»¥ç®€åŒ–æˆ

    - $T=(x âˆ’ g(c))^T(x âˆ’ g(c))$ 

      - $=(x^T-g(c)^T)(x-g(c))$(è½¬ç½®è¿ç®—å¯¹åŠ (å‡)æ³•çš„åˆ†é…å¾‹)
      - $= x^Tx âˆ’ x^Tg(c) âˆ’ g(c)^Tx + g(c)^Tg(c)$ (çŸ©é˜µä¹˜æ³•å¯¹åŠ (å‡)æ³•çš„åˆ†é…å¾‹)
      - $= x^Tx âˆ’ 2x^Tg(c) + g(c)^Tg(c)$(æ ‡é‡ $g(c)^Tx$ çš„è½¬ç½®ç­‰äºè‡ªå·±;ç‚¹ç§¯è¿ç®—æ»¡è¶³äº¤æ¢å¾‹$a^Tb=b^Ta$)

    - è§‚å¯Ÿå‘ç°ç¬¬ä¸€é¡¹$x^Tx$ä¸$c$æ— å…³,ä»…è€ƒå¯Ÿ$âˆ’ 2x^Tg(c) + g(c)^Tg(c)$

    - ä¼˜åŒ–ç›®æ ‡å¯ä»¥è®°ä¸º$c^*=\underset{c}{\arg{min}}(-2x^Tg(c)+g(c)^Tg(c))$

      - å¸¦å…¥$g(c)=Dc$,åˆ™

      - $c^*=\underset{c}{\arg{min}}\;(-2x^TDc+c^TD^TDc)$

        - $=\underset{c}{\arg{min}}\:-2x^TDc+c^TI_lc$(çŸ©é˜µ D <u>çš„**æ­£äº¤æ€§**å’Œ**å•ä½èŒƒæ•°**çº¦æŸ</u>)

          - $D^TD=I_{l}$

            - $D\in\mathbb{R}^{n\times{l}}$

            - $D^T\in\mathbb{R}^{l\times{n}}$

            - $D^TD\in\mathbb{R}^{l\times{l}}$,æ ¹æ®å•ä½æ­£äº¤æ€§,$D^TD=I_l$($l$é˜¶å•ä½å‘é‡)

              - $(\alpha_1^T,\cdots,\alpha_l^T)^T(\alpha_1,\cdots,\alpha_l)=I_l$

              - å•ä½æ­£äº¤æ€§:

                - $$
                  \delta_{ij}=\alpha_i^T\alpha_j=(\alpha_i,\alpha_j)=
                  \begin{cases}
                  1,&i=j\\
                  0,&i\neq{j}
                  \end{cases}
                  $$

        - $=\underset{c}{\arg{min}}\:-2x^TDc+c^Tc$

  - é€šè¿‡<u>å‘é‡å¾®ç§¯åˆ†</u>æ±‚è§£æœ€ä¼˜åŒ–é—®é¢˜:å¯¹$c$æ±‚**æ¢¯åº¦**,å¹¶ä»¤å…¶ä¸ºé›¶

    - $$
      \nabla_{c}(-2x^TDc+c^Tc)=0
      \\
      -2D^Tx+2c=0
      \\
      c=D^Tx
      $$

    - æ±‚$c^Tc$å¯¹$c$çš„å¯¼æ•°

      - æ ‡é‡å‡½æ•°å¯¹å‘é‡æ±‚å¯¼çš„è§’åº¦

      - $\mathbf{c}=(c_1,\cdots,c_n)$

      - $y(\mathbf{c})=\mathbf{c}^T\mathbf{c}=(\mathbf{c},\mathbf{c})=\sum\limits_{i}c_i^2$(æ˜¯ä¸€ä¸ªæ ‡é‡)

      - $$
        \frac{d}{dc}y
        =(\frac{\partial{y}}{\partial{c_1}},\cdots,\frac{\partial{y}}{\partial{c_n}})
        =(\frac{\partial{}}{\partial{c_1}}\sum\limits_{i}c_i^2,\cdots,\frac{\partial{}}{\partial{c_n}}\sum\limits_{i}c_i^2)
        \\
        =(2c_1,\cdots,{2c_n})
        =2(c_1,\cdots,c_n)
        =2\mathbf{c}
        $$

    - å¯è§,æœ€ä¼˜ç¼–ç åªéœ€è¦ä¸€ä¸ªçŸ©é˜µ-å‘é‡ä¹˜æ³•æ“ä½œ

    - å¯ä»¥ç”¨ç¼–ç å‡½æ•°$f(x)=D^Tx$

    - å®šä¹‰PCAé‡æ„æ“ä½œ(æ­¤æ—¶é€šè¿‡<u>ç¼–ç è§£ç </u>å¾—åˆ°çš„é‡æ„ä¸º)

      - $r(x)=g(f(x))=DD^Tx$


#### s.t.(subject to)

- s.t.æ˜¯æ•°å­¦ä¸­å¸¸ç”¨çš„ç¼©å†™ï¼Œè¡¨ç¤ºsubject toæˆ–such thatï¼Œæ„æ€æ˜¯â€œå—çº¦æŸçš„â€æˆ–â€œä½¿å¾—â€

#### éœ€è¦æŒ‘é€‰ç¼–ç çŸ©é˜µ D

- $D\in R^{nÃ—l}$
  - $DD^T\in{R^{n\times{n}}}$
  - $D^TD\in{R^{l\times{l}}}$

- è¦åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å›é¡¾æœ€å°åŒ–è¾“å…¥å’Œé‡æ„ä¹‹é—´ $L^2$ è·ç¦»çš„è¿™ä¸ªæƒ³æ³•ã€‚

- å› ä¸ºç”¨ç›¸åŒçš„çŸ©é˜µ D å¯¹æ‰€æœ‰ç‚¹è¿›è¡Œè§£ç ï¼Œæˆ‘ä»¬ä¸èƒ½å†å­¤ç«‹åœ°çœ‹å¾…æ¯ä¸ªç‚¹ã€‚

- åä¹‹ï¼Œæˆ‘ä»¬å¿…é¡»æœ€å°åŒ–**è¯¯å·®çŸ©é˜µ**<u>æ‰€æœ‰ç»´æ•°å’Œæ‰€æœ‰ç‚¹</u>ä¸Šçš„ Frobenius èŒƒæ•°ï¼š

  - $$
    D^*=\underset{D}{\arg{min}}\sqrt{\sum_{i,j}(x_j^{(i)}-[r(x^{(i)})]_j})^2
    \\
    s.t.\;D^TD=I_l
    $$

    - $x_j^{i}$ç¬¬iä¸ªå‘é‡çš„ç¬¬jä¸ªå…ƒç´ 
    - $D^*$?è¡¨ç¤ºæ‰€æ±‚çš„èƒ½å¤Ÿä½¿å¾—è¯¯å·®çŸ©é˜µçš„FrobeniusèŒƒæ•°çš„çŸ©é˜µ
    - è¢«è®¡ç®—çš„è¯¯å·®çŸ©é˜µä¸º$E=(x^{(1)}-r(x^{(1)}),\cdots,x^{(n)}-r(x^{(n)}))$,

#### çº¦æŸçŸ©é˜µDä¸ºå‘é‡d

- è€ƒè™‘$l=1$æ—¶(å³Dè¡¨ç¤ºä¸€ä¸ªåˆ—å‘é‡$d\in\mathbb{R}^{n\times{1}}$)

  - $dd^t\in\mathbb{R}^{n\times{n}}$

  - åˆ—å‘é‡$x\in\mathbb{R}^{n\times{1}}$

  - æ­¤æ—¶å¸¦å…¥å…¬å¼$r(x)=DD^Tx=dd^Tx$,å¯ä»¥ç”¨å‘é‡èŒƒæ•°($L^2$èŒƒæ•°)æ›¿æ¢

  - $$
    d^*=\underset{d}{\arg{min}}\sqrt{\sum_{i,j}(x_j^{(i)}-dd^Tx_j^{(i)})^2}
    \\
    ä½¿ç”¨å¹³æ–¹FrobeniusèŒƒæ•°ä»£æ›¿FrobeniusèŒƒæ•°(å› ä¸ºå®ƒä»¬åœ¨ç›¸åŒçš„æ¡ä»¶å–å¾—æœ€å°å€¼)
    \\
    å…¬å¼å˜ä¸º
    d^*=\underset{d}{\arg{min}}{\sum_{i,j}(x_j^{(i)}-dd^Tx_j^{(i)})^2}
    \\
    =\underset{d}{\arg{min}}{\sum_{i}\sum_{j}(x_j^{(i)}-dd^Tx_j^{(i)})^2}
    \\=\underset{d}{\arg{min}}{\sum_{i}||(x^{(i)}-dd^Tx^{(i)})||_2^2}
    \\s.t.\;{||d||_2=1}
    $$

  - é‡æ’æ•´ç†:(å°†æ ‡é‡å†™åœ¨å‰,æ”¹å†™$dd^Tx$ä¸º$d^Txd$)

    - $d\in{R}^{n\times{1}}$

    - $d^T\in{R^{1\times{n}}}$

    - $d^Tx\in{R^{1\times{1}}}$,å³$d^Tx$çœ‹ä½œä¸€ä¸ªæ ‡é‡

      - å› æ­¤æœ‰:$dd^Tx=d(d^Tx)=(d^Tx)d$

    - $$
      d^*=\underset{d}{\arg{min}}{\sum_{i}||(x^{(i)}-d^Tx^{(i)}d)||_2^2}
      \\s.t.\;{||d||_2=1}
      $$

    - è€ƒè™‘åˆ°æ ‡é‡$d^Tx^{(i)}$çš„è½¬ç½®ä¸è‡ªèº«ç›¸ç­‰:$d^Tx^{(i)}=(d^Tx^{(i)})^T=(x^{(i)})^Td$

    $$
    d^*=\underset{d}{\arg{min}}{\sum_{i}||(x^{(i)}-(x^{(i)})^Tdd)||_2^2}
    \\s.t.\;{||d||_2=1}
    $$

  - æ­¤æ—¶ï¼Œä½¿ç”¨å•ä¸€çŸ©é˜µæ¥é‡è¿°é—®é¢˜ï¼Œæ¯”å°†é—®é¢˜å†™æˆæ±‚å’Œå½¢å¼æ›´æœ‰å¸®åŠ©ã€‚è¿™æœ‰åŠ©äºæˆ‘ä»¬ä½¿ç”¨æ›´ç´§å‡‘çš„ç¬¦å·ã€‚

  - å°†è¡¨ç¤ºå„ç‚¹çš„å‘é‡å †å æˆä¸€ä¸ªçŸ©é˜µ(æ ¹æ®ä¸Šå¼ä¸­çš„$(x^{(i)})^T\in{R}^{1\times{n}}$:

    - å †å $\left((x^{(1)})^T,\cdots,{(x^{(m)})^T}\right)^T$;è®°ä¸º $X\in R^{mÃ—n}$ï¼Œå…¶ä¸­ç¬¬iè¡Œå‘é‡$X_{i,:} = (x^{(i)})^T$ã€‚

  - åŸé—®é¢˜å¯ä»¥é‡æ–°(ä½¿ç”¨æ–°è®°å·$||X||_F$æè¿°çŸ©é˜µçš„<u>å¹³æ–¹FrobeniusèŒƒæ•°</u>è¡¨è¿°ä¸ºï¼š

    - $$
      d^*=\underset{d}{\arg{min}}\;||(X-Xdd^T)||_F^2
      \\s.t.\;{||d||_2=1}
      $$

      - $X,Xdd^T\in{R^{m\times{n}}}$

      - $X_{ij}$,$(Xdd^T)_{ij}$
        - $X,d,d^Tåˆ†åˆ«ä¸ºm\times{n},n\times{1},1\times{n}$
      - $((x^{(i)})^Td)\in{R^{1\times{1}}}$
      - è®°å‘é‡$q^{(i)}=x^{(i)}-(x^{(i)})^Tdd$
        - $||q^{(i)}||_2=||(q^{(i)})^T||_2$
        - å³$||x^{(i)}-(x^{(i)})^Tdd||=||(x^{(i)})^T-((x^{(i)})^Td)d^T||_2$
        - è®°$X=(x^{(i)})^T$,åˆ™$||q^{(i)}||_2^2=||X-Xdd^T||_2^2$
        - $\sum\limits_{i}(x^{(i)}-(x^{(i)})^Tdd)=||(X-Xdd^T)||_F^2$

    - ç°åœ¨æˆ‘ä»¬æŠŠå¹³æ–¹$F$èŒƒæ•°(Frobenius èŒƒæ•°)ç”¨è¿¹è¿ç®—(Tr)æ¥è¡¨è¾¾:

      - ä½¿ç”¨è¿¹è¿ç®—æ¥è¡¨è¾¾FèŒƒæ•°å¼æ˜¯ä¸€ç§å¸¸è§çš„åšæ³•å’ŒæŠ€å·§:$||Q||_F^2=Tr(Q^TQ)$

      $$
      ||X-Xdd^T||_F^2=Tr((X-Xdd^T)^T(X-Xdd^T))
      $$

      - å±•å¼€å¤„ç†Trçš„å‚æ•°,å‚æ•°è®°ä¸ºQ=$(X-Xdd^T)^T(X-Xdd^T)$

      - $$
        Tr((X-Xdd^T)^T(X-Xdd^T))
        \\=Tr(X^TX-X^TXdd^T-dd^TX^TX+dd^TX^TXdd^T)
        \\
        =Tr(X^TX)-Tr(X^TXdd^T)-Tr(dd^TX^TX)+Tr(dd^TX^TXdd^T)
        \\è¿¹è¿ç®—ä¸­ç›¸ä¹˜çš„çŸ©é˜µé¡ºåºè°ƒæ•´ä¸å½±å“ç»“æœTrè®¡ç®—ç»“æœ(åˆå¹¶ç¬¬2,3é¡¹)
        \\(ä¸éœ€è¦ä¿è¯X^TXdd^T=dd^TX^TX,ç”šè‡³ä¸éœ€è¦æœ‰ç›¸åŒçš„è¡Œæ•°å’Œåˆ—æ•°)
        \\(å…¶ä¸­ä¸€å®šæœ‰Tr(X^TXdd^T)=Tr(dd^TX^TX))
        \\
        =Tr(X^TX)-2Tr(X^TXdd^T)+Tr(dd^TX^TXdd^T)
        $$

      - å…¶ä¸­ç¬¬ä¸€é¡¹$Tr(X^TX)$å’Œdæ— å…³,ä¸ä¼šå½±å“$\arg{min}$,æ‰€ä»¥å¯ä»¥çœç•¥æ‰

        - æ³¨æ„$dd^T\in\mathbb{R}^{n\times{n}}$

    - $d^*=\underset{d}{\arg{min}}\;Tr(Q)$

      - $=\underset{d}{\arg{min}}\;-2Tr(X^TXdd^T)+Tr(dd^TX^TXdd^T)$
      - $=\underset{d}{\arg{min}}\;-2Tr(X^TXdd^T)+Tr(X^TXdd^Tdd^T)$

    - å†è€ƒè™‘çº¦æŸæ¡ä»¶$d^Td=1$(å¸¦å…¥ä¸Šå¼)

      - $d^*=\underset{d}{\arg{min}}\;-2Tr(X^TXdd^T)+Tr(X^TXdd^T)$(åˆå¹¶)
        - $=\underset{d}{\arg{min}}\;-Tr(X^TXdd^T)$
        - å°†é—®é¢˜è½¬ä¸ºæœ€å¤§å€¼
        - $=\underset{d}{\arg{max}}\;Tr(X^TXdd^T)$
        - $=\underset{d}{\arg{max}}\;Tr(d^TX^TXd)$(s.t. $d^Td=1$)

- è¿™ä¸ªä¼˜åŒ–é—®é¢˜å¯ä»¥é€šè¿‡ç‰¹å¾åˆ†è§£æ¥æ±‚è§£ã€‚
- å…·ä½“æ¥è®²ï¼Œæœ€ä¼˜çš„ d æ˜¯ $X^TX$ æœ€å¤§ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡ã€‚
- ä»¥ä¸Šæ¨å¯¼ç‰¹å®šäº l = 1 çš„æƒ…å†µï¼Œä»…å¾—åˆ°äº†ç¬¬ä¸€ä¸ªä¸»æˆåˆ†ã€‚
- æ›´ä¸€èˆ¬åœ°ï¼Œå½“æˆ‘ä»¬å¸Œæœ›å¾—åˆ°ä¸»æˆåˆ†çš„åŸºæ—¶ï¼ŒçŸ©é˜µ D ç”±å‰ l ä¸ªæœ€å¤§çš„ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡ç»„æˆã€‚
  - è¿™ä¸ªç»“è®ºå¯ä»¥é€šè¿‡å½’çº³æ³•è¯æ˜

### ä½¿ç”¨sklearnçš„apiè¿›è¡Œpcaè®¡ç®—

- [sklearn.decomposition.PCA â€” scikit-learn  documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)

- [Decomposing signals in components (matrix factorization problems) â€” scikit-learn 1.2.2 documentation](https://scikit-learn.org/stable/modules/decomposition.html#pca)

- Principal component analysis (PCA).

- Linear dimensionality reduction using Singular (å¥‡å¼‚)Value Decomposition of the data to project it to a lower dimensional space. 

- The input data is centered but not scaled for each feature before applying the SVD.

  It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.

  It can also use the scipy.sparse.linalg ARPACK implementation of the truncated SVD.

  Notice that this class does not support sparse input. See [`TruncatedSVD`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD) for an alternative with sparse data.

- ä½¿ç”¨æ•°æ®çš„å¥‡å¼‚å€¼åˆ†è§£å°†å…¶æŠ•å½±åˆ°ä¸€ä¸ªä½ç»´ç©ºé—´ä¸­è¿›è¡Œ**çº¿æ€§é™ç»´**ã€‚

- åœ¨åº”ç”¨å¥‡å¼‚å€¼åˆ†è§£ä¹‹å‰ï¼Œè¾“å…¥æ•°æ®è¢«å±…ä¸­ä½†ä¸è¿›è¡Œç‰¹å¾ç¼©æ”¾ã€‚

- å®ƒä½¿ç”¨LAPACKå®ç°å®Œæ•´çš„å¥‡å¼‚å€¼åˆ†è§£æˆ–Halkoç­‰äººï¼ˆ2009å¹´ï¼‰æ–¹æ³•å®ç°çš„éšæœºæˆªæ–­å¥‡å¼‚å€¼åˆ†è§£ï¼ˆå–å†³äºè¾“å…¥æ•°æ®çš„å½¢çŠ¶å’Œè¦æå–çš„ä¸»æˆåˆ†æ•°é‡ï¼‰ã€‚

  å®ƒè¿˜å¯ä»¥ä½¿ç”¨scipy.sparse.linalg ARPACKå®ç°è¿›è¡Œæˆªæ–­å¥‡å¼‚å€¼åˆ†è§£ã€‚

- è¯·æ³¨æ„ï¼Œæ­¤ç±»ä¸æ”¯æŒç¨€ç–è¾“å…¥ã€‚å¯¹äºç¨€ç–æ•°æ®ï¼Œè¯·å‚é˜…TruncatedSVDè¿›è¡Œæ›¿ä»£

#### eg

- åŸºæœ¬ä½¿ç”¨æ–¹å¼

  ```python
  from sklearn.decomposition import PCA
  from sklearn.datasets import load_iris
  from sklearn.model_selection import train_test_split
  from sklearn.neighbors import KNeighborsClassifier
  
  # åŠ è½½æ•°æ®é›†
  iris = load_iris()
  X = iris.data
  y = iris.target
  
  # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
  
  # åˆ›å»ºPCAå¯¹è±¡ï¼ŒæŒ‡å®šä¸»æˆåˆ†æ•°é‡ä¸º2
  pca = PCA(n_components=2)
  
  # å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†è¿›è¡Œé™ç»´
  X_train_pca = pca.fit_transform(X_train)
  X_test_pca = pca.transform(X_test)
  
  # æ„å»ºKNNåˆ†ç±»å™¨ï¼Œä½¿ç”¨é™ç»´åçš„æ•°æ®è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•
  knn = KNeighborsClassifier()
  knn.fit(X_train_pca, y_train)
  accuracy = knn.score(X_test_pca, y_test)
  
  # æ‰“å°åˆ†ç±»å™¨å‡†ç¡®ç‡
  print("Accuracy: {:.2f}%".format(accuracy * 100))
  ```

#### eg

- ä»¥æ‰‹å†™æ•°æ®é›†ä¸ºä¾‹,å¯¹æ¯”æœªä½¿ç”¨pcaé™ç»´å’Œä½¿ç”¨äº†pcaé™ç»´åçš„æ•ˆæœ

  - ```python
    from sklearn.datasets import load_digits
    from sklearn.decomposition import PCA
    from sklearn.model_selection import train_test_split
    from sklearn.neighbors import KNeighborsClassifier
    
    # åŠ è½½æ•°æ®é›†
    # mnist = load('mnist_784', version=1)
    mnist=load_digits()
    X = mnist.data
    y = mnist.target
    
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    # ä¸ä½¿ç”¨PCAè¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•
    knn = KNeighborsClassifier()
    knn.fit(X_train, y_train)
    accuracy = knn.score(X_test, y_test)
    print("Accuracy without PCA: {:.2f}%".format(accuracy * 100))
    
    # ä½¿ç”¨PCAè¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•
    pca = PCA(n_components=0.95)
    X_train_pca = pca.fit_transform(X_train)
    X_test_pca = pca.transform(X_test)
    knn_pca = KNeighborsClassifier()
    knn_pca.fit(X_train_pca, y_train)
    accuracy_pca = knn_pca.score(X_test_pca, y_test)
    print("Accuracy with PCA: {:.2f}%".format(accuracy_pca * 100))
    ```

    - ```bash
      Accuracy without PCA: 99.26%
      Accuracy with PCA: 99.07%
      ```

### é‡è¦å‚æ•°

`n_components:int, float or â€˜mleâ€™, default=None`

Number of components to keep. if n_components is not set all components are kept:

`n_components == min(n_samples, n_features)`

If `n_components == 'mle'` and `svd_solver == 'full'`, Minkaâ€™s MLE is used to guess the dimension. 

- Use of `n_components == 'mle'` will interpret `svd_solver == 'auto'` as `svd_solver == 'full'`

If `0 < n_components < 1` and `svd_solver == 'full'`, select the number of components <u>such that the amount of **variance** that needs to be explained is greater than the percentage specified by n_components</u>.

If `svd_solver == 'arpack'`, the number of components must be strictly less than the minimum of n_features and n_samples.

- Hence, the None case results in:`n_components == min(n_samples, n_features) - 1`

å‚æ•°n_componentså¯ä»¥æ˜¯æ•´æ•°ã€æµ®ç‚¹æ•°æˆ–å­—ç¬¦ä¸²'mle'ï¼Œé»˜è®¤ä¸ºNoneã€‚

- å®ƒç”¨äºæŒ‡å®šè¦ä¿ç•™çš„ä¸»æˆåˆ†æ•°é‡ã€‚å¦‚æœæœªè®¾ç½®n_componentsï¼Œåˆ™ä¼šä¿ç•™æ‰€æœ‰ä¸»æˆåˆ†ï¼ˆå³n_componentsç­‰äºmin(n_samples, n_features)ï¼‰ã€‚

- å¦‚æœn_componentsæ˜¯å­—ç¬¦ä¸²'mle'ï¼Œå¹¶ä¸”svd_solverä¸º'full'ï¼Œåˆ™ä½¿ç”¨Minkaçš„MLEç®—æ³•æ¥çŒœæµ‹ä¸»æˆåˆ†æ•°é‡ã€‚
  - ä½¿ç”¨`n_components='mle'`ä¼šå°†`svd_solver='auto'`è§£é‡Šä¸º`'svd_solver'='full'`ã€‚

- å¦‚æœ0 < n_components < 1ï¼Œå¹¶ä¸”svd_solverä¸º'full'ï¼Œåˆ™é€‰æ‹©<u>éœ€è¦è§£é‡Šçš„æ–¹å·®é‡å¤§äºn_componentsæŒ‡å®šçš„ç™¾åˆ†æ¯”çš„</u>ä¸»æˆåˆ†æ•°é‡ã€‚

å¦‚æœsvd_solverä¸º'arpack'ï¼Œåˆ™ä¸»æˆåˆ†æ•°é‡å¿…é¡»ä¸¥æ ¼å°äºn_featureså’Œn_samplesä¸­çš„æœ€å°å€¼ã€‚

- å› æ­¤ï¼Œå¦‚æœn_componentsä¸ºNoneï¼Œåˆ™ç»“æœå°†æ˜¯ï¼š`n_components == min(n_samples, n_features) - 1`
- åœ¨PCAä¸­ï¼Œ"svd_solver"å‚æ•°å¯ä»¥è®¾ç½®ä¸º"arpack"ï¼Œè¡¨ç¤ºä½¿ç”¨ARPACKç®—æ³•è¿›è¡ŒçŸ©é˜µåˆ†è§£ï¼Œæ±‚è§£ä¸»æˆåˆ†ã€‚å½“æ•°æ®é›†æ˜¯å¤§è§„æ¨¡ã€ç¨€ç–çš„æ—¶å€™ï¼Œä½¿ç”¨"arpack"æ¯”å…¶ä»–ç®—æ³•ï¼ˆæ¯”å¦‚"full"ï¼‰æ›´åŠ é«˜æ•ˆã€‚ç„¶è€Œï¼Œ"arpack"ç®—æ³•å¯¹äºå‚æ•°"n_components"çš„é™åˆ¶æ¯”è¾ƒä¸¥æ ¼ï¼Œä¸»æˆåˆ†æ•°é‡å¿…é¡»ä¸¥æ ¼å°äºn_featureså’Œn_samplesä¸­çš„æœ€å°å€¼ã€‚å¦å¤–ï¼Œ"arpack"ç®—æ³•å¯èƒ½ä¼šå—åˆ°çŸ©é˜µå¥‡å¼‚æ€§çš„å½±å“ï¼Œå¯¼è‡´æ±‚è§£ç»“æœä¸ç¨³å®šã€‚å› æ­¤ï¼Œåœ¨ä½¿ç”¨"arpack"ç®—æ³•æ—¶éœ€è¦æ³¨æ„è¿™äº›é™åˆ¶å’Œç¼ºç‚¹ã€‚
- åœ¨PCAä¸­ï¼Œ"svd_solver"å‚æ•°å¯ä»¥è®¾ç½®ä¸º"full"ï¼Œè¡¨ç¤ºä½¿ç”¨æ ‡å‡†çš„å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰ç®—æ³•è¿›è¡ŒçŸ©é˜µåˆ†è§£ï¼Œæ±‚è§£ä¸»æˆåˆ†ã€‚è¿™ç§æ–¹æ³•å¯ä»¥å¾—åˆ°ç²¾ç¡®çš„ä¸»æˆåˆ†è§£ï¼Œä½†æ˜¯åœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†æ—¶å¯èƒ½ä¼šå˜å¾—éå¸¸è€—æ—¶å’Œè€—å†…å­˜ã€‚å› æ­¤ï¼Œå½“æ•°æ®é›†è¾ƒå°çš„æ—¶å€™ï¼Œä½¿ç”¨"full"å¯ä»¥å¾—åˆ°è¾ƒå¥½çš„ç»“æœï¼Œä½†æ˜¯å½“å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†æ—¶ï¼Œéœ€è¦è€ƒè™‘ä½¿ç”¨å…¶ä»–æ›´é«˜æ•ˆçš„ç®—æ³•ï¼ˆæ¯”å¦‚"arpack"ï¼‰ã€‚

åœ¨PCAä¸­ï¼Œ"n_components"å‚æ•°å¯ä»¥è®¾ç½®ä¸º"mle"ï¼Œè¡¨ç¤ºä½¿ç”¨Minkaç®—æ³•æ¥ä¼°è®¡ä¿ç•™çš„ä¸»æˆåˆ†æ•°é‡ã€‚

- Minkaç®—æ³•æ˜¯ä¸€ç§åŸºäºæœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„æ–¹æ³•ï¼Œå¯ä»¥æ ¹æ®æ•°æ®é›†çš„ç‰¹å¾è‡ªåŠ¨ä¼°è®¡å‡ºæœ€ä½³çš„ä¸»æˆåˆ†æ•°é‡ã€‚
- å½“è®¾ç½®"n_components"ä¸º"mle"æ—¶ï¼Œå¦‚æœ"svd_solver"å‚æ•°ä¸º"auto"ï¼Œåˆ™ä¼šè‡ªåŠ¨å°†å…¶è§£é‡Šä¸º"svd_solver"ä¸º"full"ã€‚
- ä½¿ç”¨"mle"æ–¹æ³•å¯ä»¥é¿å…æ‰‹åŠ¨è®¾ç½®ä¸»æˆåˆ†æ•°é‡çš„ä¸»è§‚æ€§ï¼Œä½†æ˜¯è®¡ç®—é‡æ¯”è¾ƒå¤§ï¼Œå¯èƒ½ä¼šå¢åŠ ç®—æ³•çš„è¿è¡Œæ—¶é—´ã€‚

## è¡¥å……

### SVD

- SVDçš„å…¨ç§°æ˜¯å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSingular Value Decompositionï¼‰ï¼Œä¹Ÿè¢«ç§°ä¸ºå¥‡å¼‚å€¼åˆ†è§£å®šç†ã€‚SVDæ˜¯ä¸€ç§é‡è¦çš„çŸ©é˜µåˆ†è§£æ–¹æ³•ï¼Œå¯ä»¥å°†ä¸€ä¸ªçŸ©é˜µåˆ†è§£ä¸ºä¸‰ä¸ªçŸ©é˜µçš„ä¹˜ç§¯
  - å…¶ä¸­ä¸€ä¸ªçŸ©é˜µæ˜¯åŒ…å«åŸçŸ©é˜µæ‰€æœ‰ç‰¹å¾å‘é‡çš„çŸ©é˜µï¼Œè€Œå¦å¤–ä¸¤ä¸ªçŸ©é˜µæ˜¯å¯¹è§’çŸ©é˜µï¼ŒåŒ…å«äº†åŸçŸ©é˜µçš„å¥‡å¼‚å€¼ã€‚
- SVDåœ¨æ•°æ®åˆ†æã€å›¾åƒå¤„ç†ã€æœºå™¨å­¦ä¹ ç­‰é¢†åŸŸæœ‰ç€å¹¿æ³›çš„åº”ç”¨ï¼Œå…¶ä¸­åœ¨ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ä¸­ï¼ŒSVDè¢«ç”¨äºæ±‚è§£æ•°æ®é›†çš„ä¸»æˆåˆ†ã€‚



### ARPACK

- ARPACKï¼ˆå…¨ç§°ä¸ºArnoldi Packageï¼‰æ˜¯ä¸€ç§ç”¨äºæ±‚è§£å¤§å‹ç¨€ç–çŸ©é˜µç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡çš„ç®—æ³•åŒ…ã€‚
- å®ƒä½¿ç”¨éšå¼é‡æ–°å¯åŠ¨çš„Arnoldiæ–¹æ³•ï¼Œé€šè¿‡è¿­ä»£è®¡ç®—çŸ©é˜µçš„éƒ¨åˆ†ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ã€‚
- ARPACKä¸»è¦ç”¨äºè§£å†³å¤§è§„æ¨¡çš„ç‰¹å¾å€¼é—®é¢˜ï¼Œä¾‹å¦‚ç»“æ„åŠ¨åŠ›å­¦ã€é‡å­åŒ–å­¦ã€ä¿¡å·å¤„ç†ç­‰é¢†åŸŸã€‚
- åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼ŒARPACKé€šå¸¸ç”¨äºPCAç®—æ³•ä¸­çš„å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰æ±‚è§£è¿‡ç¨‹ä¸­ã€‚

## PCAæ–¹æ³•çš„ç‰¹ç‚¹

- PCAæä¾›äº†ä¸€ç§é™ä½ç»´åº¦çš„æœ‰æ•ˆåŠæ³•ï¼Œæœ¬è´¨ä¸Šï¼Œå®ƒåˆ©ç”¨æ­£äº¤å˜æ¢å°†å›´ç»•å¹³å‡ç‚¹çš„ç‚¹é›†ä¸­å°½å¯èƒ½å¤šçš„å˜é‡æŠ•å½±åˆ°ç¬¬ä¸€ç»´ä¸­å»ï¼Œå› æ­¤ï¼Œé™ä½ç»´åº¦å¿…å®šæ˜¯å¤±å»è®¯æ¯æœ€å°‘çš„æ–¹æ³•ã€‚

- PCAå…·æœ‰ä¿æŒå­ç©ºé—´æ‹¥æœ‰æœ€å¤§æ–¹å·®çš„æœ€ä¼˜æ­£äº¤å˜æ¢çš„ç‰¹æ€§ã€‚

- ç„¶è€Œï¼Œå½“ä¸ç¦»æ•£ä½™å¼¦å˜æ¢ç›¸æ¯”æ—¶ï¼Œå®ƒéœ€è¦æ›´å¤§çš„è®¡ç®—éœ€æ±‚ä»£ä»·ã€‚

- éçº¿æ€§é™ç»´æŠ€æœ¯ç›¸å¯¹äºPCAæ¥è¯´åˆ™éœ€è¦æ›´é«˜çš„è®¡ç®—è¦æ±‚ã€‚

- PCAå¯¹å˜é‡çš„ç¼©æ”¾å¾ˆæ•æ„Ÿã€‚å¦‚æœæˆ‘ä»¬åªæœ‰ä¸¤ä¸ªå˜é‡ï¼Œè€Œä¸”å®ƒä»¬å…·æœ‰ç›¸åŒçš„æ ·æœ¬æ–¹å·®ï¼Œå¹¶ä¸”æˆæ­£ç›¸å…³ï¼Œé‚£ä¹ˆPCAå°†æ¶‰åŠä¸¤ä¸ªå˜é‡çš„ä¸»æˆåˆ†çš„æ—‹è½¬ã€‚

- ä½†æ˜¯ï¼Œå¦‚æœæŠŠç¬¬ä¸€ä¸ªå˜é‡çš„æ‰€æœ‰å€¼éƒ½ä¹˜ä»¥100ï¼Œé‚£ä¹ˆç¬¬ä¸€ä¸»æˆåˆ†å°±å‡ ä¹å’Œè¿™ä¸ªå˜é‡ä¸€æ ·ï¼Œå¦ä¸€ä¸ªå˜é‡åªæä¾›äº†å¾ˆå°çš„è´¡çŒ®ï¼Œç¬¬äºŒä¸»æˆåˆ†ä¹Ÿå°†å’Œç¬¬äºŒä¸ªåŸå§‹å˜é‡å‡ ä¹ä¸€è‡´ã€‚è¿™å°±æ„å‘³ç€å½“ä¸åŒçš„å˜é‡ä»£è¡¨ä¸åŒçš„å•ä½ï¼ˆå¦‚æ¸©åº¦å’Œè´¨é‡ï¼‰æ—¶ï¼ŒPCAæ˜¯ä¸€ç§æ¯”è¾ƒæ­¦æ–­çš„åˆ†ææ–¹æ³•ã€‚
  - ä½†æ˜¯åœ¨Pearsonçš„é¢˜ä¸º "On Lines and Planes of Closest Fit to Systems of Points in Space"çš„åŸå§‹æ–‡ä»¶é‡Œï¼Œæ˜¯å‡è®¾åœ¨æ¬§å‡ é‡Œå¾—ç©ºé—´é‡Œä¸è€ƒè™‘è¿™äº›ã€‚

- ä¸€ç§ä½¿PCAä¸é‚£ä¹ˆæ­¦æ–­çš„æ–¹æ³•æ˜¯ä½¿ç”¨å˜é‡ç¼©æ”¾ä»¥å¾—åˆ°å•ä½æ–¹å·®ã€‚

- PCA is used to decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance. In scikit-learn, [`PCA`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA) is implemented as a *transformer* object that learns $n$ components in its `fit` method, and can be used on new data to project it on these components.

  PCA centers but does not scale the input data for each feature before applying the SVD. The optional parameter `whiten=True` makes it possible to project the data onto the singular space while scaling each component to unit variance. This is often useful if the models down-stream make strong assumptions on the isotropy of the signal: this is for example the case for Support Vector Machines with the RBF kernel and the K-Means clustering algorithm.

  Below is an example of the iris dataset, which is comprised of 4 features, projected on the 2 dimensions that explain most variance:

- PCAï¼ˆä¸»æˆåˆ†åˆ†æï¼‰æ˜¯ä¸€ç§å¤šå…ƒæ•°æ®é›†åˆ†è§£æ–¹æ³•ï¼Œå®ƒå°†æ•°æ®é›†åˆ†è§£æˆä¸€ç»„è¿ç»­çš„æ­£äº¤æˆåˆ†ï¼Œè¿™äº›æˆåˆ†è§£é‡Šäº†æœ€å¤§é‡çš„æ–¹å·®ã€‚åœ¨scikit-learnä¸­ï¼ŒPCAè¢«å®ç°ä¸ºä¸€ä¸ªè½¬æ¢å™¨å¯¹è±¡ï¼Œåœ¨PCAçš„æ‹Ÿåˆæ–¹æ³•ä¸­ï¼Œå®ƒä¼šå­¦ä¹ nä¸ªä¸»æˆåˆ†ã€‚å­¦ä¹ å®Œæˆåï¼ŒPCAå¯¹è±¡å¯ä»¥ç”¨äºå°†æ–°æ•°æ®æŠ•å½±åˆ°è¿™äº›ä¸»æˆåˆ†ä¸Š.

- åœ¨åº”ç”¨SVDä¹‹å‰ï¼ŒPCAä¼šå¯¹æ¯ä¸ªç‰¹å¾çš„è¾“å…¥æ•°æ®è¿›è¡Œ**ä¸­å¿ƒåŒ–**ï¼Œä½†ä¸ä¼šè¿›è¡Œç¼©æ”¾ã€‚

- å¯é€‰å‚æ•°whiten=Trueå¯ä»¥å°†æ•°æ®æŠ•å½±åˆ°å¥‡å¼‚ç©ºé—´ï¼ŒåŒæ—¶å°†æ¯ä¸ªæˆåˆ†ç¼©æ”¾åˆ°å•ä½æ–¹å·®ã€‚å¦‚æœä¸‹æ¸¸æ¨¡å‹å¯¹ä¿¡å·çš„å„å‘åŒæ€§åšå‡ºäº†å¼ºçƒˆçš„å‡è®¾ï¼Œè¿™é€šå¸¸æ˜¯å¾ˆæœ‰ç”¨çš„ï¼Œä¾‹å¦‚æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰ä½¿ç”¨å¾„å‘åŸºå‡½æ•°ï¼ˆRBFï¼‰æ ¸å’ŒKå‡å€¼èšç±»ç®—æ³•ç­‰ã€‚

  - å„å‘åŒæ€§æŒ‡çš„æ˜¯ç‰©ç†æˆ–æ•°å­¦ç³»ç»Ÿåœ¨å„ä¸ªæ–¹å‘ä¸Šçš„æ€§è´¨æ˜¯ç›¸åŒçš„ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œç³»ç»Ÿåœ¨æ—‹è½¬æˆ–å˜æ¢æ–¹å‘åï¼Œå…¶æ€§è´¨ä¸ä¼šå‘ç”Ÿæ”¹å˜ã€‚åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œå„å‘åŒæ€§é€šå¸¸ç”¨äºæè¿°æ•°æ®çš„ç‰¹å¾ï¼Œä¾‹å¦‚ï¼Œå¦‚æœæ•°æ®åœ¨å„ä¸ªæ–¹å‘ä¸Šçš„ç‰¹å¾æ˜¯ç›¸ä¼¼çš„ï¼Œåˆ™å¯ä»¥è¯´è¿™ä¸ªæ•°æ®é›†æ˜¯å„å‘åŒæ€§çš„ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨PCAç­‰é™ç»´ç®—æ³•æ¥å‡å°‘æ•°æ®çš„ç»´åº¦ï¼ŒåŒæ—¶ä¿ç•™æ•°æ®çš„ä¸»è¦ç‰¹å¾ã€‚åœ¨æŸäº›æœºå™¨å­¦ä¹ ç®—æ³•ä¸­ï¼Œå¦‚æœæ•°æ®ä¸æ˜¯å„å‘åŒæ€§çš„ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´ç®—æ³•çš„æ€§èƒ½ä¸‹é™ï¼Œå› ä¸ºç®—æ³•å¯èƒ½ä¼šåœ¨ä¸€äº›æ–¹å‘ä¸Šè¿‡åº¦æ‹Ÿåˆæ•°æ®ï¼Œè€Œåœ¨å…¶ä»–æ–¹å‘ä¸Šæ¬ æ‹Ÿåˆæ•°æ®ã€‚å› æ­¤ï¼Œåœ¨ä½¿ç”¨è¿™äº›ç®—æ³•ä¹‹å‰ï¼Œé€šå¸¸éœ€è¦å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œä»¥ç¡®ä¿æ•°æ®å…·æœ‰å„å‘åŒæ€§

- ä»¥ä¸‹æ˜¯é¸¢å°¾èŠ±æ•°æ®é›†çš„ä¸€ä¸ªç¤ºä¾‹ï¼Œè¯¥æ•°æ®é›†ç”±4ä¸ªç‰¹å¾ç»„æˆï¼Œè¢«æŠ•å½±åˆ°è§£é‡Šæœ€å¤§æ–¹å·®çš„2ä¸ªç»´åº¦ä¸Šï¼š

  - ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/6b13bf5ee9af410ca16bc31606f5c6ea.png)
  - The [`PCA`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA) object also provides a probabilistic interpretation of the PCA that can give a likelihood of data based on the amount of variance it explains. As such it implements a [score](https://scikit-learn.org/stable/glossary.html#term-score) method that can be used in cross-validation:
  - é™¤äº†èƒ½å¤Ÿå°†æ•°æ®é™ç»´å¤–ï¼ŒPCAå¯¹è±¡è¿˜æä¾›äº†PCAçš„æ¦‚ç‡è§£é‡Šï¼Œå¯ä»¥åŸºäºè§£é‡Šçš„æ–¹å·®é‡ç»™å‡ºæ•°æ®çš„å¯èƒ½æ€§ã€‚å› æ­¤ï¼ŒPCAå¯¹è±¡è¿˜å®ç°äº†ä¸€ä¸ªåˆ†æ•°ï¼ˆscoreï¼‰æ–¹æ³•ï¼Œå¯ä»¥åœ¨äº¤å‰éªŒè¯ä¸­ä½¿ç”¨ã€‚
  - ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/7466d5a49dca4bb29127c937391469c8.png)


### Examples

- [Comparison of LDA and PCA 2D projection of Iris dataset](https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_lda.html#sphx-glr-auto-examples-decomposition-plot-pca-vs-lda-py)
- [Model selection with Probabilistic PCA and Factor Analysis (FA)](https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_fa_model_selection.html#sphx-glr-auto-examples-decomposition-plot-pca-vs-fa-model-selection-py)

### eg

- ```python
  import numpy as np
  from sklearn.decomposition import PCA
  X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
  pca = PCA(n_components=2)
  pca.fit(X)
  
  print(pca.explained_variance_ratio_)
  
  print(pca.singular_values_)
  
  ```

  - ```bash
    [0.99244289 0.00755711]
    [6.30061232 0.54980396]
    ```

    

- ä¸Šè¿°ä»£ç ä½¿ç”¨äº†Pythonä¸­çš„NumPyå’Œscikit-learnåº“ï¼Œå®ç°äº†å¯¹ä¸€ä¸ªäºŒç»´æ•°æ®é›†è¿›è¡ŒPCAé™ç»´çš„è¿‡ç¨‹ã€‚å…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š

  1. å¯¼å…¥éœ€è¦çš„åº“ï¼šNumPyå’Œsklearn.decompositionä¸­çš„PCAæ¨¡å—ã€‚
  2. åˆ›å»ºä¸€ä¸ªäºŒç»´æ•°æ®çŸ©é˜µ"X"ï¼Œå…¶ä¸­åŒ…å«6ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬æœ‰2ä¸ªç»´åº¦ã€‚
  3. åˆ›å»ºä¸€ä¸ªPCAå¯¹è±¡"pca"ï¼Œå¹¶å°†"n_components"å‚æ•°è®¾ç½®ä¸º2ï¼Œè¡¨ç¤ºæˆ‘ä»¬å¸Œæœ›å°†æ•°æ®é™åˆ°2ç»´ã€‚
  4. ä½¿ç”¨"fit"æ–¹æ³•å¯¹æ•°æ®è¿›è¡Œæ‹Ÿåˆï¼Œå¾—åˆ°PCAæ¨¡å‹ã€‚
  5. ä½¿ç”¨"explained_variance_ratio_"å±æ€§è·å–æ¯ä¸ªä¸»æˆåˆ†è§£é‡Šæ–¹å·®çš„æ¯”ä¾‹ã€‚

- è¿™ç»„è®¡ç®—ç»“æœæ˜¯PCAç®—æ³•çš„è¾“å‡ºï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªæ•°ç»„[0.99244289, 0.00755711]è¡¨ç¤ºæ¯ä¸ªä¸»æˆåˆ†è§£é‡Šçš„æ–¹å·®æ¯”ä¾‹ï¼Œç¬¬äºŒä¸ªæ•°ç»„[6.30061232, 0.54980396]è¡¨ç¤ºæ¯ä¸ªä¸»æˆåˆ†çš„å¥‡å¼‚å€¼ã€‚

  æ ¹æ®æ–¹å·®æ¯”ä¾‹ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç¬¬ä¸€ä¸ªä¸»æˆåˆ†è§£é‡Šäº†çº¦99.2%çš„æ–¹å·®ï¼Œç¬¬äºŒä¸ªä¸»æˆåˆ†è§£é‡Šäº†çº¦0.8%çš„æ–¹å·®ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥ä»…ä½¿ç”¨ç¬¬ä¸€ä¸ªä¸»æˆåˆ†æ¥è¡¨ç¤ºæ•°æ®çš„å¤§éƒ¨åˆ†æ–¹å·®ï¼Œè€Œç¬¬äºŒä¸ªä¸»æˆåˆ†çš„è´¡çŒ®éå¸¸å°ï¼Œå¯ä»¥å¿½ç•¥ä¸è®¡ã€‚

  æ ¹æ®å¥‡å¼‚å€¼ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç¬¬ä¸€ä¸ªä¸»æˆåˆ†çš„å¥‡å¼‚å€¼ä¸º6.30061232ï¼Œç¬¬äºŒä¸ªä¸»æˆåˆ†çš„å¥‡å¼‚å€¼ä¸º0.54980396ã€‚å¥‡å¼‚å€¼æ˜¯è¡¡é‡ä¸»æˆåˆ†é‡è¦æ€§çš„ä¸€ä¸ªæŒ‡æ ‡ï¼Œä¸æ–¹å·®æ¯”ä¾‹å¯†åˆ‡ç›¸å…³ã€‚è¾ƒå¤§çš„å¥‡å¼‚å€¼è¡¨ç¤ºå¯¹åº”çš„ä¸»æˆåˆ†å…·æœ‰æ›´é«˜çš„é‡è¦æ€§ï¼Œå¯ä»¥å¸®åŠ©æ›´å¥½åœ°è§£é‡Šæ•°æ®ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œç¬¬ä¸€ä¸ªä¸»æˆåˆ†çš„å¥‡å¼‚å€¼è¿œå¤§äºç¬¬äºŒä¸ªä¸»æˆåˆ†çš„å¥‡å¼‚å€¼ï¼Œè¿›ä¸€æ­¥è¯´æ˜äº†ç¬¬ä¸€ä¸ªä¸»æˆåˆ†å¯¹æ•°æ®å…·æœ‰æ›´é«˜çš„é‡è¦æ€§ã€‚

### eg

- pcaå¯¹è±¡çš„å±æ€§

```python

import numpy as np
from sklearn.decomposition import PCA

rng=np.random.default_rng()
X=rng.integers(10,size=(15,10))


def check_attributes_of_pca(n_components='mle'):
    pca = PCA(n_components=n_components)
    # è®­ç»ƒPCAæ¨¡å‹ï¼Œå¹¶å¯¹æ ·æœ¬è¿›è¡Œé™ç»´
    X_pca = pca.fit_transform(X)
    # æŸ¥çœ‹PCAæ¨¡å‹çš„å„ä¸ªå±æ€§
    print("PCAæ¨¡å‹çš„ä¸»æˆåˆ†æ•°ï¼š", pca.n_components_)
    print("PCAæ¨¡å‹çš„ä¸»æˆåˆ†ï¼š", pca.components_)
    print("PCAæ¨¡å‹çš„å„ä¸»æˆåˆ†çš„æ–¹å·®å€¼ï¼š", pca.explained_variance_)
    print("PCAæ¨¡å‹å„ä¸»æˆåˆ†æ–¹å·®å€¼æ‰€å æ¯”ä¾‹ï¼š", pca.explained_variance_ratio_)
    print("PCAæ¨¡å‹çš„å‡å€¼ï¼š", pca.mean_)
    print("PCAæ¨¡å‹çš„å™ªå£°æ–¹å·®ï¼š", pca.noise_variance_)
    print("é™ç»´åçš„æ ·æœ¬çŸ©é˜µï¼š\n", X_pca)

for nc in ['mle',2,5,None]:
    check_attributes_of_pca(n_components=nc)
    print("-"*20)

```



### argmax@argmin

- $\arg{max}$å’Œ$\arg{min}$å‡½æ•°å¾€å¾€ä¸å†™æ‹¬å·,ä¸Šé¢æˆ‘ä¸ºäº†æé†’è‡ªå·±å’Œåˆå­¦è€…,ç‰¹æ„åŠ ä¸Šæ‹¬å·
  - å³$\arg{min}\:f(x)$å’Œ$\arg{min}(f(x))$è¡¨è¾¾çš„æ˜¯ä¸€æ ·çš„æ„æ€
  - å‰ä¸€ç§å†™æ³•è¦æ³¨æ„å‡½æ•°å$\arg{min}$å’Œå‚æ•°$f(x)$ä¿ç•™ç©ºæ ¼é—´éš™,è€Œä¸æ˜¯$\arg{min}\times{f(x)}$
  - ç±»ä¼¼äºåä¸‰è§’å‡½æ•°$\arcsin{x}$

#### argmax

- In [mathematics](https://en.wikipedia.org/wiki/Mathematics), the **arguments of the maxima** (abbreviated **arg max** or **argmax**) are the points, or [elements](https://en.wikipedia.org/wiki/Greatest_and_least_elements), of the [domain](https://en.wikipedia.org/wiki/Domain_of_a_function) of some [function](https://en.wikipedia.org/wiki/Function_(mathematics)) <u>at which</u> the function values are [maximized](https://en.wikipedia.org/wiki/Maxima_and_minima).

- In contrast to [global maxima](https://en.wikipedia.org/wiki/Global_maximum), which refers to <u>the largest *outputs* of a function</u>,

- arg max refers to the *inputs*, or [arguments](https://en.wikipedia.org/wiki/Argument_of_a_function), at which the function outputs are as large as possible.

- Given an arbitrary set ${X}$, a totally ordered set ${Y}$, and a function, ${f\colon X\to Y}$, the ${{\displaystyle \operatorname {argmax} }}$ over some subset ${S}$ of ${X}$ is defined by

  - $$
    {\displaystyle \operatorname {argmax} _{S}f:={\underset {x\in S}{\operatorname {arg\,max} }}\,f(x):=\{x\in S~:~f(s)\leq f(x){\text{ for all }}s\in S\}.}
    $$

  - If ${{\displaystyle S=X}}$ or ${S}$ is clear from the context, then ${S}$ is often left out, as in 

    - $$
      {\displaystyle {\underset {x}{\operatorname {arg\,max} }}\,f(x):=\{x~:~f(s)\leq f(x){\text{ for all }}s\in X\}.}
      $$

    - In other words, ${{\displaystyle \operatorname {argmax} }}$ is the set of points ${x}$ for which (Image: f(x)) attains the function's largest value (if it exists). 

  - ${{\displaystyle \operatorname {Argmax} }}$ may be the empty set, a singleton, or contain multiple elements.

  - In the fields of convex analysis and variational analysis, a slightly different definition is used in the special case where ${{\displaystyle Y=[-\infty ,\infty ]=\mathbb {R} \cup \{\pm \infty \}}}$ are the extended real numbers.

  -  In this case, if ${f}$ is identically equal to ${\infty}$ on ${S}$ then ${{\displaystyle \operatorname {argmax} _{S}f:=\varnothing }}$ (that is, ${{\displaystyle \operatorname {argmax} _{S}\infty :=\varnothing }}$) and otherwise ${{\displaystyle \operatorname {argmax} _{S}f}}$ is defined as above, where in this case ${{\displaystyle \operatorname {argmax} _{S}f}}$ can also be written as:

     - $$
       {\displaystyle \operatorname {argmax} _{S}f
       :=\left\{x\in S~:~f(x)=\sup {}_{S}f\right\}
       }
       $$

     - where it is emphasized that this equality involving <u>$\Large{{\displaystyle \sup {}_{S}f}}$</u> holds only when ${f}$ is not identically ${\infty}$ on ${S}$.

##### ç¬¦å·è¯´æ˜

- `:=`è¡¨ç¤º**å®šä¹‰ä¸º**
- $\sup_sf$è¡¨ç¤º$f$åœ¨å®šä¹‰åŸŸ$S$å†…çš„æœ€å¤§å€¼
  - where it is emphasized that this equality involving $\sup_sf$  holds only when $f$ is not identically $\infty$ on S.
  - è¿™é‡Œå¼ºè°ƒçš„æ˜¯ï¼Œè¿™ä¸ªåŒ…å«$\sup_sf$çš„ç­‰å¼åªæœ‰å½“Sä¸Šçš„$f$ä¸æ’ç­‰äº$\infty$æ—¶æ‰æˆç«‹ã€‚

#### Arg min

- The notion of  ${\displaystyle \operatorname {argmin} }$ (or  ${\displaystyle \operatorname {arg\,min} }$), which stands for argument of the minimum, is defined analogously. 

- For instance,
  $$
  {\displaystyle {\underset {x\in S}{\operatorname {arg\,min} }}\,f(x):=\{x\in S~:~f(s)\geq f(x){\text{ for all }}s\in S\}}
  $$

  - are points  $x$ for which  $f(x)$ attains its smallest value.
  - It is the complementary operator of  ${\displaystyle \operatorname {arg\,max} }$

- ç‰¹æ®Šæƒ…å†µ

  - In the special case where  ${\displaystyle Y=[-\infty ,\infty ]=\mathbb {R} \cup \{\pm \infty \}}$ are the extended real numbers, 
  - if  $f$ is **identically equal**(æ’ç­‰äº) to  $-\infty$ on  $S$ then  ${\displaystyle \operatorname {argmin} _{S}f:=\varnothing }$
    - that is,  ${\displaystyle \operatorname {argmin} _{S}-\infty :=\varnothing }$ 
  - and otherwise  ${\displaystyle \operatorname {argmin} _{S}f}$ is defined as above and moreover, 
    - in this case (of  $f$ not identically equal to  $-\infty$) it also satisfies:
    - ${\displaystyle \operatorname {argmin} _{S}f:=\left\{x\in S~:~f(x)=\inf {}_{S}f\right\}.}$





























â€‹	