[toc]



## 范数

- 有时我们需要衡量一个**向量的大小**。
  - 在机器学习中，我们经常使用被称为 **范数**（norm）的**函数**衡量向量大小。

- 严格地说,**范数**可以是满足以下性质的任意函数:
- 半范数:
  - $f(x)\geqslant{0}$
    - 半正定性
  - $f(x+y)\leqslant{f(x)+f(y)}$,(次可加性)
    - 即三角不等式,
    - 例如函数$f(x)=|x|$就满足$|x+y|\leqslant{|x|+|y|}$
  - $\forall{a}\in\mathbb{R},f(ax)=|a|f(x)$
    - 具有绝对一次齐次性
- **范数**是一个**半范数**加上额外性质：
  - $f(x)=0\Rightarrow{x=0}$(正定性)

### ref

- [范数 (wikipedia.org)](https://zh.wikipedia.org/zh-cn/范数)

- **范数**（英语：Norm），是具有“长度”概念的函数。

  - 在[线性代数](https://zh.wikipedia.org/wiki/線性代數)、[泛函分析](https://zh.wikipedia.org/wiki/泛函分析)及相关的数学领域，是一个[函数](https://zh.wikipedia.org/wiki/函數)，其为[向量空间](https://zh.wikipedia.org/wiki/向量空間)内的所有[向量](https://zh.wikipedia.org/wiki/向量)赋予非零的正**长度**或**大小**。

- 另一方面，**半范数**（英语：seminorm）可以为非零的[向量](https://zh.wikipedia.org/wiki/向量)赋予零长度。

- 例，一个二维度的欧氏几何空间$\mathbb {R} ^{2}$就有欧氏范数。在这个[向量空间](https://zh.wikipedia.org/wiki/向量空間)的元素（譬如：(3,7)）常常在[笛卡尔坐标系统](https://zh.wikipedia.org/wiki/直角坐标系)被画成一个从原点出发的箭号。每一个[向量](https://zh.wikipedia.org/wiki/向量)的欧氏范数就是箭号的长度。

  拥有范数的[向量空间](https://zh.wikipedia.org/wiki/向量空間)就是[赋范向量空间](https://zh.wikipedia.org/wiki/賦範向量空間)。同样，拥有半范数的[向量空间](https://zh.wikipedia.org/wiki/向量空間)就是赋半范向量空间。

### $L^p$范数

- [Lp範數  (wikipedia.org)](https://zh.wikipedia.org/wiki/Lp范数)

- 形式上，$L^p$ 范数定义如下$||x||_{p}$:

  - $$
    \large||x||_{p}=\left(\sum_{i}|x_i|^p\right)^{\frac{1}{p}}\label{Lp}
    $$

    - $x_i$是向量$x$的元素
    - 其中$p\in\mathbb{R},p\geqslant{1}$
    - $\frac{1}{p}\in(0,1]$

  - 范数是将向量映射到**非负值**(容易证明$L^p\geqslant{0}$)

    - 由幂函数的知识,在函数$f(x)=x^p(p>0)$是递增函数
    - $|x_i|\geqslant{0}$,则$0=0^{p}\leqslant{|x_i|^p}$
    - 所以$\sum_{i}|x_i|^p\geqslant{0}$
    - $||x||_p\geqslant{0}$

  - 补充:由指数函数知识,$g(x)=t^x(0<t<1)$是递减的,在$t>1$是递增的

    - 如果$|x_i|<1$,则$\sum_{i}|x_i|^p\leqslant{\sum_{i}|x_i|}$
    - 如果$|x_i|>1$,则:$\sum_{i}|x_i|^p\geqslant{\sum_{i}|x_i|}$

    

- 向量x的范数衡量从原点(零向量)到点x的距离

### $L^2$范数@欧几里得距离

- 当$p=2$时,$L^2$范数被称为**Euclidean norm**(欧几里得范数)

  - $||x||^2$破坏了范数规则,比如**次可加性**

  - 它表示从**原点**出发到向量$x$确定的点的**欧几里得距离**

- [欧几里得距离 (wikipedia.org)](https://zh.wikipedia.org/wiki/欧几里得距离)

  - 对于n维向量空间,原点$O=(0,0,\cdots,0)$到$x=(x_1,x_2,\cdots,x_n)$描述的点的欧式距离

  - $$
    \|{\vec  {x}}\|_{2}={\sqrt  {|x_{1}|^{2}+\cdots +|x_{n}|^{2}}}
    \\
    ||\mathbf{x}||_2=\sqrt{\sum_{i}^{n}(x_i)^2}
    $$
    


- 尽管如此,但是在述**误差矩阵**矩阵的时候,常常会以形如$D=A-B$;$D_{ij}=(A_{ij}-B_{ij})_{m\times{n}}$

- 那么D的向量$d^{(i)}$的范数就表示为

  - 符号说明:$\mathbf{x}^{(i)}$表示某个矩阵中的第i个向量(通常指列向量),在可以区分的情况下,下面可能不使用粗体来表示,直接作$x^{(i)}$

  - $x^{(i)}_j$表示第i个向量的第j个元素

  - $d^{(i)}=a^{(i)}-b^{(i)}$

  - $$
    ||{d^{(i)}}||_2=\sqrt{\sum_{j=1}^{n}(d_j^{(i)})^2}
    =\sqrt{\sum_{j=1}^{n}(a_j^{(i)}-b_j^{(i)})^2}
    $$

  - 公式逆用,可以将求和号化简甚至消掉(也可以起到减少求和指数的作用)

    - $$
      \sqrt{\sum_{i}(x_i-y_i)^2}=||x||_2
      \\
      \sum_{i}(x_i-y_i)^2=||x||_2^2
      $$

    - 例如

      - $$
        \sum_{i}\sum_{j}(x_{ij})^2
        =\sum_{i}||x_i||_2^2
        \\
        \sum_{i}\sum_{j}(x_{ij}-y_{ij})^2
        =\sum_{i}||x_i-y_i||_2^2
        $$

        

      

#### 更一般的欧式距离

- 更一般的,从点$p$到$q$的欧几里得距离:

  - $$
    d(\mathbf {p,q})= \sqrt{\sum \limits_{i=1}^n (q_i-p_i)^2}
    $$

    - $p,q$	=	two points in Euclidean n-space
      $q_i, p_i$	=	Euclidean vectors, starting from the origin of the space (initial point)
      $n$	=	n-space

  - 或描述为:
    $$
    {\displaystyle d(x,y)={\sqrt {(x_{1}-y_{1})^{2}+(x_{2}-y_{2})^{2}+\cdots +(x_{n}-y_{n})^{2}}}}
    $$



### 平方$L^2$范数

- $L^2$ 范数在机器学习中出现地十分频繁
  - 平方$L^2$范数$||x||_2^2$,经常简化表示为 $∥x∥$，略去了角标2
  - <u>平方 $L^2$ 范数</u>也经常用来衡量向量的大小，可以简单地通过**点积**  ($x^Tx$)计算。
  - 平方 $L^2$ 范数在数学和计算上都比 <u>$L^2$ 范数本身</u>更方便。
    - 例如，平方 $L^2$ 范数对$x$中每个元素的导数只取决于对应的元素，
      - 而 $L^2$ 范数对每个元素的<u>导数却和整个向量相关</u>。
    - 但是在很多情况下，平方 $L^2$ 范数也可能不受欢迎，因为它在原点附近增长得十分缓慢。


### $L^1$范数

- 在某些机器学习应用中，区分恰好是零的元素和非零但值很小的元素是很重要的。

  - 在这些情况下，我们转而使用在各个位置斜率相同，同时保持简单的数学形式的函数：L1 范数。

  - L1 范数可以简化如下：

    - $$
      ||x||_1=\sum_i|x_i|
      $$


### 最大范数

- $$
  ||x||_{\infin}=max(x_1,x_2,\cdots,x_n)
  \\
  或描述为:
  \\
  {\displaystyle \lVert {\vec {x}}\rVert _{\infty }=\lim _{p\to +\infty }{\Bigl (}\sum \limits _{i=1}^{n}|x_{i}|^{p}{\Bigr )}^{1/p}=\max _{i}|x_{i}|}
  $$

### 向量点积用范数表示

- $$
  x^Ty=||x||_2||y||_2\cos{\theta}
  $$

  - 其中$\theta$表示$x,y$之间的夹角
  - $x^Ty=(x,y)=\sum_{i=1}^{n}x_iy_i$
  - $\cos\theta=\frac{x\cdot y}{|x||y|}=\frac{(x,y)}{||x||_2||y||_2}$
    - $|x|=||x||_2$
    - $|y|=||y||_2$
  - $x^Ty=(x,y)=||x||_2||y||_2\cos{\theta}$

###  衡量矩阵大小Frobenius Norm

- 有时候我们可能也希望衡量矩阵的大小。

- 在深度学习中，最常见的做法是使用 Frobenius 范数（Frobenius norm）

  - $$
    ||A||_{F}=\sqrt{\sum\limits_{i,j}(A_{i,j})^2}
    \\
    =\sqrt{Tr(AA^T)}
    \\
    A=\begin{pmatrix}
    	a_{11}  &a_{12}  &\cdots  &a_{1n}  	\\
    	a_{21}  &a_{22}  &\cdots  &a_{2n}  	\\
    	\vdots  &\vdots  &        &\vdots  	\\
    	a_{m1}  &a_{m2}  &\cdots  &a_{mn}  	\\
    \end{pmatrix}
    \\
    B=A^T=\begin{pmatrix}
      	a_{11}  &a_{21}  &\cdots  &a_{m1}  	\\
      	a_{12}  &a_{22}  &\cdots  &a_{m2}  	\\
      	\vdots  &\vdots  &        &\vdots  	\\
      	a_{1n}  &a_{2n}  &\cdots  &a_{mn}  	\\
      \end{pmatrix}
    $$
  
    - 其中$A_{i,j}$是矩阵A的第i行第j列元素
    - $b_{ij}$表示A转置后的结果矩阵B的第i行第j列元素,并且对应于矩阵A的$a_{ji}$
      - $b_{ij}=a_{ji}$这个公式告诉我们,转置后的第i行第j列元素是原矩阵的第j行第i列元素
      - $i,j$的取值范围在转置前后没有发生变化
  
  - $$
    AA^T\in\mathbb{R}^{m\times{m}}
    \\
    C=AA^T
    \\C的任意一个位置的元素可以表示为:
    \\
    c_{ij}=\sum\limits_{i=k}^{n}a_{ik}b_{kj}
    =\sum\limits_{k=1}^{n}a_{ik}a_{jk}
    \\
    特别的,当i=j时,c_{ii}=\sum\limits_{k=1}^{n}a_{ik}a_{ik}
    =\sum\limits_{k=1}^{n}a_{ik}^2
    \quad i=1,2,\cdots,m
    \\
    \sum\limits_{i=1}^{m}c_{ii}=\sum\limits_{i=1}^{m}\sum_{k=1}^{n}a_{ik}^2
    =\sum_{ij}(a_{ij})^2
    $$
  
    - 因此,如果使用迹运算表示$Tr(AA^T)=\sum_{ij}(a_{ij})^2$
    - 那么Frobenius 范数就可以表示为$||A||_{F}=Tr(AA^T)$
    - 其类似于$L^2$范数
  
  - 事实上,矩阵乘法链中的矩阵调换位置后(如果仍然可以执行乘法运算),调换前后矩阵链乘的迹保持不变
  
    - $Tr(AA^T)=Tr(A^TA)$

### pytorch计算范数🎈

- Use [`torch.linalg.vector_norm()`](https://pytorch.org/docs/stable/generated/torch.linalg.vector_norm.html#torch.linalg.vector_norm) when computing **vector norms** and [`torch.linalg.matrix_norm()`](https://pytorch.org/docs/stable/generated/torch.linalg.matrix_norm.html#torch.linalg.matrix_norm) when computing **matrix norms**.

## 特殊类型矩阵和向量

### 对角阵

- [Diagonal matrix - Wikipedia](https://en.wikipedia.org/wiki/Diagonal_matrix)
- [Main diagonal - Wikipedia](https://en.wikipedia.org/wiki/Main_diagonal)

- 对角矩阵（diagonal matrix）只在**主对角线**上含有非零元素，其他位置都是零。

  - 形式上,设矩阵D满足,$D_{ij}=0$,if $i\neq{j}$,则D是对角阵

    - 啰嗦的讲:

      - $$
        D_{ij}=
        \begin{cases}
        0,&i\neq{j}\\
        x,&i =j
        \end{cases}
        \quad x可以是任何数
        $$

  - 矩阵主对角线上的元素是$D_{ij},i=j$的元素

    - 非方阵矩阵也有主对角线元素,并且主对角线长度取决于行数和列数种的较小者

    - $$
      {\displaystyle {\begin{bmatrix}\color {red}{1}&0&0\\0&\color {red}{1}&0\\0&0&\color {red}{1}\end{bmatrix}}\qquad {\begin{bmatrix}\color {red}{1}&0&0&0\\0&\color {red}{1}&0&0\\0&0&\color {red}{1}&0\end{bmatrix}}\qquad {\begin{bmatrix}\color {red}{1}&0&0\\0&\color {red}{1}&0\\0&0&\color {red}{1}\\0&0&0\end{bmatrix}}\qquad {\begin{bmatrix}\color {red}{1}&0&0&0\\0&\color {red}{1}&0&0\\0&0&\color {red}{1}&0\\0&0&0&\color {red}{1}\end{bmatrix}}\qquad }
      $$

      - 主对角线元素用红色表出
      - 且这些矩阵都是满足**对角阵**的定义(对于非方阵而言,对角阵的主对角线两侧0的分布不对称)
        - 非零元素的占比:设q=min(m,n),其中m,n分别是对角阵D的行数和列数
        - 则对角阵D种的非零元素比例不超过$\frac{q}{q^2}=\frac{1}{q}$
        - 这是计算机计算对角阵乘法快速的原因之一

- 我们用 $diag(v)$ 表示一个对角元素由向量 v 中元素给定的对角方阵。

- 对角矩阵受到关注的部分原因是对角矩阵的乘法计算很高效。

  - 计算乘法 $diag(v)x$，我们只需要将 x 中的每个元素 $x_i$ 放大 $v_i$ 倍。换言之，$diag(v)x = v ⊙ x$。
  - 计算<u>对角**方阵**</u>的逆矩阵也很高效。
    - 在很多情况下，我们可以根据任意矩阵导出一些通用的机器学习算法；
    - 但通过将一些矩阵限制为对角矩阵，我们可以得到计算代价较低的（并且简明扼要的）算法。

- 不是所有的对角矩阵都是方阵。🎈

  - 长方形的矩阵也有可能是对角矩阵。
  - 非方阵的对角矩阵没有逆矩阵，但我们仍然可以高效地计算它们的乘法。
  - 对于一个长方形对角矩阵 D 而言,和向量$x$的乘法 $Dx$ 会涉及到向量$x$ 中每个元素的缩放
    - 如果 D 是<u>瘦长型矩阵</u>，那么在缩放后的末尾添加一些零；
    - 如果 D 是<u>胖宽型</u>矩阵，那么在缩放后去掉最后一些元素。

  - $$
    \Sigma_1=
    \begin{pmatrix}
    	1 &0 &0 &0 \\
    	0 &2 &0 &0 \\
    	0 &0 &3 &0 \\
    \end{pmatrix}
    \\
    \Sigma_2=
    \begin{pmatrix}
    	1 &0 &0 \\
    	0 &2 &0 \\
    	0 &0 &3 \\
    	0 &0 &0 \\
    \end{pmatrix}
    \\\Sigma_1\Sigma_2=
    \begin{pmatrix}
    	1 &0 &0 \\
    	0 &2 &0 \\
    	0 &0 &3 \\
    \end{pmatrix}
    $$

  - 设$\Sigma\in\mathbb{R}^{m\times{n}}$,$\Sigma^T\in\mathbb{R}^{n\times{m}}$

    - $\Lambda_1=\Sigma\Sigma^T\in\mathbb{R}^{n\times{n}}$
    - $\Lambda_2=\Sigma^T\Sigma\in\mathbb{R}^{n\times{n}}$



## 正交

### 向量正交

- 如果 $(x,y)=x^Ty = 0$，那么<u>列向量</u> x 和向量 y 互相 **正交**（orthogonal）,记为$x\perp{y}$
  - 如果两个向量都有**非零范数**(长度大于0)，那么这两个<u>向量之间的夹角是 90 度</u>。

- 在 $\mathbb{R}^n$ 中，至多有 n 个范数非零向量互相正交。

  - 如果这些向量不仅互相正交，并且<u>范数都为 1</u>，那么我们称它们是 **标准正交**（orthonormal）。

### 正交向量组

- 若$\Phi=\alpha_1\cdots,\alpha_n$,$\alpha_i\neq{0},i=1,2,\cdots,n$中向量两两正交,$(\alpha_i,\alpha_j)=0,(i\neq{j}),i,j=1,2,\cdots,n$,则称$\Phi$是一个**正交向量组**

  - 显然有

    - $$
      (\alpha_i,\alpha_j)
      \begin{cases}
      0,&i\neq{j}\\
      R^+,&i=j
      \end{cases}
      \\R^+>0
      $$
      
      

- 正交向量组$\Phi_{\perp}$线性无关

  - 证明

    - 设$\beta=\alpha_{p},p\in\{1,2,\cdots,n\}$

    - 设存在常数$K=k_1,\cdots,k_n$

      - $\sum_{i}^{n}k_i\alpha_i=0$

      - 两边同时和$\beta$做内积

      - $(\beta,\sum_{i}^{n}k_i\alpha_i)=0$

      - $$
        \sum_i^n(\beta,k_i\alpha_i)
        =\sum_i^nk_i(\beta,\alpha_i)=0
        $$

    - 由于

      - $(\alpha_p,\alpha_i)=0$,if $p\neq{i}$
      - $(\alpha_p,\alpha_i)>0$,if $p=i$

    - 所以

      - $$
        k_i(\beta,\alpha_i)=0,i\neq{p}
        $$

        - 或者描述为:
          $$
          或\Large k_{\overline{p}}(\alpha_p,\alpha_{\overline{p}})=0
          \\
          或\Large k_{-p}(\alpha_p,\alpha_{-{p}})=0
          \\
          \overline{p}和-p都表示不等于p的数
          $$

      - $k_p(\beta,\alpha_p)=0,k_p=0$🎈

    - 类似的,当$\beta=\alpha_p$,p取遍$1,2,\cdots,n$,可得$k_1=k_2=\cdots=k_n=0$

### 标准正交基@Kronecker函数

- 设正交向量组$\Phi$的每个向量都是单位向量,则称$\Phi$为标准正交基(规范正交基),还可以描述为:

  - $$
    (\alpha_i,\alpha_j)=\delta_{ij}
    =\begin{cases}
    1,&i=j\\
    0,&i\neq{j}
    \end{cases}
    \quad(i,j=1,2,\cdots<n)
    $$

- 专用符号$\delta_{ij}$是Kronecker符号

  - 在数学中，**克罗内克函数**（又称克罗内克δ函数、克罗内克δ)$\delta$ 是一个[二元函数](https://zh.wikipedia.org/wiki/函數#多元函數)，得名于德国数学家[利奥波德·克罗内克](https://zh.wikipedia.org/wiki/利奥波德·克罗内克)。

  - <u>克罗内克函数的自变量（输入值）一般是两个[整数](https://zh.wikipedia.org/wiki/整数)，</u>

    - 如果两者相等，则其输出值为1

    - 否则为0

    - $$
      \delta _{{ij}}=\delta(i,j)
      =\left\{{\begin{matrix}1&(i=j)\\0&(i\neq j)\end{matrix}}\right.\,\!
      $$

      

  - 克罗内克函数的值一般简写为 $\delta_{ij}$

  - 克罗内克函数和[狄拉克δ函数](https://zh.wikipedia.org/wiki/狄拉克δ函数)都使用δ作为符号

    - 但是克罗内克δ用的时候带两个下标，

    - 而狄拉克δ函数则只有一个变量。



### 正交化(schmidt)

- 一个向量组线性无关是该型两组称为正交向量组的**必要条件**(却不充分条件)

- 对于一个线性无关组$\Phi$,可以通过施密特正交化方法,求出一个等价的正交向量组$\Psi$,$\Psi\sim{\Phi}$

  - 这是一种递推计算的方法

- 设$\Phi=(\alpha_1,\alpha_2,\cdots,\alpha_n)$

  - 令

    - $$
      \beta_{i}=\alpha_i-\sum_{i=1}^{s}(\frac{(\alpha_i,\beta_{i-1})}{\beta_{i-1},\beta_{i-1}})\beta_{i-1}
      \\=\alpha_i-\sum_{i=1}^{n}\frac{(\alpha_i,\beta_{i-1})}{||\beta_{i-1}||^2}\beta_{i-1}
      \\令\beta_0=0
      \\
      i=1,2,\cdots,n
      $$

  - 令$\Psi=\beta_1,\cdots,\beta_n$,$\Psi$正交向量组

  - 则$\Psi\cong{\Phi}$(等价)

### 正交矩阵🎈

- 正交矩阵是一种特殊的**可逆矩阵**

- 在[矩阵论](https://zh.wikipedia.org/wiki/矩阵论)中，**正交矩阵**（英语：**orthogonal matrix**）是一个[方块矩阵](https://zh.wikipedia.org/wiki/方块矩阵)，其元素为[实数](https://zh.wikipedia.org/wiki/实数)，而且行向量与列向量皆为[正交](https://zh.wikipedia.org/wiki/正交)的[单位向量](https://zh.wikipedia.org/wiki/单位向量)，使得该矩阵的[转置矩阵](https://zh.wikipedia.org/wiki/转置矩阵)为其[逆矩阵](https://zh.wikipedia.org/wiki/逆矩阵)：

  - $$
    Q^{T}=Q^{-1}\Leftrightarrow Q^{T}Q=QQ^{T}=I.\,\!
    $$

    

- 设n阶方阵A满足$A^TA=E$(或$AA^T=E$),则称A是正交矩阵(正交阵)

  - $$
    A^TA=E
    \\
    A^TA=
    \begin{pmatrix}
    \alpha_1^T\\
    \vdots\\
    \alpha_n^T
    \end{pmatrix}
    (\alpha_1,\cdots,\alpha_n)
    \\
    =\begin{pmatrix}
    \alpha_1^T\alpha_1&\alpha_1^T\alpha_2&\cdots&\alpha_1^T\alpha_n\\
    \alpha_2^T\alpha_1&\alpha_2^T\alpha_2&\cdots&\alpha_2^T\alpha_n\\
    \vdots&\vdots&&\vdots\\
    \alpha_n^T\alpha_1&\alpha_n^T\alpha_2&\cdots&\alpha_n^T\alpha_n
    \end{pmatrix}
    =\begin{pmatrix}
    1&0&\cdots&0\\
    0&1&\cdots&0\\
    \vdots&\vdots&&\vdots\\
    0&0&\cdots&1
    \end{pmatrix}
    \\
    \alpha_i^T\alpha_j=(\alpha_i,\alpha_i)=
    \begin{cases}
    0,&i\neq{j}\\
    1,&i=j
    \end{cases}
    \\i=1,2,\cdots,n,即\alpha_i是单位向量
    $$

  - 可见,方阵A是正交向量当且仅当(充分必要条件是)A的列(行)向量都是单位向量,且两两正交

    - 上面的推导是根据$A^TA=E$,是关于A的列向量,使用$AA^T=E$可以推到出A的行向量的相同jie'lun
  
  - n阶正交矩阵的n个列(行)向量构成n维向量空间$R^n$的一个**标准正交基**

#### 性质👺

- 正交矩阵的逆:$A^{-1}=A^T$
  - 由$(A^TA)=E$和可逆矩阵的定义可以推出$A^T=A^{-1}$
    - 即有$A^{-1}A=AA^{-1}=E$
    - 因此$A^TA=AA^T=E$
- $A^{-1}$依然是正交矩阵
  - $A^{-1}=A^T$
  - 因为$(A^{-1})^T=(A^T)^T=A$
  - $(A^{-1})^T(A^{-1})=AA^T=E$
  - 所以$A^{-1}$依然可逆
- $|A|=1$
  - $|A^TA|=|E|$
  - $|A^T||A|=1$
  - $|A^T|=|A|,|A|^2=1$
  - $|A|=\pm 1$
- $(A^*)^TA^*=E$
  - 证明:
    - $A^{-1}=|A|^{-1}A^{*}$
    - $(A^{-1})^T(A^{-1})=E$
    - $(|A|^{-1}A^{*})^T|A|^{-1}A^{*}=E$
    - $|A|^{-1}(A^*)^T|A|^{-1}A^*=E$
      - $|A|^2=1$
      - $|A|^{-2}=1$
    - $(A^*)^TA^*=E$
- $C=AB$,有$C^TC=E$
  - 其中A,B为同阶正交矩阵,$A^TA=E,B^TB=E$,
  - 证:
    - $(AB)^T(AB)=B^TA^TAB=B^T(A^TA)B=B^TEB=B^TB=E$
    - 因此$C=(AB)$为正交矩阵
- $|\lambda_i|=1$
  - 其中$\lambda_i,i=1,2,\cdots,n$是A的特征值
  - 证明:
    - 设$\lambda_i,\alpha_i$满足$A\alpha_i=\lambda_i\alpha_i$,$i=1,2,\cdots,n$
    - 则$(A\alpha_i)^T=(\lambda_i\alpha_i)^T$
    - $\alpha_i^TA^T=\lambda_i\alpha_i^T$
    - 因为,$A^TA=E$,考虑对上式同时右乘以$A$
    - $\alpha_i^TA^TA=\lambda_i\alpha_i^TA$
    - $\alpha_i^TE=\lambda_i\alpha_i^TA$
    - 再对上式右乘以$\alpha_i$
    - $\alpha_i^T\alpha_i=\lambda_i\alpha_i^TA\alpha_i$
    - $\lambda_i\alpha_i^TA\alpha_i=\lambda_i\alpha_i^T\lambda_i\alpha_i=\lambda_i^2\alpha_i^T\alpha_i$
    - $\alpha_i^T\alpha_i=\lambda_i^2\alpha_i^T\alpha_i$
    - $(1-\lambda_i^2)\alpha_i^T\alpha_i=0$
    - 由于$\alpha_i\neq{0}$,所以$\alpha_i^T\alpha_i=\sum_{j=1}^{n}\alpha_{ij}^2>0$
    - 从而$1-\lambda_i^2=0$
    - 从而$\lambda_i^2=1,\lambda_i=\pm{1}$
  - 推广到酉矩阵,也有相仿的结论

#### 矩阵是正交矩阵的充要条件

- n阶方阵Q的行(列)向量组是标准正交基是Q为正交矩阵的充要条件

  - 设$Q=(\alpha_1,\cdots,\alpha_n)^T$

    - $\alpha_i$是**行向量**

  - $Q^T=(\alpha_1^T,\cdots,\alpha_n^T)$

    - $\alpha_i^T$是列向量

  - $QQ^T=Q^TQ=E$

  - $$
    QQ^T=
    \begin{pmatrix}
    \alpha_1\\
    \alpha_2\\
    \vdots\\
    \alpha_n
    \end{pmatrix}
    (\alpha_1^T,\cdots,\alpha_n^T)
    \\
    =\begin{pmatrix}
    \alpha_1\alpha_1^T&\alpha_1\alpha_2^T&\cdots&\alpha_1\alpha_n^T\\
    \alpha_2\alpha_1^T&\alpha_2\alpha_2^T&\cdots&\alpha_2\alpha_n^T\\
    \vdots&\vdots&&\vdots\\
    \alpha_n\alpha_1^T&\alpha_n\alpha_2^T&\cdots&\alpha_n\alpha_n^T
    \end{pmatrix}
    =E=
    \begin{pmatrix}
    1&0&\cdots&0\\
    0&1&\cdots&0\\
    \vdots&\vdots&&\vdots\\
    0&0&\cdots&1
    \end{pmatrix}
    $$

    - $$
      \alpha_i\alpha_j^T=(\alpha_i,\alpha_j)
      =\begin{cases}
      1,&i=j\\
      0,&i\neq{j}
      \end{cases}
      \quad(i,j=1,2,\cdots，n)
      \\
      $$

    - 说明矩阵$Q$的行向量组$\alpha_1,\cdots,\alpha_n$是**标准正交向量组**

  - 类似的

    - 对于$Q^TQ=E$,$Q^T$的行向量组$\beta_1,\cdots,\beta_n$是标准正交向量组
    - 而$Q^T$的行向量就是Q的列向量,因此Q的列向量也是标准正交向量组

  - 如果Q的行向量组是标准正交向量组,那么Q是正交矩阵

    - 设Q的行向量组为$\Phi=\alpha_1,\cdots,\alpha_n$,$\Phi$是个标准正交向量组,则

      - $$
        (\alpha_i,\alpha_j)
        =\begin{cases}
        1,&i=j\\
        0,&i\neq{j}
        \end{cases}
        \quad(i,j=1,2,\cdots，n)
        \\
        (\alpha_i,\alpha_j)=\alpha_i\alpha_{j}^T
        \\
        即:\begin{pmatrix}
        \alpha_1\alpha_1^T&\alpha_1\alpha_2^T&\cdots&\alpha_1\alpha_n^T\\
        \alpha_2\alpha_1^T&\alpha_2\alpha_2^T&\cdots&\alpha_2\alpha_n^T\\
        \vdots&\vdots&&\vdots\\
        \alpha_n\alpha_1^T&\alpha_n\alpha_2^T&\cdots&\alpha_n\alpha_n^T
        \end{pmatrix}
        =\begin{pmatrix}
        1&0&\cdots&0\\
        0&1&\cdots&0\\
        \vdots&\vdots&&\vdots\\
        0&0&\cdots&1
        \end{pmatrix}
        \\
        \begin{pmatrix}
        \alpha_1\\
        \alpha_2\\
        \vdots\\
        \alpha_n
        \end{pmatrix}
        (\alpha_1^T,\cdots,\alpha_n^T)=E
        \\QQ^T=E
        \\所以Q是正交矩阵
        $$

    - 类似的$Q^T$的行向量组是标准正交向量组,则$Q^TQ=E$

      - $Q^T$的行向量组就是Q的列向量组,从而Q的列向量组是表征正交向量组可以推出Q是正交矩阵

## 对称矩阵

- 若方阵A满足$A^T=A$,则$A$是对称阵
  - $a_{ij}=a_{ji},(i,j=1,2,\cdots{n})$
- 若方阵A满足$A^T=-A$,则$A$是反对称阵
  - $a_{ij}=-a_{ji},(i,j=1,2,\cdots{n})$
  - 反对称阵的主对角线全为0

## 正交相似

- 如果方阵A:$Q^{-1}AQ=B$,(Q为正交矩阵($Q^TQ=E$),则称A(关于Q)正交相似于$B$

## 概念区分🎈

- 区分**相似对角化**和**正交相似**
  - 基本相似$A\sim{B}$:$P^{-1}AP=B$
    - 相似对角化要求B是某个对角阵$\Lambda$
    - 后者要求P是个正交矩阵($P^TP=E$)
- 另外还要区分**对称**和**正交**,两者都涉及到**方阵**的**转置**

- 不是所有方阵都可以对角化

### 正交相似对角化

- 对于方阵A,存在正交矩阵Q,使得$Q^{-1}AQ=\Lambda$,则A可以被**正交相似对角化**(简称**正交对角化**)

#### 实对称方阵A正交对角化方法

- 求出实对称阵A的全部特征值(对称阵才可以正交对角化)
  - 如果特征值$\lambda_i$是单根,则从$f(\lambda_i)=0,即(\lambda_iE-A)x=0$对应的求出一个特征向量$\alpha_i$
  - 如果特征值$\lambda_i$是$n_i$重根,
    - 则从$f(\lambda_i)=0$求出$n_i$个线性无关特征向量$\Phi_i=\alpha_{i_1},\cdots,\alpha_{n_i}$
    - 对$\Phi_i$执行Schmidt正交化
    - 在执行Normalization单位化
  - 将得到的所有向量依此排列起来得到正交矩阵Q=$(\alpha_1,\cdots,\alpha_n)$

