[toc]



## 范数

- 有时我们需要衡量一个**向量的大小**。
  - 在机器学习中，我们经常使用被称为 **范数**（norm）的**函数**衡量向量大小。

- 严格地说,**范数**可以是满足以下性质的任意函数:
- 半范数:
  - $f(x)\geqslant{0}$
    - 半正定性
  - $f(x+y)\leqslant{f(x)+f(y)}$,(次可加性)
    - 即三角不等式,
    - 例如函数$f(x)=|x|$就满足$|x+y|\leqslant{|x|+|y|}$
  - $\forall{a}\in\mathbb{R},f(ax)=|a|f(x)$
    - 具有绝对一次齐次性
- **范数**是一个**半范数**加上额外性质：
  - $f(x)=0\Rightarrow{x=0}$(正定性)

### ref

- [范数 (wikipedia.org)](https://zh.wikipedia.org/zh-cn/范数)

- **范数**（英语：Norm），是具有“长度”概念的函数。

  - 在[线性代数](https://zh.wikipedia.org/wiki/線性代數)、[泛函分析](https://zh.wikipedia.org/wiki/泛函分析)及相关的数学领域，是一个[函数](https://zh.wikipedia.org/wiki/函數)，其为[向量空间](https://zh.wikipedia.org/wiki/向量空間)内的所有[向量](https://zh.wikipedia.org/wiki/向量)赋予非零的正**长度**或**大小**。

- 另一方面，**半范数**（英语：seminorm）可以为非零的[向量](https://zh.wikipedia.org/wiki/向量)赋予零长度。

- 例，一个二维度的欧氏几何空间$\mathbb {R} ^{2}$就有欧氏范数。在这个[向量空间](https://zh.wikipedia.org/wiki/向量空間)的元素（譬如：(3,7)）常常在[笛卡尔坐标系统](https://zh.wikipedia.org/wiki/直角坐标系)被画成一个从原点出发的箭号。每一个[向量](https://zh.wikipedia.org/wiki/向量)的欧氏范数就是箭号的长度。

  拥有范数的[向量空间](https://zh.wikipedia.org/wiki/向量空間)就是[赋范向量空间](https://zh.wikipedia.org/wiki/賦範向量空間)。同样，拥有半范数的[向量空间](https://zh.wikipedia.org/wiki/向量空間)就是赋半范向量空间。

### $L^p$范数

- [Lp範數  (wikipedia.org)](https://zh.wikipedia.org/wiki/Lp范数)

- 形式上，$L^p$ 范数定义如下$||x||_{p}$:

  - $$
    \large||x||_{p}=\left(\sum_{i}|x_i|^p\right)^{\frac{1}{p}}\label{Lp}
    $$

    - $x_i$是向量$x$的元素
    - 其中$p\in\mathbb{R},p\geqslant{1}$
    - $\frac{1}{p}\in(0,1]$

  - 范数是将向量映射到**非负值**(容易证明$L^p\geqslant{0}$)

    - 由幂函数的知识,在函数$f(x)=x^p(p>0)$是递增函数
    - $|x_i|\geqslant{0}$,则$0=0^{p}\leqslant{|x_i|^p}$
    - 所以$\sum_{i}|x_i|^p\geqslant{0}$
    - $||x||_p\geqslant{0}$

  - 补充:由指数函数知识,$g(x)=t^x(0<t<1)$是递减的,在$t>1$是递增的

    - 如果$|x_i|<1$,则$\sum_{i}|x_i|^p\leqslant{\sum_{i}|x_i|}$
    - 如果$|x_i|>1$,则:$\sum_{i}|x_i|^p\geqslant{\sum_{i}|x_i|}$

    

- 向量x的范数衡量从原点(零向量)到点x的距离

### $L^2$范数@欧几里得距离

- 当$p=2$时,$L^2$范数被称为**Euclidean norm**(欧几里得范数)

  - $||x||^2$破坏了范数规则,比如**次可加性**

  - 它表示从**原点**出发到向量$x$确定的点的**欧几里得距离**

- [欧几里得距离 (wikipedia.org)](https://zh.wikipedia.org/wiki/欧几里得距离)

  - 对于n维向量空间,原点$O=(0,0,\cdots,0)$到$x=(x_1,x_2,\cdots,x_n)$描述的点的欧式距离

  - $$
    \|{\vec  {x}}\|_{2}={\sqrt  {|x_{1}|^{2}+\cdots +|x_{n}|^{2}}}
    \\
    ||\mathbf{x}||_2=\sqrt{\sum_{i}^{n}(x_i)^2}
    $$
    


- 尽管如此,但是在述**误差矩阵**矩阵的时候,常常会以形如$D=A-B$;$D_{ij}=(A_{ij}-B_{ij})_{m\times{n}}$

- 那么D的向量$d^{(i)}$的范数就表示为

  - 符号说明:$\mathbf{x}^{(i)}$表示某个矩阵中的第i个向量(通常指列向量),在可以区分的情况下,下面可能不使用粗体来表示,直接作$x^{(i)}$

  - $x^{(i)}_j$表示第i个向量的第j个元素

  - $d^{(i)}=a^{(i)}-b^{(i)}$

  - $$
    ||{d^{(i)}}||_2=\sqrt{\sum_{j=1}^{n}(d_j^{(i)})^2}
    =\sqrt{\sum_{j=1}^{n}(a_j^{(i)}-b_j^{(i)})^2}
    $$

  - 公式逆用,可以将求和号化简甚至消掉(也可以起到减少求和指数的作用)

    - $$
      \sqrt{\sum_{i}(x_i-y_i)^2}=||x||_2
      \\
      \sum_{i}(x_i-y_i)^2=||x||_2^2
      $$

    - 例如

      - $$
        \sum_{i}\sum_{j}(x_{ij})^2
        =\sum_{i}||x_i||_2^2
        \\
        \sum_{i}\sum_{j}(x_{ij}-y_{ij})^2
        =\sum_{i}||x_i-y_i||_2^2
        $$

        

      

#### 更一般的欧式距离

- 更一般的,从点$p$到$q$的欧几里得距离:

  - $$
    d(\mathbf {p,q})= \sqrt{\sum \limits_{i=1}^n (q_i-p_i)^2}
    $$

    - $p,q$	=	two points in Euclidean n-space
      $q_i, p_i$	=	Euclidean vectors, starting from the origin of the space (initial point)
      $n$	=	n-space

  - 或描述为:
    $$
    {\displaystyle d(x,y)={\sqrt {(x_{1}-y_{1})^{2}+(x_{2}-y_{2})^{2}+\cdots +(x_{n}-y_{n})^{2}}}}
    $$



### 平方$L^2$范数

- $L^2$ 范数在机器学习中出现地十分频繁
  - 平方$L^2$范数$||x||_2^2$,经常简化表示为 $∥x∥$，略去了角标2
  - <u>平方 $L^2$ 范数</u>也经常用来衡量向量的大小，可以简单地通过**点积**  ($x^Tx$)计算。
  - 平方 $L^2$ 范数在数学和计算上都比 <u>$L^2$ 范数本身</u>更方便。
    - 例如，平方 $L^2$ 范数对$x$中每个元素的导数只取决于对应的元素，
      - 而 $L^2$ 范数对每个元素的<u>导数却和整个向量相关</u>。
    - 但是在很多情况下，平方 $L^2$ 范数也可能不受欢迎，因为它在原点附近增长得十分缓慢。


### $L^1$范数

- 在某些机器学习应用中，区分恰好是零的元素和非零但值很小的元素是很重要的。

  - 在这些情况下，我们转而使用在各个位置斜率相同，同时保持简单的数学形式的函数：L1 范数。

  - L1 范数可以简化如下：

    - $$
      ||x||_1=\sum_i|x_i|
      $$


### 最大范数

- $$
  ||x||_{\infin}=max(x_1,x_2,\cdots,x_n)
  \\
  或描述为:
  \\
  {\displaystyle \lVert {\vec {x}}\rVert _{\infty }=\lim _{p\to +\infty }{\Bigl (}\sum \limits _{i=1}^{n}|x_{i}|^{p}{\Bigr )}^{1/p}=\max _{i}|x_{i}|}
  $$

### 向量点积用范数表示

- $$
  x^Ty=||x||_2||y||_2\cos{\theta}
  $$

  - 其中$\theta$表示$x,y$之间的夹角
  - $x^Ty=(x,y)=\sum_{i=1}^{n}x_iy_i$
  - $\cos\theta=\frac{x\cdot y}{|x||y|}=\frac{(x,y)}{||x||_2||y||_2}$
    - $|x|=||x||_2$
    - $|y|=||y||_2$
  - $x^Ty=(x,y)=||x||_2||y||_2\cos{\theta}$

###  衡量矩阵大小Frobenius Norm

- 有时候我们可能也希望衡量矩阵的大小。

- 在深度学习中，最常见的做法是使用 Frobenius 范数（Frobenius norm）

  - $$
    ||A||_{F}=\sqrt{\sum\limits_{i,j}(A_{i,j})^2}
    \\
    =\sqrt{Tr(AA^T)}
    \\
    A=\begin{pmatrix}
    	a_{11}  &a_{12}  &\cdots  &a_{1n}  	\\
    	a_{21}  &a_{22}  &\cdots  &a_{2n}  	\\
    	\vdots  &\vdots  &        &\vdots  	\\
    	a_{m1}  &a_{m2}  &\cdots  &a_{mn}  	\\
    \end{pmatrix}
    \\
    B=A^T=\begin{pmatrix}
      	a_{11}  &a_{21}  &\cdots  &a_{m1}  	\\
      	a_{12}  &a_{22}  &\cdots  &a_{m2}  	\\
      	\vdots  &\vdots  &        &\vdots  	\\
      	a_{1n}  &a_{2n}  &\cdots  &a_{mn}  	\\
      \end{pmatrix}
    $$
  
    - 其中$A_{i,j}$是矩阵A的第i行第j列元素
    - $b_{ij}$表示A转置后的结果矩阵B的第i行第j列元素,并且对应于矩阵A的$a_{ji}$
      - $b_{ij}=a_{ji}$这个公式告诉我们,转置后的第i行第j列元素是原矩阵的第j行第i列元素
      - $i,j$的取值范围在转置前后没有发生变化
  
  - $$
    AA^T\in\mathbb{R}^{m\times{m}}
    \\
    C=AA^T
    \\C的任意一个位置的元素可以表示为:
    \\
    c_{ij}=\sum\limits_{i=k}^{n}a_{ik}b_{kj}
    =\sum\limits_{k=1}^{n}a_{ik}a_{jk}
    \\
    特别的,当i=j时,c_{ii}=\sum\limits_{k=1}^{n}a_{ik}a_{ik}
    =\sum\limits_{k=1}^{n}a_{ik}^2
    \quad i=1,2,\cdots,m
    \\
    \sum\limits_{i=1}^{m}c_{ii}=\sum\limits_{i=1}^{m}\sum_{k=1}^{n}a_{ik}^2
    =\sum_{ij}(a_{ij})^2
    $$
  
    - 因此,如果使用迹运算表示$Tr(AA^T)=\sum_{ij}(a_{ij})^2$
    - 那么Frobenius 范数就可以表示为$||A||_{F}=Tr(AA^T)$
    - 其类似于$L^2$范数
  
  - 事实上,矩阵乘法链中的矩阵调换位置后(如果仍然可以执行乘法运算),调换前后矩阵链乘的迹保持不变
  
    - $Tr(AA^T)=Tr(A^TA)$

### pytorch计算范数🎈

- Use [`torch.linalg.vector_norm()`](https://pytorch.org/docs/stable/generated/torch.linalg.vector_norm.html#torch.linalg.vector_norm) when computing **vector norms** and [`torch.linalg.matrix_norm()`](https://pytorch.org/docs/stable/generated/torch.linalg.matrix_norm.html#torch.linalg.matrix_norm) when computing **matrix norms**.

## 特殊类型矩阵和向量

### 对角阵

- [Diagonal matrix - Wikipedia](https://en.wikipedia.org/wiki/Diagonal_matrix)
- [Main diagonal - Wikipedia](https://en.wikipedia.org/wiki/Main_diagonal)

- 对角矩阵（diagonal matrix）只在**主对角线**上含有非零元素，其他位置都是零。

  - 形式上,设矩阵D满足,$D_{ij}=0$,if $i\neq{j}$,则D是对角阵

    - 啰嗦的讲:

      - $$
        D_{ij}=
        \begin{cases}
        0,&i\neq{j}\\
        x,&i =j
        \end{cases}
        \quad x可以是任何数
        $$

  - 矩阵主对角线上的元素是$D_{ij},i=j$的元素

    - 非方阵矩阵也有主对角线元素,并且主对角线长度取决于行数和列数种的较小者

    - $$
      {\displaystyle {\begin{bmatrix}\color {red}{1}&0&0\\0&\color {red}{1}&0\\0&0&\color {red}{1}\end{bmatrix}}\qquad {\begin{bmatrix}\color {red}{1}&0&0&0\\0&\color {red}{1}&0&0\\0&0&\color {red}{1}&0\end{bmatrix}}\qquad {\begin{bmatrix}\color {red}{1}&0&0\\0&\color {red}{1}&0\\0&0&\color {red}{1}\\0&0&0\end{bmatrix}}\qquad {\begin{bmatrix}\color {red}{1}&0&0&0\\0&\color {red}{1}&0&0\\0&0&\color {red}{1}&0\\0&0&0&\color {red}{1}\end{bmatrix}}\qquad }
      $$

      - 主对角线元素用红色表出
      - 且这些矩阵都是满足**对角阵**的定义(对于非方阵而言,对角阵的主对角线两侧0的分布不对称)
        - 非零元素的占比:设q=min(m,n),其中m,n分别是对角阵D的行数和列数
        - 则对角阵D种的非零元素比例不超过$\frac{q}{q^2}=\frac{1}{q}$
        - 这是计算机计算对角阵乘法快速的原因之一

- 我们用 $diag(v)$ 表示一个对角元素由向量 v 中元素给定的对角方阵。

- 对角矩阵受到关注的部分原因是对角矩阵的乘法计算很高效。

  - 计算乘法 $diag(v)x$，我们只需要将 x 中的每个元素 $x_i$ 放大 $v_i$ 倍。换言之，$diag(v)x = v ⊙ x$。
  - 计算<u>对角**方阵**</u>的逆矩阵也很高效。
    - 在很多情况下，我们可以根据任意矩阵导出一些通用的机器学习算法；
    - 但通过将一些矩阵限制为对角矩阵，我们可以得到计算代价较低的（并且简明扼要的）算法。

- 不是所有的对角矩阵都是方阵。🎈

  - 长方形的矩阵也有可能是对角矩阵。
  - 非方阵的对角矩阵没有逆矩阵，但我们仍然可以高效地计算它们的乘法。
  - 对于一个长方形对角矩阵 D 而言,和向量$x$的乘法 $Dx$ 会涉及到向量$x$ 中每个元素的缩放
    - 如果 D 是<u>瘦长型矩阵</u>，那么在缩放后的末尾添加一些零；
    - 如果 D 是<u>胖宽型</u>矩阵，那么在缩放后去掉最后一些元素。

  - $$
    \Sigma_1=
    \begin{pmatrix}
    	1 &0 &0 &0 \\
    	0 &2 &0 &0 \\
    	0 &0 &3 &0 \\
    \end{pmatrix}
    \\
    \Sigma_2=
    \begin{pmatrix}
    	1 &0 &0 \\
    	0 &2 &0 \\
    	0 &0 &3 \\
    	0 &0 &0 \\
    \end{pmatrix}
    \\\Sigma_1\Sigma_2=
    \begin{pmatrix}
    	1 &0 &0 \\
    	0 &2 &0 \\
    	0 &0 &3 \\
    \end{pmatrix}
    $$

  - 设$\Sigma\in\mathbb{R}^{m\times{n}}$,$\Sigma^T\in\mathbb{R}^{n\times{m}}$

    - $\Lambda_1=\Sigma\Sigma^T\in\mathbb{R}^{n\times{n}}$
    - $\Lambda_2=\Sigma^T\Sigma\in\mathbb{R}^{n\times{n}}$



