

## 线性组合与线性方程组

- 如果A是方阵,其逆矩阵 $A^{−1}$ 存在，那么式 $Ax=b$ 肯定对于每一个向量 $b$ 恰好存在一个解。

- 但是，对于一般的方程组而言(A不一定是方阵)，对于向量 b 的某些值，有可能**不存在解**，或者**存在无限多个解**两,或者存在**唯一解**。

  - 存在多于一个解(是少2个)但是少于无限多个解(解的数量有限而不是无穷大)的情况是<u>不可能发生的</u>；

  - 因为如果 x 和y 都是某方程组的解($Ax=b,Ay=b)$，则

    - $$
      z=\alpha{x}+\beta{y},其中\alpha+\beta=1
      $$

      - 对于任意$\alpha\in{R}$,$z$肯定也是$Ax=b$的解,因为:

      - $$
        Az=\alpha{A}x+\beta{A}y=\alpha{b}+\beta{b}=(\alpha+\beta)b=b
        $$

        

- 为了分析方程有多少个解，我们可以将 **A 的列向量**看作从 **原点**（origin）（元素都是零的向量,对于n维向量,可以理解为n维点,例如三维空间原点(0,0,0))出发的**不同方向**(用$A$的一个**列向量**来对应表示一个**方向**)，确定有多少种方法可以到达向量 $b$。

  - 设$A\in\mathbb{R}^{m\times{n}}$,则$x\in\mathbb{R}^{n}$,也即是说A可以看成由n个列向量构成的矩阵(用$\alpha_i$表示第i个方向)

    - $A=(\alpha_1,\alpha_2,\cdots,\alpha_n)$

    - $$
      x=\begin{pmatrix}
      	x_{1}	\\
      	x_{2}	\\
      	\vdots		\\
      	x_{n}	\\
      \end{pmatrix}
      $$

      

  - 解向量 $x$ 中的每个元素$x_i$表示应该沿着方向$\alpha_i$走多的距离为 $x_i$

  - 将这些步骤效果叠加:

    - $$
      Ax=\sum\limits_{i=1}^{n}\alpha_{i}x_i=b
      $$

    - 这种操作称为向量组的**线性组合**(向量$b$用矩阵A的列向量组线性表出,表出系数为向量$x$)
    
      - 其中$\alpha_i$是向量,$x_i$是标量
    
    - 而$Ax=b$的线性方程组展开
    
      - 是从矩阵乘积的结果$b$(或解向量$x$)的逐个分量的角度描述.
    
      - $$
        Ax=\sum\limits_{i=1}^{m}\beta_{i}x=b\\
        \left
            \{\begin{aligned}{}
            a_{11} x_{1}+a_{12} x_{2}+\cdots+a_{1 n} x_{n}&=b_{1}, \\
            a_{21} x_{1}+a_{22} x_{2}+\cdots+a_{2 n} x_{n}&=b_{2}, \\
        	\vdots&\\
            a_{m1} x_{1}+a_{m 2} x_{2}+\cdots+a_{m n} x_{n}&=b_{n}
            \end{aligned}
        \right.
        $$
    
        - 其中$\beta_i$是矩阵A的第i个行向量(分块),$x$是解向量

## 生成子空间

- 一组向量的 **生成子空间**（span）是<u>原始向量**线性组合**</u>后所能抵达的结果的**集合**。
- 确定 $Ax = b$ 是否有解相当于确定向量 b 是否在 A 列向量的**生成子空间**中。
  - $A\in\mathbb{R}^{m\times{n}}$
  - $x\in\mathbb{R}^{n\times{1}}$
  - $b\in\R^{m\times{1}}$
- A向量组的<u>生成子空间</u>被称为 A 的 **列空间**（column space）或者 A 的 **值域**（range）。

- 为了使方程 $Ax = b$ 对于任意向量 $b\in\mathbb{R}^m$ 都存在解，我们要求 A 的列空间构成整个 $\mathbb{R}^m$。
  - 这意味者b一定会落在A的列空间
- 如果 $\mathbb{R}^m$ 中的某个点不在 A 的列空间中(向量b无法被矩阵A线性表出)，那么该点对应的 b 会使得该方程没有解。
- 矩阵 A 的列空间是整个 $\mathbb{R}^m$ 的要求，意味着 A 至少有 m 列，即
  - $n\geqslant m$。否则，A 列空间的维数会小于 m。
    - 例如，假设 A 是一个 3 × 2 的矩阵。目标 b 是 3 维的，但是 x 只有 2 维。
    - 所以无论如何修改二维向量 $x$ 的值，也只能描绘出 $\mathbb{R}^3$ 空间中的二维平面,当且仅当向量 b 在该二维平面中时，该方程有解。
  - $n\geqslant{m}$仅是方程对每一点都有解的必要条件。这不是一个充分条件，因为有些列向量可能是冗余的。
    - 假设有一个 $\mathbb{R}^{2\times{2}}$ 中的矩阵，它的两个列向量是相同的。
    - 那么它的列空间和它的一个列向量作为矩阵时的列空间是一样的。
    - 换言之，虽然该矩阵有 2 列，但是它的<u>列空间仍然只是一条线</u>(只能描述某个方向)，不能涵盖整个 $\mathbb{R}^2$ 空间。

## 范数

- 有时我们需要衡量一个**向量的大小**。
  - 在机器学习中，我们经常使用被称为 **范数**（norm）的**函数**衡量向量大小。

- 严格地说,**范数**可以是满足以下性质的任意函数:
- 半范数:
  - $f(x)\geqslant{0}$
    - 半正定性
  - $f(x+y)\leqslant{f(x)+f(y)}$,(次可加性)
    - 即三角不等式,
    - 例如函数$f(x)=|x|$就满足$|x+y|\leqslant{|x|+|y|}$
  - $\forall{a}\in\mathbb{R},f(ax)=|a|f(x)$
    - 具有绝对一次齐次性
- **范数**是一个**半范数**加上额外性质：
  - $f(x)=0\Rightarrow{x=0}$(正定性)

### $L^p$范数

- [Lp範數  (wikipedia.org)](https://zh.wikipedia.org/wiki/Lp范数)

- 形式上，$L^p$ 范数定义如下$||x||_{p}$:

  - $$
    \large||x||_{p}=\left(\sum_{i}|x_i|^p\right)^{\frac{1}{p}}
    $$

    - $x_i$是向量$x$的元素
    - 其中$p\in\mathbb{R},p\geqslant{1}$
    - $\frac{1}{p}\in(0,1]$

  - 范数是将向量映射到**非负值**(容易证明$L^p\geqslant{0}$)

    - 由幂函数的知识,在函数$f(x)=x^p(p>0)$是递增函数
    - $|x_i|\geqslant{0}$,则$0=0^{p}\leqslant{|x_i|^p}$
    - 所以$\sum_{i}|x_i|^p\geqslant{0}$
    - $||x||_p\geqslant{0}$

  - 补充:由指数函数知识,$g(x)=t^x(0<t<1)$是递减的,在$t>1$是递增的

    - 如果$|x_i|<1$,则$\sum_{i}|x_i|^p\leqslant{\sum_{i}|x_i|}$
    - 如果$|x_i|>1$,则:$\sum_{i}|x_i|^p\geqslant{\sum_{i}|x_i|}$

    

- 向量x的范数衡量从原点(零向量)到点x的距离

- 当$p=2$时,$L^2$范数被称为**Euclidean norm**(欧几里得范数)

  - $||x||^2$破坏了范数规则,比如**次可加性**

  - 它表示从**原点**出发到向量$x$确定的点的**欧几里得距离**

    - [欧几里得距离 (wikipedia.org)](https://zh.wikipedia.org/wiki/欧几里得距离)

      - 对于n维向量空间,原点$O=(0,0,\cdots,0)$到$x=(x_1,x_2,\cdots,x_n)$描述的点的欧式距离

      - $$
        \|{\vec  {x}}\|_{2}={\sqrt  {|x_{1}|^{2}+\cdots +|x_{n}|^{2}}}
        $$

      - 更一般的,从点$p$到$q$的欧几里得距离:

        - $$
          d(\mathbf {p,q})= \sqrt{\sum \limits_{i=1}^n (q_i-p_i)^2}
          $$

          - $p,q$	=	two points in Euclidean n-space
            $q_i, p_i$	=	Euclidean vectors, starting from the origin of the space (initial point)
            $n$	=	n-space

        - 或描述为:
          $$
          {\displaystyle d(x,y)={\sqrt {(x_{1}-y_{1})^{2}+(x_{2}-y_{2})^{2}+\cdots +(x_{n}-y_{n})^{2}}}}
          $$

- $L^2$ 范数在机器学习中出现地十分频繁

  - 平方$L^2$范数$||x||_2^2$,经常简化表示为 $∥x∥$，略去了角标2
  - <u>平方 $L^2$ 范数</u>也经常用来衡量向量的大小，可以简单地通过**点积**  ($x^Tx$)计算。
  - 平方 $L^2$ 范数在数学和计算上都比 <u>$L^2$ 范数本身</u>更方便。
    - 例如，平方 $L^2$ 范数对$x$中每个元素的导数只取决于对应的元素，
      - 而 $L^2$ 范数对每个元素的<u>导数却和整个向量相关</u>。
    - 但是在很多情况下，平方 $L^2$ 范数也可能不受欢迎，因为它在原点附近增长得十分缓慢。

- 在某些机器学习应用中，区分恰好是零的元素和非零但值很小的元素是很重要的。

  - 在这些情况下，我们转而使用在各个位置斜率相同，同时保持简单的数学形式的函数：L1 范数。

  - L1 范数可以简化如下：

    - $$
      ||x||_1=\sum_i|x_i|
      $$

- 最大范数

  - $$
    ||x||_{\infin}=max(x_1,x_2,\cdots,x_n)
    \\
    或描述为:
    \\
    {\displaystyle \lVert {\vec {x}}\rVert _{\infty }=\lim _{p\to +\infty }{\Bigl (}\sum \limits _{i=1}^{n}|x_{i}|^{p}{\Bigr )}^{1/p}=\max _{i}|x_{i}|}
    $$

### 向量点积用范数表示

- $$
  x^Ty=||x||_2||y||_2\cos{\theta}
  $$

  - 其中$\theta$表示$x,y$之间的夹角

### ref

- [范数 (wikipedia.org)](https://zh.wikipedia.org/zh-cn/范数)

- **范数**（英语：Norm），是具有“长度”概念的函数。

  - 在[线性代数](https://zh.wikipedia.org/wiki/線性代數)、[泛函分析](https://zh.wikipedia.org/wiki/泛函分析)及相关的数学领域，是一个[函数](https://zh.wikipedia.org/wiki/函數)，其为[向量空间](https://zh.wikipedia.org/wiki/向量空間)内的所有[向量](https://zh.wikipedia.org/wiki/向量)赋予非零的正**长度**或**大小**。

- 另一方面，**半范数**（英语：seminorm）可以为非零的[向量](https://zh.wikipedia.org/wiki/向量)赋予零长度。

- 例，一个二维度的欧氏几何空间$\mathbb {R} ^{2}$就有欧氏范数。在这个[向量空间](https://zh.wikipedia.org/wiki/向量空間)的元素（譬如：(3,7)）常常在[笛卡尔坐标系统](https://zh.wikipedia.org/wiki/直角坐标系)被画成一个从原点出发的箭号。每一个[向量](https://zh.wikipedia.org/wiki/向量)的欧氏范数就是箭号的长度。

  拥有范数的[向量空间](https://zh.wikipedia.org/wiki/向量空間)就是[赋范向量空间](https://zh.wikipedia.org/wiki/賦範向量空間)。同样，拥有半范数的[向量空间](https://zh.wikipedia.org/wiki/向量空間)就是赋半范向量空间。

##  衡量矩阵大小

- 有时候我们可能也希望衡量矩阵的大小。在深度学习中，最常见的做法是使用 Frobenius 范数（Frobenius norm）

  - $$
    ||A||_{F}=\sqrt{\sum\limits_{i,j}A^2_{i,j}}
    $$

    - 其中$A_{i,j}$是矩阵A的第i行第j列元素

  - 其类似于$L^2$范数

## 特殊类型矩阵和向量

### 对角阵

- [Diagonal matrix - Wikipedia](https://en.wikipedia.org/wiki/Diagonal_matrix)
- [Main diagonal - Wikipedia](https://en.wikipedia.org/wiki/Main_diagonal)

- 对角矩阵（diagonal matrix）只在**主对角线**上含有非零元素，其他位置都是零。

  - 形式上,设矩阵D满足,$D_{ij}=0$,if $i\neq{j}$,则D是对角阵

    - 啰嗦的讲:

      - $$
        D_{ij}=
        \begin{cases}
        0,&i\neq{j}\\
        x,&i =j
        \end{cases}
        \quad x可以是任何数
        $$

  - 矩阵主对角线上的元素是$D_{ij},i=j$的元素

    - 非方阵矩阵也有主对角线元素,并且主对角线长度取决于行数和列数种的较小者

    - $$
      {\displaystyle {\begin{bmatrix}\color {red}{1}&0&0\\0&\color {red}{1}&0\\0&0&\color {red}{1}\end{bmatrix}}\qquad {\begin{bmatrix}\color {red}{1}&0&0&0\\0&\color {red}{1}&0&0\\0&0&\color {red}{1}&0\end{bmatrix}}\qquad {\begin{bmatrix}\color {red}{1}&0&0\\0&\color {red}{1}&0\\0&0&\color {red}{1}\\0&0&0\end{bmatrix}}\qquad {\begin{bmatrix}\color {red}{1}&0&0&0\\0&\color {red}{1}&0&0\\0&0&\color {red}{1}&0\\0&0&0&\color {red}{1}\end{bmatrix}}\qquad }
      $$

      - 主对角线元素用红色表出
      - 且这些矩阵都是满足**对角阵**的定义(对于非方阵而言,对角阵的主对角线两侧0的分布不对称)
        - 非零元素的占比:设q=min(m,n),其中m,n分别是对角阵D的行数和列数
        - 则对角阵D种的非零元素比例不超过$\frac{q}{q^2}=\frac{1}{q}$
        - 这是计算机计算对角阵乘法快速的原因之一

- 我们用 $diag(v)$ 表示一个对角元素由向量 v 中元素给定的对角方阵。

- 对角矩阵受到关注的部分原因是对角矩阵的乘法计算很高效。

  - 计算乘法 $diag(v)x$，我们只需要将 x 中的每个元素 $x_i$ 放大 $v_i$ 倍。换言之，$diag(v)x = v ⊙ x$。
  - 计算<u>对角**方阵**</u>的逆矩阵也很高效。
    - 在很多情况下，我们可以根据任意矩阵导出一些通用的机器学习算法；
    - 但通过将一些矩阵限制为对角矩阵，我们可以得到计算代价较低的（并且简明扼要的）算法。

- 不是所有的对角矩阵都是方阵。🎈

  - 长方形的矩阵也有可能是对角矩阵。
  - 非方阵的对角矩阵没有逆矩阵，但我们仍然可以高效地计算它们的乘法。
  - 对于一个长方形对角矩阵 D 而言,和向量$x$的乘法 $Dx$ 会涉及到向量$x$ 中每个元素的缩放
    - 如果 D 是<u>瘦长型矩阵</u>，那么在缩放后的末尾添加一些零；
    - 如果 D 是<u>胖宽型</u>矩阵，那么在缩放后去掉最后一些元素。

### 单位向量

- 单位向量（unit vector）是具有单位范数（unit norm）的向量：

  - $$
    ∥x∥_2 = 1
    $$

- 如果 $x^Ty = 0$，那么向量 x 和向量 y 互相 **正交**（orthogonal）

- 如果两个向量都有**非零范数**，那么这两个<u>向量之间的夹角是 90 度</u>。

- 在 $\mathbb{R}^n$ 中，至多有 n 个范数非零向量互相正交。

  - 如果这些向量不仅互相正交，并且<u>范数都为 1</u>，那么我们称它们是 **标准正交**（orthonormal）。

- 非单位向量可以通过正规化得到同方向的单位向量









