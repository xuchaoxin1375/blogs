[toc]

# ML@sklearn@MLæµç¨‹Part2.1@è½½å…¥æ•°æ®é›†@æ¨¡å‹è¯„ä¼°

## model_selectionğŸˆğŸ˜

- [model_selection](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection)

  - **User guide:** See the [Cross-validation: evaluating estimator performance](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation), [Tuning the hyper-parameters of an estimator](https://scikit-learn.org/stable/modules/grid_search.html#grid-search) and [Learning curve](https://scikit-learn.org/stable/modules/learning_curve.html#learning-curve) sections for further details.

- Scikit-learnåº“ä¸­çš„`sklearn.model_selection`æ¨¡å—æä¾›äº†ä¸€ç³»åˆ—ç”¨äº**æ¨¡å‹é€‰æ‹©å’Œè¯„ä¼°**çš„å·¥å…·ã€‚è¯¥æ¨¡å—ä¸­åŒ…å«çš„ç±»å’Œå‡½æ•°å¯ä»¥å¸®åŠ©æˆ‘ä»¬è¿›è¡Œ**æ•°æ®é›†çš„åˆ’åˆ†ã€äº¤å‰éªŒè¯ã€è¶…å‚æ•°ä¼˜åŒ–ç­‰æ“ä½œ**ï¼Œä»è€Œé€‰æ‹©æœ€ä¼˜çš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚

- è¯¥æ¨¡å—ä¸­ä¸€äº›å¸¸ç”¨çš„ç±»å’Œå‡½æ•°å¦‚ä¸‹ï¼š

  - `train_test_split`: ç”¨äºå°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚	
  - `KFold`: ç”¨äºè¿›è¡ŒKæŠ˜äº¤å‰éªŒè¯ã€‚
  - `StratifiedKFold`: ç”¨äºè¿›è¡Œåˆ†å±‚KæŠ˜äº¤å‰éªŒè¯ï¼Œå¯ä»¥å¤„ç†ä¸å¹³è¡¡æ•°æ®é›†ã€‚
  - `GridSearchCV`: ç”¨äºè¿›è¡Œç½‘æ ¼æœç´¢å’Œäº¤å‰éªŒè¯ï¼Œå¯»æ‰¾æœ€ä¼˜çš„è¶…å‚æ•°ç»„åˆã€‚
  - `RandomizedSearchCV`: ç”¨äºè¿›è¡Œéšæœºæœç´¢å’Œäº¤å‰éªŒè¯ï¼Œå¯»æ‰¾æœ€ä¼˜çš„è¶…å‚æ•°ç»„åˆã€‚
  - `cross_val_score`: ç”¨äºå¯¹æ¨¡å‹è¿›è¡Œäº¤å‰éªŒè¯å¹¶è®¡ç®—æ€§èƒ½æŒ‡æ ‡çš„å¹³å‡å€¼ã€‚ğŸˆ
  - `learning_curve`: ç”¨äºç»˜åˆ¶å­¦ä¹ æ›²çº¿ï¼Œå¸®åŠ©æˆ‘ä»¬åˆ¤æ–­æ¨¡å‹æ˜¯å¦æ¬ æ‹Ÿåˆæˆ–è¿‡æ‹Ÿåˆã€‚
  - `validation_curve`: ç”¨äºç»˜åˆ¶éªŒè¯æ›²çº¿ï¼Œå¸®åŠ©æˆ‘ä»¬é€‰æ‹©æœ€ä¼˜çš„è¶…å‚æ•°ã€‚

  è¿™äº›ç±»å’Œå‡½æ•°å¯ä»¥æ ¹æ®éœ€è¦è¿›è¡Œç»„åˆä½¿ç”¨ï¼Œä»¥å®Œæˆä¸åŒçš„æ¨¡å‹é€‰æ‹©å’Œè¯„ä¼°ä»»åŠ¡ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`train_test_split`å°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œç„¶åä½¿ç”¨`GridSearchCV`å¯»æ‰¾æœ€ä¼˜çš„è¶…å‚æ•°ç»„åˆï¼Œæœ€åä½¿ç”¨`cross_val_score`å¯¹æ¨¡å‹è¿›è¡Œäº¤å‰éªŒè¯å¹¶è®¡ç®—æ€§èƒ½æŒ‡æ ‡çš„å¹³å‡å€¼ã€‚

### train_test_split

- [sklearn.model_selection.train_test_split â€” scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)

- Split arrays or matrices into random train and test subsets.

  Quick utility that wraps input validation, `next(ShuffleSplit().split(X, y))`, and application to input data into a single call for splitting (and optionally subsampling) data into a one-liner.

  å°†æ•°ç»„æˆ–çŸ©é˜µéšæœºåˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„å¿«é€Ÿå®ç”¨ç¨‹åºã€‚

  è¿™æ˜¯ä¸€ä¸ªå¿«æ·å®ç”¨ç¨‹åºï¼Œå®ƒå°†è¾“å…¥éªŒè¯ã€next(ShuffleSplit().split(X, y))å’Œå°†å…¶åº”ç”¨äºè¾“å…¥æ•°æ®çš„æ“ä½œå°è£…æˆä¸€ä¸ªå•ç‹¬çš„å‡½æ•°è°ƒç”¨ï¼Œç”¨äºå°†æ•°æ®æ‹†åˆ†ï¼ˆå’Œå¯é€‰åœ°è¿›è¡Œå­é‡‡æ ·ï¼‰ä¸ºä¸€è¡Œä»£ç ã€‚

### train_test_split vs KFold

- `train_test_split`å’Œ`KFold`éƒ½æ˜¯ç”¨äºæœºå™¨å­¦ä¹ ä¸­çš„æ•°æ®æ‹†åˆ†å’Œäº¤å‰éªŒè¯çš„å·¥å…·ï¼Œä½†å®ƒä»¬çš„ä½¿ç”¨åœºæ™¯ç•¥æœ‰ä¸åŒã€‚

- `train_test_split`æ˜¯ç”¨äºå°†æ•°æ®é›†æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„å·¥å…·ã€‚å®ƒå¯ä»¥å°†æ•°æ®é›†éšæœºåœ°æ‹†åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼Œä¸€éƒ¨åˆ†ç”¨äºè®­ç»ƒæ¨¡å‹ï¼Œå¦ä¸€éƒ¨åˆ†ç”¨äºæµ‹è¯•æ¨¡å‹ã€‚`train_test_split`çš„ä¸€ä¸ªä¸»è¦ä¼˜ç‚¹æ˜¯å®ƒéå¸¸å®¹æ˜“ä½¿ç”¨ï¼Œåªéœ€è¦ä¸€è¡Œä»£ç å°±å¯ä»¥å®Œæˆæ•°æ®æ‹†åˆ†ã€‚ä¾‹å¦‚ï¼š

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

- åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œ`train_test_split`å°†æ•°æ®é›†Xå’Œæ ‡ç­¾yæ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå…¶ä¸­æµ‹è¯•é›†å æ€»æ•°æ®é›†çš„20%ã€‚`random_state`å‚æ•°ç”¨äºè®¾ç½®éšæœºæ•°ç§å­ï¼Œä»¥ç¡®ä¿åœ¨å¤šæ¬¡è¿è¡Œä»£ç æ—¶ï¼Œå¾—åˆ°çš„æ‹†åˆ†ç»“æœç›¸åŒã€‚

- `KFold`æ˜¯ä¸€ç§äº¤å‰éªŒè¯æ–¹æ³•ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚å®ƒå°†æ•°æ®é›†æ‹†åˆ†ä¸ºkä¸ªæŠ˜å ï¼ˆfoldï¼‰ï¼Œæ¯ä¸ªæŠ˜å éƒ½ç”¨äºè®­ç»ƒæ¨¡å‹å’Œè¯„ä¼°æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒå°†æ•°æ®é›†åˆ†æˆkä¸ªç­‰å¤§å°çš„å­é›†ï¼Œæ¯ä¸ªå­é›†éƒ½è¢«ç”¨ä½œæµ‹è¯•é›†ä¸€æ¬¡ï¼Œå‰©ä½™çš„k-1ä¸ªå­é›†è¢«ç”¨ä½œè®­ç»ƒé›†ã€‚è¿™ä¸ªè¿‡ç¨‹è¢«é‡å¤kæ¬¡ï¼Œæ¯æ¬¡ä½¿ç”¨ä¸åŒçš„æµ‹è¯•é›†ã€‚æœ€ç»ˆï¼Œæ‰€æœ‰çš„æµ‹è¯•ç»“æœè¢«å¹³å‡å¾—åˆ°ä¸€ä¸ªæ€§èƒ½æŒ‡æ ‡ã€‚
- `KFold`çš„ä¸€ä¸ªä¸»è¦ä¼˜ç‚¹æ˜¯å®ƒå¯ä»¥ä½¿ç”¨æ‰€æœ‰çš„æ•°æ®æ¥è®­ç»ƒå’Œæµ‹è¯•æ¨¡å‹ï¼Œè¿™æ ·å¯ä»¥æ›´å¥½åœ°è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼š

```python
from sklearn.model_selection import KFold
from sklearn.linear_model import LinearRegression

kf = KFold(n_splits=5, shuffle=True, random_state=42)

for train_index, test_index in kf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    model = LinearRegression()
    model.fit(X_train, y_train)
    score = model.score(X_test, y_test)
    
    print("Score:", score)
```

- åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œ`KFold`å°†æ•°æ®é›†Xå’Œæ ‡ç­¾yæ‹†åˆ†ä¸º5ä¸ªæŠ˜å ï¼Œæ¯ä¸ªæŠ˜å éƒ½ç”¨äºè®­ç»ƒæ¨¡å‹å’Œè¯„ä¼°æ¨¡å‹ã€‚åœ¨æ¯ä¸ªå¾ªç¯ä¸­ï¼Œæˆ‘ä»¬è·å–è®­ç»ƒç´¢å¼•å’Œæµ‹è¯•ç´¢å¼•ï¼Œç„¶åä½¿ç”¨å®ƒä»¬æ¥æ‹†åˆ†æ•°æ®é›†ã€‚æ¥ç€ï¼Œæˆ‘ä»¬è®­ç»ƒçº¿æ€§å›å½’æ¨¡å‹ï¼Œå¹¶åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬æ‰“å°å‡ºæ¯ä¸ªæŠ˜å çš„å¾—åˆ†ã€‚
- éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œ`KFold`éœ€è¦æ‰‹åŠ¨ç¼–å†™å¾ªç¯æ¥è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ï¼Œè¿™æ ·éœ€è¦æ›´å¤šçš„ä»£ç å’Œæ—¶é—´ã€‚ä½†å®ƒå¯ä»¥æ›´å¥½åœ°è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚

### cross_validateğŸˆ

- [sklearn.model_selection.cross_validate â€” scikit-learn 1.2.2 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html)

  - Evaluate metric(s) by cross-validation and also record fit/score times.é€šè¿‡äº¤å‰éªŒè¯è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶è®°å½•æ‹Ÿåˆ/å¾—åˆ†æ—¶é—´ã€‚

- `cross_validate`æ˜¯Scikit-learnåº“ä¸­çš„ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºå¯¹ç»™å®šçš„æ¨¡å‹è¿›è¡Œäº¤å‰éªŒè¯ï¼Œå¹¶è¿”å›äº¤å‰éªŒè¯çš„ç»“æœã€‚

  å…·ä½“æ¥è¯´ï¼Œ`cross_validate`å‡½æ•°çš„åŠŸèƒ½å¦‚ä¸‹ï¼š

  1. å¯¹ç»™å®šçš„æ¨¡å‹è¿›è¡Œäº¤å‰éªŒè¯ï¼Œä»¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚
  2. æ ¹æ®æ•°æ®é›†çš„åˆ’åˆ†æ–¹å¼ï¼Œå°†æ•°æ®é›†åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼Œå¤šæ¬¡è¿›è¡Œè®­ç»ƒå’ŒéªŒè¯ï¼Œå¹¶è®¡ç®—æ¨¡å‹çš„æ€§èƒ½æŒ‡æ ‡ï¼Œå¦‚ç²¾åº¦ã€F1åˆ†æ•°ã€Rå¹³æ–¹ç­‰ã€‚
  3. è¿”å›äº¤å‰éªŒè¯çš„ç»“æœï¼ŒåŒ…æ‹¬è®­<u>ç»ƒé›†å¾—åˆ†ã€éªŒè¯é›†å¾—åˆ†ã€æ‹Ÿåˆæ—¶é—´ã€é¢„æµ‹æ—¶é—´</u>ç­‰ã€‚

- `cross_validate`å‡½æ•°çš„å‚æ•°åŒ…æ‹¬ï¼š

  - `estimator`ï¼šè¡¨ç¤ºè¦è¿›è¡Œäº¤å‰éªŒè¯çš„æ¨¡å‹ã€‚
  - `X`ï¼šè¡¨ç¤ºæ•°æ®é›†çš„è‡ªå˜é‡ã€‚
  - `y`ï¼šè¡¨ç¤ºæ•°æ®é›†çš„å› å˜é‡ã€‚
  - `cv`ï¼šè¡¨ç¤ºäº¤å‰éªŒè¯çš„åˆ’åˆ†æ–¹å¼ã€‚
    - åœ¨Scikit-learnåº“ä¸­ï¼Œ`cv`å‚æ•°ç”¨äºæŒ‡å®šäº¤å‰éªŒè¯çš„ç”Ÿæˆå™¨æˆ–æ•´æ•°å€¼ï¼Œä»¥æ§åˆ¶æ•°æ®é›†çš„åˆ’åˆ†æ–¹å¼ã€‚`cv`å‚æ•°çš„ç¼©å†™é€šå¸¸æœ‰ä»¥ä¸‹å‡ ç§ï¼š
      - `cv`ï¼šè¡¨ç¤ºCross-validationï¼Œå³äº¤å‰éªŒè¯ï¼Œæ˜¯é»˜è®¤çš„ç¼©å†™ã€‚
      - `k`ï¼šè¡¨ç¤ºK-foldï¼Œå³KæŠ˜äº¤å‰éªŒè¯ï¼Œå…¶ä¸­Kè¡¨ç¤ºæ•°æ®é›†è¢«åˆ†æˆçš„æŠ˜æ•°ã€‚
      - `loo`ï¼šè¡¨ç¤ºLeave-One-Outï¼Œå³ç•™ä¸€æ³•äº¤å‰éªŒè¯ï¼Œå…¶ä¸­æ¯ä¸ªæ ·æœ¬éƒ½è¢«ç”¨ä½œéªŒè¯é›†ï¼Œå…¶ä½™æ ·æœ¬ç”¨ä½œè®­ç»ƒé›†ã€‚
      - `shuffle`ï¼šè¡¨ç¤ºShuffle-splitï¼Œå³éšæœºåˆ’åˆ†äº¤å‰éªŒè¯ï¼Œå…¶ä¸­æ¯æ¬¡åˆ’åˆ†éƒ½æ˜¯éšæœºçš„ï¼Œå¹¶æŒ‡å®šäº†åˆ’åˆ†çš„æ¬¡æ•°å’Œæµ‹è¯•é›†çš„å¤§å°ã€‚
  - `scoring`ï¼šè¡¨ç¤ºè¯„ä¼°æŒ‡æ ‡ã€‚
  - `return_train_score`ï¼šè¡¨ç¤ºæ˜¯å¦è¿”å›è®­ç»ƒé›†å¾—åˆ†ã€‚
  - `n_jobs`ï¼šè¡¨ç¤ºå¹¶è¡Œè¿ç®—çš„æ•°é‡ã€‚

- è¿”å›å€¼

  - `cross_validate()`å®ƒè¿”å›ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«äº†å¤šä¸ªè¯„ä¼°æŒ‡æ ‡çš„å€¼ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›é‡è¦çš„è¾“å‡ºç»“æœï¼š

    1. `fit_time`ï¼šè¡¨ç¤ºæ¯ä¸ªäº¤å‰éªŒè¯çš„è®­ç»ƒæ—¶é—´ï¼Œå³åœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆæ¨¡å‹æ‰€éœ€çš„æ—¶é—´ã€‚
    2. `score_time`ï¼šè¡¨ç¤ºæ¯ä¸ªäº¤å‰éªŒè¯çš„è¯„ä¼°æ—¶é—´ï¼Œå³åœ¨éªŒè¯é›†ä¸Šå¯¹æ¨¡å‹è¿›è¡Œè¯„ä¼°æ‰€éœ€çš„æ—¶é—´ã€‚
    3. `test_score`ï¼šè¡¨ç¤ºæ¯ä¸ªäº¤å‰éªŒè¯çš„æµ‹è¯•å¾—åˆ†ï¼Œå³åœ¨éªŒè¯é›†ä¸Šçš„æ¨¡å‹æ€§èƒ½è¯„ä¼°å¾—åˆ†ã€‚
       1. å¯¹äºåˆ†ç±»é—®é¢˜ï¼Œé€šå¸¸ä½¿ç”¨å‡†ç¡®ç‡ï¼ˆaccuracyï¼‰æ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼›
       2. å¯¹äºå›å½’é—®é¢˜ï¼Œé€šå¸¸ä½¿ç”¨å‡æ–¹è¯¯å·®ï¼ˆmean squared errorï¼‰æˆ–Rå¹³æ–¹ï¼ˆR-squaredï¼‰æ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚
    4. `train_score`ï¼šè¡¨ç¤ºæ¯ä¸ªäº¤å‰éªŒè¯çš„è®­ç»ƒå¾—åˆ†ï¼Œå³åœ¨è®­ç»ƒé›†ä¸Šçš„æ¨¡å‹æ€§èƒ½è¯„ä¼°å¾—åˆ†ã€‚

  - åœ¨æ¨¡å‹æ²¡æœ‰è¿‡æ‹Ÿåˆçš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šçš„è¡¨ç°åº”è¯¥ä¸åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°ç›¸ä¼¼ã€‚å› ä¸ºæ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šå­¦åˆ°çš„çŸ¥è¯†åº”è¯¥é€‚ç”¨äºæµ‹è¯•é›†ï¼Œå¦‚æœè®­ç»ƒå¾—åˆ†æ¯”æµ‹è¯•å¾—åˆ†é«˜å¾ˆå¤šï¼Œåˆ™è¡¨ç¤ºæ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šè¿‡åº¦æ‹Ÿåˆï¼Œä¸èƒ½å¾ˆå¥½åœ°æ³›åŒ–åˆ°æ–°çš„æ•°æ®ã€‚

    é™¤äº†ä¸Šè¿°æŒ‡æ ‡å¤–ï¼Œ`cross_validate`è¿˜å¯ä»¥è¿”å›å…¶ä»–æŒ‡æ ‡ï¼Œå¦‚å¹³å‡è®­ç»ƒå¾—åˆ†ã€å¹³å‡æµ‹è¯•å¾—åˆ†ã€æµ‹è¯•å¾—åˆ†çš„æ ‡å‡†å·®ç­‰ã€‚è¿™äº›æŒ‡æ ‡å¯ä»¥å¸®åŠ©æˆ‘ä»¬æ›´å…¨é¢åœ°è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½å’Œç¨³å®šæ€§ã€‚

    éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œ`cross_validate`çš„è¾“å‡ºç»“æœæ˜¯é’ˆå¯¹æ¯ä¸ªäº¤å‰éªŒè¯çš„ç»“æœï¼Œå› æ­¤éœ€è¦å¯¹ç»“æœè¿›è¡Œå¹³å‡æˆ–æ±‡æ€»ä»¥å¾—åˆ°æœ€ç»ˆçš„æ¨¡å‹æ€§èƒ½è¯„ä¼°ã€‚

#### eg

ä¾‹å¦‚ï¼Œä¸‹é¢çš„ä»£ç æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨`cross_validate`å‡½æ•°å¯¹çº¿æ€§å›å½’æ¨¡å‹è¿›è¡Œäº¤å‰éªŒè¯ï¼Œå¹¶è¿”å›äº¤å‰éªŒè¯çš„ç»“æœï¼š

- ```python
  from sklearn.datasets import make_regression
  from sklearn.linear_model import LinearRegression
  from sklearn.model_selection import cross_validate
  
  X, y = make_regression(n_samples=1000, random_state=0)
  lr = LinearRegression()
  
  result = cross_validate(lr, X, y, cv=5, return_train_score=True)
  print(result)
  ```

- ```bash
  ('fit_time', array([0.01501846, 0.00799823, 0.00999498, 0.01046562, 0.00799966]))
  ('score_time', array([0.00098014, 0.00175047, 0.00099993, 0.00201273, 0.00199795]))
  ('test_score', array([1., 1., 1., 1., 1.]))
  ('train_score', array([1., 1., 1., 1., 1.]))
  ```

  

- è¾“å‡ºçš„ç»“æœåŒ…æ‹¬è®­ç»ƒé›†å¾—åˆ†ã€éªŒè¯é›†å¾—åˆ†ã€æ‹Ÿåˆæ—¶é—´ã€é¢„æµ‹æ—¶é—´ç­‰ä¿¡æ¯ï¼Œå¯ä»¥ç”¨äºè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½å’Œè¿›è¡Œæ¨¡å‹çš„é€‰æ‹©å’Œè°ƒä¼˜ã€‚

### å…³äºæ•°æ®éšæœºåŒ–

- `cross_val_score`å‡½æ•°åœ¨é»˜è®¤æƒ…å†µä¸‹ä¸æä¾›æ•°æ®éšæœºåŒ–åŠŸèƒ½ï¼ˆå³ä¸ä¼šæ‰“ä¹±æ•°æ®é¡ºåºï¼‰ï¼Œè¿™æ˜¯å› ä¸ºäº¤å‰éªŒè¯çš„ç›®çš„æ˜¯è¯„ä¼°æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œè€Œä¸æ˜¯å­¦ä¹ æ•°æ®é›†ä¸­çš„å…·ä½“ç‰¹å¾ã€‚
- å› æ­¤ï¼Œåœ¨äº¤å‰éªŒè¯è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿æ¯ä¸ªfoldä¸­çš„æ•°æ®æ ·æœ¬éƒ½æ˜¯éšæœºé€‰æ‹©çš„ï¼Œä»¥é¿å…è¿‡åº¦æ‹Ÿåˆæˆ–æ¬ æ‹Ÿåˆçš„æƒ…å†µå‘ç”Ÿã€‚
- `scikit-learn`åº“æä¾›äº†è®¸å¤šç”¨äºæ•°æ®éšæœºåŒ–çš„å‡½æ•°å’Œç±»ï¼Œæ¯”å¦‚`shuffle`ã€`StratifiedShuffleSplit`ç­‰ã€‚å¦‚æœéœ€è¦åœ¨äº¤å‰éªŒè¯è¿‡ç¨‹ä¸­è¿›è¡Œæ•°æ®éšæœºåŒ–ï¼Œå¯ä»¥ä½¿ç”¨è¿™äº›å‡½æ•°æˆ–ç±»æ¥å®ç°ã€‚

### cross_val_scoreğŸˆ

- [sklearn.model_selection.cross_val_score â€” scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)

- `cross_val_score`æ˜¯Scikit-learnä¸­ç”¨äºæ‰§è¡ŒKæŠ˜äº¤å‰éªŒè¯çš„å‡½æ•°ä¹‹ä¸€ã€‚å®ƒå¯ä»¥å¸®åŠ©æˆ‘ä»¬è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒçš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šçš„æ€§èƒ½ï¼Œå¹¶è®¡ç®—æ‰€é€‰è¯„ä¼°æŒ‡æ ‡çš„å¹³å‡å€¼å’Œæ ‡å‡†å·®ã€‚
- ä½¿ç”¨`cross_val_score`å‡½æ•°éå¸¸ç®€å•ï¼Œåªéœ€è¦æä¾›è¦è¯„ä¼°çš„æ¨¡å‹ã€è¦ä½¿ç”¨çš„æ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡ä»¥åŠäº¤å‰éªŒè¯çš„æŠ˜æ•°å³å¯ã€‚

ä»¥ä¸‹æ˜¯`cross_val_score`å‡½æ•°çš„åŸºæœ¬è¯­æ³•ï¼š

```python
from sklearn.model_selection import cross_val_score

scores = cross_val_score(estimator, X, y, cv=k, scoring=None)
```

å…¶ä¸­ï¼Œå‚æ•°å«ä¹‰å¦‚ä¸‹ï¼š

- `estimator`: è¦è¯„ä¼°çš„æ¨¡å‹æˆ–ç®¡é“å¯¹è±¡ã€‚
- `X`: ç‰¹å¾æ•°æ®é›†ã€‚
- `y`: æ ‡ç­¾æ•°æ®é›†ã€‚
- `cv`: äº¤å‰éªŒè¯çš„æŠ˜æ•°ï¼Œé»˜è®¤ä¸º5ã€‚
- `scoring`: è¦ä½¿ç”¨çš„è¯„ä¼°æŒ‡æ ‡ï¼Œé»˜è®¤ä¸ºæ¨¡å‹çš„é»˜è®¤æŒ‡æ ‡ã€‚

- `cross_val_score`å‡½æ•°è¿”å›ä¸€ä¸ªåŒ…å«æ¯ä¸ªæŠ˜çš„è¯„ä¼°æŒ‡æ ‡å¾—åˆ†çš„æ•°ç»„ï¼Œå¯ä»¥é€šè¿‡è®¡ç®—å…¶å¹³å‡å€¼å’Œæ ‡å‡†å·®æ¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚å¦‚æœæ²¡æœ‰æŒ‡å®šè¯„ä¼°æŒ‡æ ‡ï¼Œåˆ™é»˜è®¤ä½¿ç”¨æ¨¡å‹çš„é»˜è®¤æŒ‡æ ‡ã€‚

- ä»¥ä¸‹æ˜¯ä¸€ä¸ªä½¿ç”¨`cross_val_score`å‡½æ•°è¯„ä¼°Logisticå›å½’æ¨¡å‹æ€§èƒ½çš„ç®€å•ç¤ºä¾‹ï¼š

```python
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score

X, y = make_classification(n_samples=1000, random_state=0)

lr = LogisticRegression()
scores = cross_val_score(lr, X, y, cv=5)

print(f'{scores=}')
print("Accuracy: {:.2f} (+/- {:.2f})".format(scores.mean(), scores.std()))
```

- åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨`make_classification`å‡½æ•°ç”Ÿæˆä¸€ä¸ªäºŒåˆ†ç±»æ•°æ®é›†ï¼Œç„¶åä½¿ç”¨`LogisticRegression`ä½œä¸ºè¯„ä¼°æ¨¡å‹ã€‚æˆ‘ä»¬é€šè¿‡ä¼ é€’æ¨¡å‹ã€ç‰¹å¾æ•°æ®é›†å’Œæ ‡ç­¾æ•°æ®é›†ä»¥åŠ5æŠ˜äº¤å‰éªŒè¯æ¥è°ƒç”¨`cross_val_score`å‡½æ•°ã€‚æœ€åï¼Œæˆ‘ä»¬è®¡ç®—è¯„ä¼°æŒ‡æ ‡å¾—åˆ†çš„å¹³å‡å€¼å’Œæ ‡å‡†å·®ï¼Œå¹¶å°†å…¶æ‰“å°å‡ºæ¥ã€‚

### cvå‚æ•°

- `cv`å‚æ•°æ˜¯`scikit-learn`åº“ä¸­è®¸å¤šæœºå™¨å­¦ä¹ æ¨¡å‹çš„äº¤å‰éªŒè¯ç­–ç•¥å‚æ•°ï¼Œå®ƒå¯ä»¥ç”¨æ¥æŒ‡å®šäº¤å‰éªŒè¯çš„æŠ˜æ•°æˆ–è€…å…·ä½“çš„äº¤å‰éªŒè¯åˆ’åˆ†æ–¹æ³•ã€‚`cv`å‚æ•°å¯ä»¥ä¼ é€’ä»¥ä¸‹å‡ ç§å€¼ï¼š

  - `None`ï¼ˆé»˜è®¤å€¼ï¼‰ï¼šä½¿ç”¨é»˜è®¤çš„5æŠ˜äº¤å‰éªŒè¯æ–¹æ³•ï¼›
  - æ•´æ•°ï¼šæŒ‡å®šKFoldæˆ–StratifiedKFoldçš„æŠ˜æ•°ï¼›
  - `CV splitter`ï¼šè‡ªå®šä¹‰äº¤å‰éªŒè¯ç”Ÿæˆå™¨ï¼›
  - è¿­ä»£å™¨ï¼šç”Ÿæˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ç´¢å¼•ã€‚

  å¯¹äºæ•´æ•°æˆ–`None`ç±»å‹çš„è¾“å…¥ï¼Œå¦‚æœæ¨¡å‹æ˜¯åˆ†ç±»å™¨å¹¶ä¸”yæ˜¯äºŒå…ƒæˆ–å¤šå…ƒåˆ†ç±»çš„ï¼Œå°†ä½¿ç”¨`StratifiedKFold`æ–¹æ³•ã€‚å¦åˆ™ï¼Œå°†ä½¿ç”¨`KFold`æ–¹æ³•ã€‚è¿™äº›æ‹†åˆ†å™¨çš„`shuffle`å‚æ•°é»˜è®¤ä¸º`False`ï¼Œå› æ­¤æ¯æ¬¡æ‹†åˆ†çš„ç»“æœç›¸åŒã€‚

  éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä¸åŒçš„äº¤å‰éªŒè¯ç­–ç•¥å¯èƒ½é€‚ç”¨äºä¸åŒçš„æ•°æ®é›†å’Œæ¨¡å‹ï¼Œå› æ­¤å¯ä»¥æ ¹æ®å…·ä½“çš„éœ€æ±‚é€‰æ‹©ä¸åŒçš„äº¤å‰éªŒè¯æ–¹æ³•ã€‚

### cross_val_predictğŸˆ

- [sklearn.model_selection.cross_val_predict â€” scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html)

  - Generate cross-validated **estimates** for each input data point.

    The data is split according to the cv parameter. Each sample belongs to exactly one test set, and its prediction is computed with an estimator fitted on the corresponding training set.

  - Passing these predictions into an evaluation metric may not be a valid way to measure generalization performance. 

  - Results can differ from [`cross_validate`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate) and [`cross_val_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score) <u>unless all tests sets have equal size and the metric decomposes over samples.</u>

  - ä¸ºæ¯ä¸ªè¾“å…¥æ•°æ®ç‚¹ç”Ÿæˆäº¤å‰éªŒè¯ä¼°è®¡å€¼ã€‚

    æ•°æ®æ ¹æ®cvå‚æ•°è¿›è¡Œæ‹†åˆ†ã€‚æ¯ä¸ªæ ·æœ¬éƒ½å±äºä¸€ä¸ªæµ‹è¯•é›†ï¼Œå…¶é¢„æµ‹æ˜¯ä½¿ç”¨ç›¸åº”çš„è®­ç»ƒé›†æ‹Ÿåˆçš„ä¼°è®¡å™¨è®¡ç®—çš„ã€‚

  - å°†è¿™äº›é¢„æµ‹ä¼ é€’åˆ°è¯„ä¼°åº¦é‡æ ‡å‡†ä¸­å¯èƒ½ä¸æ˜¯è¡¡é‡æ³›åŒ–æ€§èƒ½çš„æœ‰æ•ˆæ–¹æ³•ã€‚

  - ç»“æœå¯èƒ½ä¸cross_validateå’Œcross_val_scoreä¸åŒï¼Œé™¤éæ‰€æœ‰æµ‹è¯•é›†å…·æœ‰ç›¸ç­‰çš„å¤§å°ï¼Œå¹¶ä¸”åº¦é‡æ ‡å‡†é’ˆå¯¹æ ·æœ¬è¿›è¡Œåˆ†è§£ã€‚

- `sklearn.model_selection.cross_val_predict`å‡½æ•°æ˜¯`scikit-learn`ä¸­çš„ä¸€ä¸ªäº¤å‰éªŒè¯å‡½æ•°ï¼Œå®ƒå¯ä»¥ç”¨äºå¯¹æ¨¡å‹è¿›è¡Œäº¤å‰éªŒè¯ï¼Œå¹¶è¿”å›äº¤å‰éªŒè¯çš„é¢„æµ‹ç»“æœã€‚

- ä¸`cross_val_score`å‡½æ•°ä¸åŒçš„æ˜¯ï¼Œ`cross_val_predict`å‡½æ•°ä¸è¿”å›æ¯æ¬¡äº¤å‰éªŒè¯çš„å¾—åˆ†ï¼Œè€Œæ˜¯è¿”å›æ¯æ¬¡äº¤å‰éªŒè¯çš„é¢„æµ‹ç»“æœã€‚è¿™äº›é¢„æµ‹ç»“æœå¯ä»¥ç”¨äºè®¡ç®—æ¨¡å‹çš„æ€§èƒ½æŒ‡æ ‡ï¼Œæ¯”å¦‚å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1å€¼ç­‰ã€‚

### cross_val_score@cross_validate

- [sklearn.model_selection.cross_val_score â€” scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)

- [sklearn.model_selection.cross_validate â€” scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html)

- `cross_val_score`å’Œ`cross_validate`éƒ½æ˜¯Scikit-learnä¸­ç”¨äºæ‰§è¡ŒKæŠ˜äº¤å‰éªŒè¯çš„å‡½æ•°ï¼Œå®ƒä»¬çš„åŒºåˆ«åœ¨äºï¼š

  1. è¿”å›ç»“æœä¸åŒï¼š`cross_val_score`åªè¿”å›ä¸€ä¸ªåŒ…å«æ¯ä¸ªæŠ˜çš„è¯„ä¼°æŒ‡æ ‡å¾—åˆ†çš„æ•°ç»„ï¼Œè€Œ`cross_validate`è¿”å›ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«æ¯ä¸ªæŒ‡æ ‡çš„å¾—åˆ†æ•°ç»„ã€æ¯ä¸ªæ‹Ÿåˆæ—¶é—´çš„æ•°ç»„å’Œæ¯ä¸ªé¢„æµ‹æ—¶é—´çš„æ•°ç»„ã€‚
  2. å¯é€‰å‚æ•°ä¸åŒï¼š`cross_validate`å‡½æ•°æ¯”`cross_val_score`å‡½æ•°å¤šäº†ä¸€äº›å¯é€‰å‚æ•°ï¼Œä¾‹å¦‚è¿”å›è®­ç»ƒå¾—åˆ†ã€æµ‹è¯•å¾—åˆ†ã€æ‹Ÿåˆæ—¶é—´å’Œé¢„æµ‹æ—¶é—´ç­‰ã€‚
  3. é€‚ç”¨åœºåˆä¸åŒï¼š`cross_val_score`é€‚ç”¨äºç®€å•çš„è¯„ä¼°æ¨¡å‹æ€§èƒ½çš„æƒ…å†µï¼Œè€Œ`cross_validate`é€‚ç”¨äºæ›´å¤æ‚çš„æƒ…å†µï¼Œä¾‹å¦‚éœ€è¦åŒæ—¶è¯„ä¼°å¤šä¸ªæŒ‡æ ‡å’Œè®°å½•æ¨¡å‹æ‹Ÿåˆå’Œé¢„æµ‹æ—¶é—´çš„æƒ…å†µã€‚

  å› æ­¤ï¼Œåœ¨ç®€å•çš„æ¨¡å‹è¯„ä¼°ä»»åŠ¡ä¸­ï¼Œ`cross_val_score`æ˜¯æ›´å¸¸ç”¨å’Œæ›´æ–¹ä¾¿çš„å‡½æ•°ã€‚ä½†åœ¨æ›´å¤æ‚çš„ä»»åŠ¡ä¸­ï¼Œ`cross_validate`å¯èƒ½æ›´é€‚åˆï¼Œå› ä¸ºå®ƒå¯ä»¥æä¾›æ›´å¤šçš„ä¿¡æ¯å’Œçµæ´»æ€§ã€‚

- ä»¥ä¸‹æ˜¯ä½¿ç”¨`cross_val_score`å’Œ`cross_validate`å‡½æ•°çš„ç¤ºä¾‹ï¼š

  ```python
  from sklearn.datasets import load_digits
  from sklearn.svm import SVC
  from sklearn.model_selection import cross_val_score, cross_validate
  
  digits = load_digits()
  X, y = digits.data, digits.target # type: ignore
  
  # ä½¿ç”¨cross_val_scoreå‡½æ•°
  clf = SVC(kernel='linear', C=1, random_state=0)
  scores = cross_val_score(clf, X, y, cv=5)
  print("Cross-validation scores: ", scores)
  
  # ä½¿ç”¨cross_validateå‡½æ•°
  scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']
  clf = SVC(kernel='linear', C=1, random_state=0)
  scores = cross_validate(clf, X, y, scoring=scoring, cv=5, return_train_score=True)
  print("Train scores: ", scores['train_accuracy'])
  print("Test scores: ", scores['test_accuracy'])
  print("Fit time: ", scores['fit_time'])
  print("Score time: ", scores['score_time'])
  ```

- åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨`load_digits`åŠ è½½äº†ä¸€ä¸ªæ‰‹å†™æ•°å­—åˆ†ç±»æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨`SVC`ä½œä¸ºåˆ†ç±»å™¨ã€‚æˆ‘ä»¬ä½¿ç”¨`cross_val_score`å‡½æ•°è®¡ç®—äº†5æŠ˜äº¤å‰éªŒè¯çš„å‡†ç¡®æ€§å¾—åˆ†ï¼Œå¹¶ä½¿ç”¨`cross_validate`å‡½æ•°è®¡ç®—äº†5æŠ˜äº¤å‰éªŒè¯çš„å¤šä¸ªåº¦é‡å’Œæ‹Ÿåˆæ—¶é—´ã€‚æ³¨æ„ï¼Œ`cross_validate`å‡½æ•°è¿”å›ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«è®­ç»ƒå¾—åˆ†ã€æµ‹è¯•å¾—åˆ†å’Œæ‹Ÿåˆæ—¶é—´ç­‰ä¿¡æ¯ã€‚

  

## è½½å…¥@ç”Ÿæˆæ•°æ®é›†

- [sklearn.datasets](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets)
- The sklearn.datasets module includes utilities to load datasets, including methods to load and fetch popular reference datasets. It also features some artificial data generators.
  - Loaders
  - Samples generator

### è‡ªå¸¦ç°æœ‰æ•°æ®é›†@Loaders

- ä»¥é¸¢å°¾èŠ±æ•°æ®é›†ä¸ºä¾‹

  - [sklearn.datasets.load_iris â€” scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html)

  - ```python
    X,y=load_iris(return_X_y=True)
    #æ•ˆæœç­‰åŒäº
    data=load_iris()
    X=data.data
    y=data.target
    #ç”±äºBunchå¯¹è±¡çš„ç‰¹æ€§,å¯ä»¥ç”¨å­—å…¸æ–¹å¼è®¿é—®
    X=data['data']
    y=data['target']
    # 
    # å¯¼å‡ºä¸ºpandas dataframe:
    frame_data=load_iris(as_frame=True)
    X_df=frame_data.data
    y_df=frame_data.target
    
    ```

- è¿™ç±»æ–¹æ³•è¿”å›ä¸€ç§ç‰¹å®šçš„å¯¹è±¡:Bunch[sklearn.utils.Bunch â€” scikit-learn  documentation](https://scikit-learn.org/stable/modules/generated/sklearn.utils.Bunch.html#sklearn.utils.Bunch)

### ç”Ÿæˆæ•°æ®é›†@Samples generator

#### make_regressionğŸˆ

- [sklearn.datasets.make_regression â€” scikit-learn 1.2.2 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html)

  - Generate a random regression problem.

    The input set can either be well conditioned (by default) or have a low rank-fat tail singular profile. See make_low_rank_matrix for more details.

    The output is generated by applying a (potentially biased) random linear regression model with n_informative nonzero regressors to the previously generated input and some gaussian centered noise with some adjustable scale.

    Read more in the User Guide.

  - [sklearn.datasets.make_regression-scikit-learnä¸­æ–‡ç¤¾åŒº](https://scikit-learn.org.cn/view/595.html)

    è¾“å…¥é›†å¯ä»¥æ˜¯è‰¯å¥½æ¡ä»¶çš„ï¼ˆé»˜è®¤æƒ…å†µä¸‹ï¼‰æˆ–å…·æœ‰ä½ç§©-çŸ­å°¾å¥‡å¼‚è½®å»“ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§make_low_rank_matrixã€‚

    è¾“å‡ºæ˜¯é€šè¿‡å°†ä¸€ä¸ªï¼ˆå¯èƒ½æœ‰åå·®çš„ï¼‰éšæœºçº¿æ€§å›å½’æ¨¡å‹åº”ç”¨äºå…ˆå‰ç”Ÿæˆçš„è¾“å…¥å’Œä¸€äº›å…·æœ‰å¯è°ƒèŠ‚å°ºåº¦çš„é«˜æ–¯å™ªå£°çš„n_informativeéé›¶å›å½’å™¨æ¥ç”Ÿæˆçš„ã€‚

  - `make_regression`æ˜¯Scikit-learnä¸­çš„ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºç”Ÿæˆä¸€ä¸ªéšæœºå›å½’æ•°æ®é›†ã€‚å®ƒçš„è¯­æ³•å¦‚ä¸‹ï¼š

    ```python
    make_regression(n_samples=100, n_features=100, n_informative=10,
                    n_targets=1, bias=0.0, noise=0.0, shuffle=True, 
                    coef=False, random_state=None)
    ```

    å‚æ•°è¯´æ˜ï¼š

    - n_samplesï¼šç”Ÿæˆçš„æ ·æœ¬æ•°ï¼Œé»˜è®¤ä¸º100ã€‚
    - n_featuresï¼šç”Ÿæˆçš„ç‰¹å¾æ•°ï¼Œé»˜è®¤ä¸º100ã€‚
    - n_informativeï¼šç”Ÿæˆçš„æœ‰æ•ˆç‰¹å¾æ•°ï¼Œé»˜è®¤ä¸º10ã€‚è¿™äº›æœ‰æ•ˆç‰¹å¾ä¼šå¯¹ç›®æ ‡å˜é‡æœ‰è´¡çŒ®ã€‚
    - n_targetsï¼šç”Ÿæˆçš„ç›®æ ‡å˜é‡æ•°ï¼Œé»˜è®¤ä¸º1ã€‚
    - biasï¼šåç½®é¡¹ï¼Œé»˜è®¤ä¸º0.0ã€‚
    - noiseï¼šå™ªå£°é¡¹ï¼Œé»˜è®¤ä¸º0.0ã€‚åœ¨ç›®æ ‡å˜é‡ä¸ŠåŠ ä¸Šä¸€äº›å™ªå£°æ¥ä½¿æ•°æ®æ›´çœŸå®ã€‚
    - shuffleï¼šæ˜¯å¦æ‰“ä¹±æ ·æœ¬ï¼Œé»˜è®¤ä¸ºTrueã€‚
    - coefï¼šæ˜¯å¦è¿”å›ç”¨äºç”Ÿæˆæ•°æ®é›†çš„ç³»æ•°ï¼Œé»˜è®¤ä¸ºFalseã€‚
    - random_stateï¼šéšæœºæ•°ç§å­ã€‚

    `make_regression`å‡½æ•°è¿”å›ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«ç”Ÿæˆçš„æ•°æ®é›†å’ŒçœŸå®çš„ç›®æ ‡å˜é‡ã€‚å…¶ä¸­ï¼Œæ•°æ®é›†çš„å½¢çŠ¶ä¸º(n_samples, n_features)ï¼Œç›®æ ‡å˜é‡çš„å½¢çŠ¶ä¸º(n_samples, n_targets)ã€‚

  - make_regressionå‡½æ•°ç”Ÿæˆçš„æ•°æ®å¯ä»¥ç†è§£ä¸ºä¸€ä¸ªå¸¦æœ‰ä¸€å®šè§„å¾‹çš„æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«äº†è¾“å…¥ç‰¹å¾å’Œç›®æ ‡å€¼ã€‚

    è¾“å…¥ç‰¹å¾æ˜¯ä¸€ä¸ªäºŒç»´æ•°ç»„ï¼Œæ¯è¡Œè¡¨ç¤ºä¸€ä¸ªæ ·æœ¬ï¼Œæ¯åˆ—è¡¨ç¤ºä¸€ä¸ªç‰¹å¾ã€‚æˆ‘ä»¬å¯ä»¥å°†å…¶ç†è§£ä¸ºæ•°æ®é›†ä¸­çš„å±æ€§ï¼Œä¾‹å¦‚åœ¨æˆ¿ä»·é¢„æµ‹ä¸­ï¼Œç‰¹å¾å¯ä»¥æ˜¯æˆ¿å±‹çš„é¢ç§¯ã€å§å®¤æ•°é‡ç­‰ç­‰ã€‚

    ç›®æ ‡å€¼æ˜¯ä¸€ä¸ªä¸€ç»´æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ è¡¨ç¤ºå¯¹åº”æ ·æœ¬çš„çœŸå®å€¼ã€‚æˆ‘ä»¬å¯ä»¥å°†å…¶ç†è§£ä¸ºæ•°æ®é›†ä¸­çš„æ ‡ç­¾æˆ–è¾“å‡ºï¼Œä¾‹å¦‚åœ¨æˆ¿ä»·é¢„æµ‹ä¸­ï¼Œç›®æ ‡å€¼å¯ä»¥æ˜¯è¯¥æˆ¿å±‹çš„çœŸå®å”®ä»·ã€‚

    make_regressionå‡½æ•°ç”Ÿæˆçš„æ•°æ®é›†å…·æœ‰ä¸€å®šçš„è§„å¾‹ï¼Œå¯ä»¥æ˜¯çº¿æ€§å…³ç³»æˆ–éçº¿æ€§å…³ç³»ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ç”Ÿæˆä¸€ä¸ªçº¿æ€§å…³ç³»çš„æ•°æ®é›†ï¼Œå…¶ä¸­è¾“å…¥ç‰¹å¾å’Œç›®æ ‡å€¼ä¹‹é—´å­˜åœ¨çº¿æ€§å…³ç³»ï¼Œå¯ä»¥ç”¨çº¿æ€§å›å½’ç®—æ³•æ¥é¢„æµ‹ç›®æ ‡å€¼ï¼›æˆ–è€…ç”Ÿæˆä¸€ä¸ªéçº¿æ€§å…³ç³»çš„æ•°æ®é›†ï¼Œå…¶ä¸­è¾“å…¥ç‰¹å¾å’Œç›®æ ‡å€¼ä¹‹é—´å­˜åœ¨å¤æ‚çš„éçº¿æ€§å…³ç³»ï¼Œéœ€è¦ç”¨æ›´å¤æ‚çš„ç®—æ³•æ¥é¢„æµ‹ç›®æ ‡å€¼ã€‚

    é€šè¿‡ç”Ÿæˆä¸åŒè§„å¾‹çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬å¯ä»¥æµ‹è¯•å’Œè¯„ä¼°ä¸åŒçš„å›å½’ç®—æ³•åœ¨ä¸åŒæƒ…å†µä¸‹çš„æ€§èƒ½å’Œæ•ˆæœï¼Œä»è€Œé€‰æ‹©æœ€é€‚åˆå®é™…åº”ç”¨çš„ç®—æ³•ã€‚

#### eg

- ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨`make_regression`ç”Ÿæˆéšæœºå›å½’æ•°æ®é›†çš„ä¾‹å­ï¼š

  ```python
  from sklearn.datasets import make_regression
  
  # generate a random regression dataset
  X, y = make_regression(n_samples=1000, n_features=5, n_informative=2, noise=0.5, random_state=42)
  
  # print the shape of the dataset and target variable
  print("Shape of dataset:", X.shape)
  print("Shape of target variable:", y.shape)
  ```

- åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨`make_regression`å‡½æ•°ç”Ÿæˆä¸€ä¸ªåŒ…å«1000ä¸ªæ ·æœ¬ã€5ä¸ªç‰¹å¾å’Œ2ä¸ªæœ‰æ•ˆç‰¹å¾çš„éšæœºå›å½’æ•°æ®é›†ã€‚

- æˆ‘ä»¬è¿˜åœ¨ç›®æ ‡å˜é‡ä¸Šæ·»åŠ äº†ä¸€äº›å™ªå£°ã€‚æœ€åï¼Œæˆ‘ä»¬æ‰“å°å‡ºæ•°æ®é›†å’Œç›®æ ‡å˜é‡çš„å½¢çŠ¶ã€‚

- éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œç”±äº`make_regression`æ˜¯ä¸€ä¸ªéšæœºå‡½æ•°ï¼Œæ¯æ¬¡ç”Ÿæˆçš„æ•°æ®é›†éƒ½æ˜¯ä¸åŒçš„ã€‚æ­¤å¤–ï¼Œå¯ä»¥é€šè¿‡è°ƒæ•´å‚æ•°æ¥æ§åˆ¶ç”Ÿæˆæ•°æ®é›†çš„ç‰¹å¾å’Œç›®æ ‡å˜é‡çš„å±æ€§ï¼Œä»¥åŒ¹é…å…·ä½“é—®é¢˜çš„éœ€æ±‚ã€‚

#### visualization

- å½“ç”Ÿæˆçš„æ•°æ®é›†çš„ç‰¹å¾ç»´åº¦è¾ƒä½ï¼ˆå¦‚2ç»´æˆ–3ç»´ï¼‰æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¯è§†åŒ–å·¥å…·æ¥ç›´è§‚åœ°å±•ç¤ºè¾“å‡ºç»“æœã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼Œä½¿ç”¨matplotlibåº“æ¥å¯è§†åŒ–è¾“å‡ºç»“æœï¼š

  ```python
  import numpy as np
  import matplotlib.pyplot as plt
  from sklearn.datasets import make_regression
  
  # ç”Ÿæˆæ•°æ®é›†
  X, y = make_regression(n_samples=100, n_features=1, noise=10)
  
  # ç»˜åˆ¶æ•£ç‚¹å›¾
  plt.scatter(X, y)
  
  # ç»˜åˆ¶å›å½’ç›´çº¿
  x_min, x_max = plt.xlim()
  coef, bias = np.polyfit(X.ravel(), y.ravel(), 1)
  plt.plot([x_min, x_max], [x_min * coef + bias, x_max * coef + bias], 'r')
  
  # æ·»åŠ æ ‡é¢˜å’Œæ ‡ç­¾
  plt.title("Random Regression Problem")
  plt.xlabel("Input Feature")
  plt.ylabel("Target Value")
  
  # æ˜¾ç¤ºå›¾åƒ
  plt.show()
  ```

- è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†ä¸€ä¸ªç®€å•çš„ä¸€ç»´å›å½’é—®é¢˜ï¼Œç„¶åä½¿ç”¨æ•£ç‚¹å›¾å±•ç¤ºäº†è¾“å…¥ç‰¹å¾å’Œç›®æ ‡å€¼ä¹‹é—´çš„å…³ç³»ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨polyfitå‡½æ•°æ‹Ÿåˆäº†ä¸€æ¡å›å½’ç›´çº¿ï¼Œå¹¶å°†å…¶ç»˜åˆ¶åœ¨æ•£ç‚¹å›¾ä¸Šï¼Œä»¥ä¾¿æ›´å¥½åœ°å±•ç¤ºå›å½’é—®é¢˜çš„è§„å¾‹ã€‚

  - Least squares polynomial fit.æœ€å°äºŒä¹˜å¤šé¡¹å¼æ‹Ÿåˆ:numpy.polyfit â€” NumPy v1.24 Manual](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html)

  - `np.polyfit(x, y, deg)`æ˜¯numpyåº“ä¸­çš„ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºå¤šé¡¹å¼æ‹Ÿåˆï¼Œå³å°†ä¸€ç»„æ•°æ®ç‚¹æ‹Ÿåˆæˆä¸€ä¸ªå¤šé¡¹å¼å‡½æ•°ã€‚å®ƒçš„è¾“å…¥å‚æ•°åŒ…æ‹¬ï¼š

    - xï¼šè¦æ‹Ÿåˆçš„æ•°æ®ç‚¹çš„xåæ ‡æ•°ç»„ã€‚
    - yï¼šè¦æ‹Ÿåˆçš„æ•°æ®ç‚¹çš„yåæ ‡æ•°ç»„ã€‚
    - degï¼šæ‹Ÿåˆçš„å¤šé¡¹å¼æ¬¡æ•°ã€‚åœ¨â€œDegree of the fitting polynomialâ€ä¸­ï¼Œâ€œdegreeâ€å¯ä»¥ç¿»è¯‘ä¸ºâ€œæ¬¡æ•°â€æˆ–â€œé˜¶æ•°â€ï¼Œè¡¨ç¤ºæ‹Ÿåˆå¤šé¡¹å¼çš„æ¬¡æ•°æˆ–é˜¶æ•°ã€‚å› æ­¤ï¼Œå¯ä»¥å°†è¿™ä¸ªçŸ­è¯­ç¿»è¯‘ä¸ºâ€œæ‹Ÿåˆå¤šé¡¹å¼çš„æ¬¡æ•°â€ã€‚æœ‰æ—¶ç”¨orderæ¥è¡¨ç¤ºé˜¶æ•°

  - è¯¥å‡½æ•°è¿”å›çš„æ˜¯ä¸€ä¸ªä¸€ç»´æ•°ç»„ï¼ŒåŒ…å«äº†å¤šé¡¹å¼å‡½æ•°çš„ç³»æ•°ï¼Œä»é«˜æ¬¡åˆ°ä½æ¬¡æ’åˆ—ã€‚

  - ä¾‹å¦‚ï¼Œå¯¹äºä¸€ä¸ªäºŒæ¬¡å¤šé¡¹å¼æ‹Ÿåˆï¼Œè¿”å›çš„ç³»æ•°æ•°ç»„ä¸º[a, b, c]ï¼Œè¡¨ç¤ºæ‹Ÿåˆçš„å‡½æ•°ä¸º$y=ax^2+bx+c$ã€‚

  - ä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼Œä½¿ç”¨np.polyfitå‡½æ•°æ‹Ÿåˆä¸€æ¡å›å½’ç›´çº¿ï¼š

    ```python
    import numpy as np
    import matplotlib.pyplot as plt
    
    # ç”Ÿæˆæ•°æ®
    x = np.linspace(-10, 10, 100)
    y = 3 * x + 10 + np.random.randn(100) * 5
    
    # è®¡ç®—æ‹Ÿåˆç›´çº¿çš„ç³»æ•°
    # ç”±äºæ˜¯æ‹Ÿåˆçº¿æ€§å‡½æ•°,è¿”å›çš„ç»“æœå½¢å¦‚array([3.01458133, 9.44181608])çš„æ•°ç»„y=coef*x+bias
    coef, bias = np.polyfit(x, y, 1)
    
    # ç»˜åˆ¶æ•£ç‚¹å›¾å’Œæ‹Ÿåˆç›´çº¿
    plt.scatter(x, y)
    
    y=coef * x + bias
    plt.plot(x, y, color='red')
    
    # æ·»åŠ æ ‡é¢˜å’Œæ ‡ç­¾
    plt.title("Linear Regression Example")
    plt.xlabel("Input Feature")
    plt.ylabel("Target Value")
    
    # æ˜¾ç¤ºå›¾åƒ
    plt.show()
    ```

    è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨np.polyfitå‡½æ•°æ‹Ÿåˆäº†ä¸€æ¡å›å½’ç›´çº¿ï¼Œå°†ä¸€ç»„éšæœºç”Ÿæˆçš„æ•°æ®ç‚¹æ‹Ÿåˆæˆäº†ä¸€ä¸ªä¸€æ¬¡å‡½æ•°ã€‚æˆ‘ä»¬ä½¿ç”¨Matplotlibåº“ç»˜åˆ¶äº†æ•£ç‚¹å›¾å’Œæ‹Ÿåˆç›´çº¿ï¼Œå¹¶æ·»åŠ äº†æ ‡é¢˜å’Œæ ‡ç­¾ï¼Œä½¿å›¾è¡¨æ›´åŠ æ¸…æ™°å’Œæ˜“äºç†è§£ã€‚

- å½“ç‰¹å¾ç»´åº¦è¾ƒé«˜æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨é™ç»´æŠ€æœ¯ï¼ˆå¦‚PCAï¼‰å°†æ•°æ®é›†é™åˆ°2ç»´æˆ–3ç»´ï¼Œç„¶åä½¿ç”¨å¯è§†åŒ–å·¥å…·æ¥å±•ç¤ºè¾“å‡ºç»“æœã€‚

- å¦å¤–ï¼Œè¿˜å¯ä»¥ä½¿ç”¨ä¸€äº›é«˜çº§çš„å¯è§†åŒ–å·¥å…·ï¼ˆå¦‚[seaborn](https://seaborn.pydata.org/)åº“ï¼‰ï¼Œæ¥æ›´å¥½åœ°å±•ç¤ºå¤šç»´æ•°æ®é›†çš„ç‰¹å¾å’Œè§„å¾‹ã€‚

#### make_classification

- [sklearn.datasets.make_classification â€” scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html)

- Generate a random n-class classification problem.

  This initially creates clusters of points normally distributed (std=1) about vertices of an `n_informative`-dimensional hypercube with sides of length `2*class_sep` and assigns an equal number of clusters to each class. It introduces interdependence between these features and adds various types of further noise to the data.

  Without shuffling, `X` horizontally stacks features in the following order: the primary `n_informative` features, followed by `n_redundant` linear combinations of the informative features, followed by `n_repeated` duplicates, drawn randomly with replacement from the informative and redundant features. The remaining features are filled with random noise. Thus, without shuffling, all useful features are contained in the columns `X[:, :n_informative + n_redundant + n_repeated]`.

  é¦–å…ˆï¼Œåœ¨ä¸€ä¸ªn_informativeç»´çš„è¶…ç«‹æ–¹ä½“çš„é¡¶ç‚¹å‘¨å›´ç”Ÿæˆæ ‡å‡†å·®ä¸º1çš„é«˜æ–¯åˆ†å¸ƒæ•°æ®ç‚¹ç°‡ï¼Œå¹¶å°†ç›¸ç­‰æ•°é‡çš„ç°‡åˆ†é…ç»™æ¯ä¸ªç±»åˆ«ã€‚å®ƒåœ¨è¿™äº›ç‰¹å¾ä¹‹é—´å¼•å…¥ç›¸äº’ä¾èµ–æ€§ï¼Œå¹¶å‘æ•°æ®æ·»åŠ å„ç§ç±»å‹çš„å™ªå£°ã€‚

  å¦‚æœä¸è¿›è¡Œæ´—ç‰Œï¼Œåˆ™XæŒ‰ç…§ä»¥ä¸‹é¡ºåºæ°´å¹³å †å ç‰¹å¾ï¼šn_informativeä¸ªä¸»è¦ç‰¹å¾ï¼Œåé¢æ˜¯n_redundantä¸ªä¿¡æ¯ç‰¹å¾çš„çº¿æ€§ç»„åˆï¼Œç„¶åæ˜¯éšæœºä»ä¿¡æ¯å’Œå†—ä½™ç‰¹å¾ä¸­é‡å¤é€‰æ‹©çš„n_repeatedä¸ªå‰¯æœ¬ã€‚å…¶ä½™ç‰¹å¾å¡«å……éšæœºå™ªå£°ã€‚å› æ­¤ï¼Œå¦‚æœä¸è¿›è¡Œæ´—ç‰Œï¼Œåˆ™æ‰€æœ‰æœ‰ç”¨ç‰¹å¾éƒ½åŒ…å«åœ¨X [:ï¼Œ: n_informative + n_redundant + n_repeated]åˆ—ä¸­ã€‚

- `make_classification`å‡½æ•°å¯ä»¥ç”Ÿæˆå…·æœ‰ä»¥ä¸‹ç‰¹å¾çš„åˆ†ç±»æ•°æ®é›†ï¼š

  - æ ·æœ¬æ•°å’Œç‰¹å¾æ•°å¯ä»¥è‡ªå®šä¹‰ã€‚
  - å¯ä»¥æŒ‡å®šç±»åˆ«æ•°é‡ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨é»˜è®¤å€¼ï¼ˆå³2ï¼‰ã€‚
  - å¯ä»¥æŒ‡å®šç‰¹å¾ä¹‹é—´çš„å…³è”åº¦ï¼ˆå³ç‰¹å¾ç›¸å…³æ€§ï¼‰ã€‚
  - å¯ä»¥æŒ‡å®šåˆ†ç±»å™¨çš„éš¾åº¦ï¼Œå³æ ·æœ¬æ˜¯å¦å®¹æ˜“åˆ†ç¦»ã€‚
  - å¯ä»¥æŒ‡å®šç”¨äºç”Ÿæˆæ•°æ®çš„éšæœºç§å­ï¼Œä»¥ä¾¿å®ç°å¯é‡å¤æ€§ã€‚

- `make_classification`å‡½æ•°å¯ä»¥ç”Ÿæˆä¸€ä¸ªå…·æœ‰nä¸ªç±»åˆ«çš„éšæœºåˆ†ç±»æ•°æ®é›†ï¼Œå…¶ä¸­æ¯ä¸ªç±»åˆ«åŒ…å«ä¸€ç»„ä»¥n_informativeç»´ä¸ºä¸­å¿ƒçš„é«˜æ–¯åˆ†å¸ƒæ•°æ®ç‚¹ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œå‡½æ•°è¿˜å¯ä»¥æ·»åŠ å™ªå£°å’Œå†—ä½™ç‰¹å¾ä»¥åŠå…¶ä»–ç±»å‹çš„å™ªå£°ã€‚ä¸‹é¢ç»™å‡ºä¸€ä¸ªä½¿ç”¨`make_classification`å‡½æ•°ç”Ÿæˆåˆ†ç±»æ•°æ®é›†çš„ç¤ºä¾‹ä»£ç ï¼š

  ```python
  from sklearn.datasets import make_classification
  
  X, y = make_classification(n_samples=1000, n_features=20, n_classes=5, n_informative=10, n_redundant=5, n_repeated=2, class_sep=2.0, random_state=0)
  
  print("X shape:", X.shape)
  print("y shape:", y.shape)
  ```

  - ```bash
    X shape: (1000, 20)
    y shape: (1000,)
    ```

  - ä¸Šé¢çš„ä»£ç ç”Ÿæˆäº†ä¸€ä¸ªåŒ…å«1000ä¸ªæ ·æœ¬å’Œ20ä¸ªç‰¹å¾çš„åˆ†ç±»æ•°æ®é›†ï¼Œå…¶ä¸­æœ‰5ä¸ªç±»åˆ«ã€‚

    - æ¯ä¸ªç±»åˆ«åŒ…å«ä¸€ç»„ä»¥10ç»´ä¸ºä¸­å¿ƒçš„é«˜æ–¯åˆ†å¸ƒæ•°æ®ç‚¹ï¼ŒåŒæ—¶æ·»åŠ äº†5ä¸ªå†—ä½™ç‰¹å¾å’Œ2ä¸ªé‡å¤ç‰¹å¾ï¼Œå¹¶ä¸”ç±»ä¹‹é—´çš„è·ç¦»ä¸º2.0ã€‚
    - æœ€ç»ˆå¾—åˆ°çš„æ•°æ®é›†åŒ…å«20ä¸ªç‰¹å¾å’Œ1ä¸ªç±»åˆ«æ ‡ç­¾ã€‚

  - å¯ä»¥æ ¹æ®å…·ä½“æƒ…å†µï¼Œè°ƒæ•´`make_classification`å‡½æ•°çš„å„ä¸ªå‚æ•°æ¥ç”Ÿæˆä¸åŒçš„åˆ†ç±»æ•°æ®é›†ã€‚

    - ä¾‹å¦‚ï¼Œå¯ä»¥é€šè¿‡è°ƒæ•´`n_samples`å‚æ•°æ¥æ§åˆ¶æ•°æ®é›†çš„è§„æ¨¡ï¼Œé€šè¿‡è°ƒæ•´`n_informative`å‚æ•°æ¥æ§åˆ¶æœ‰ç”¨ç‰¹å¾çš„æ•°é‡ï¼Œé€šè¿‡è°ƒæ•´`class_sep`å‚æ•°æ¥æ§åˆ¶ç±»ä¹‹é—´çš„è·ç¦»ç­‰ç­‰ã€‚



## å‚æ•°è¯´æ˜

### n_informative

åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œ`n_informative`é€šå¸¸æ˜¯ç”Ÿæˆåˆ†ç±»æ•°æ®é›†æ—¶çš„ä¸€ä¸ªå‚æ•°ï¼Œç”¨äºæ§åˆ¶ç”Ÿæˆçš„æ•°æ®é›†ä¸­æœ‰ç”¨ç‰¹å¾çš„æ•°é‡ã€‚

å…·ä½“æ¥è¯´ï¼Œ`n_informative`è¡¨ç¤ºç”Ÿæˆæ•°æ®é›†æ—¶æ¯ä¸ªç±»åˆ«çš„ç‰¹å¾ä¸­æœ‰å¤šå°‘ä¸ªæ˜¯æœ‰ç”¨çš„ï¼Œä¹Ÿå°±æ˜¯èƒ½å¤ŸåŒºåˆ†ä¸åŒç±»åˆ«çš„ç‰¹å¾ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸€ä¸ªäºŒåˆ†ç±»é—®é¢˜ä¸­ï¼Œå¦‚æœè®¾ç½®`n_informative`ä¸º1ï¼Œåˆ™ç”Ÿæˆçš„æ•°æ®é›†ä¸­æ¯ä¸ªç±»åˆ«çš„ç‰¹å¾ä¸­åªæœ‰ä¸€ä¸ªèƒ½å¤ŸåŒºåˆ†ä¸åŒç±»åˆ«ï¼Œå…¶ä½™çš„ç‰¹å¾éƒ½æ˜¯å™ªå£°æˆ–æ— ç”¨ç‰¹å¾ã€‚

é€šè¿‡è°ƒæ•´`n_informative`å‚æ•°ï¼Œå¯ä»¥æ§åˆ¶ç”Ÿæˆçš„æ•°æ®é›†ä¸­æœ‰ç”¨ç‰¹å¾çš„æ•°é‡ï¼Œä»è€Œæ§åˆ¶åˆ†ç±»é—®é¢˜çš„éš¾åº¦ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œè¾ƒå°‘çš„æœ‰ç”¨ç‰¹å¾ä¼šä½¿åˆ†ç±»é—®é¢˜æ›´åŠ å›°éš¾ï¼Œéœ€è¦æ›´å¤æ‚çš„æ¨¡å‹æ‰èƒ½è§£å†³ã€‚è€Œè¾ƒå¤šçš„æœ‰ç”¨ç‰¹å¾åˆ™ä¼šä½¿åˆ†ç±»é—®é¢˜æ›´å®¹æ˜“ï¼Œæ›´ç®€å•çš„æ¨¡å‹å°±èƒ½å¤Ÿå¾—åˆ°è¾ƒå¥½çš„ç»“æœã€‚

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œ`n_informative`ä¸æ˜¯å”¯ä¸€çš„æ§åˆ¶ç”Ÿæˆæ•°æ®é›†éš¾åº¦çš„å‚æ•°ï¼Œå…¶ä»–å‚æ•°å¦‚`n_classes`ã€`n_clusters_per_class`ã€`class_sep`ç­‰ä¹Ÿä¼šå¯¹æ•°æ®é›†çš„éš¾åº¦äº§ç”Ÿå½±å“ã€‚å› æ­¤ï¼Œåœ¨ç”Ÿæˆåˆ†ç±»æ•°æ®é›†æ—¶ï¼Œéœ€è¦ç»¼åˆè€ƒè™‘å¤šä¸ªå‚æ•°çš„å½±å“ï¼Œè°ƒæ•´å‚æ•°æ¥ç”Ÿæˆåˆé€‚çš„æ•°æ®é›†ã€‚

## sklearn metricsğŸˆ

Scikit-learnä¸­çš„`metrics`æ¨¡å—åŒ…å«äº†è®¸å¤šç”¨äºè¯„ä¼°æœºå™¨å­¦ä¹ æ¨¡å‹æ€§èƒ½çš„åº¦é‡æŒ‡æ ‡ï¼Œç”¨äºæ¯”è¾ƒé¢„æµ‹ç»“æœå’ŒçœŸå®ç»“æœä¹‹é—´çš„å·®å¼‚ï¼Œä»¥ç¡®å®šæ¨¡å‹çš„å‡†ç¡®æ€§ã€ç²¾åº¦ã€å¬å›ç‡ã€F1åˆ†æ•°ç­‰æ€§èƒ½ã€‚

`metrics`æ¨¡å—ä¸­å¸¸ç”¨çš„åº¦é‡æŒ‡æ ‡åŒ…æ‹¬ï¼š

- åˆ†ç±»é—®é¢˜åº¦é‡æŒ‡æ ‡ï¼šå¦‚å‡†ç¡®ç‡ï¼ˆaccuracyï¼‰ã€ç²¾ç¡®ç‡ï¼ˆprecisionï¼‰ã€å¬å›ç‡ï¼ˆrecallï¼‰ã€F1åˆ†æ•°ï¼ˆF1 scoreï¼‰ã€ROCæ›²çº¿ï¼ˆROC curveï¼‰ã€AUCå€¼ï¼ˆAUC scoreï¼‰ç­‰ã€‚
- å›å½’é—®é¢˜åº¦é‡æŒ‡æ ‡ï¼šå¦‚å‡æ–¹è¯¯å·®ï¼ˆmean squared errorï¼‰ã€å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆmean absolute errorï¼‰ã€Rå¹³æ–¹ï¼ˆR-squared scoreï¼‰ç­‰ã€‚
- èšç±»é—®é¢˜åº¦é‡æŒ‡æ ‡ï¼šå¦‚è½®å»“ç³»æ•°ï¼ˆsilhouette scoreï¼‰ç­‰ã€‚
- å¤šæ ‡ç­¾åˆ†ç±»é—®é¢˜åº¦é‡æŒ‡æ ‡ï¼šå¦‚æ±‰æ˜æŸå¤±ï¼ˆHamming lossï¼‰ã€Jaccardç›¸ä¼¼åº¦ï¼ˆJaccard similarityï¼‰ç­‰ã€‚

### accuracy_score

- [sklearn.metrics.accuracy_score â€” scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)

- åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œ`accuracy`æ˜¯ä¸€ç§ç”¨äºè¯„ä¼°åˆ†ç±»æ¨¡å‹æ€§èƒ½çš„æŒ‡æ ‡ï¼Œè¡¨ç¤ºæ¨¡å‹æ­£ç¡®åˆ†ç±»çš„æ ·æœ¬æ¯”ä¾‹ã€‚å…·ä½“åœ°ï¼Œ`accuracy`å¯ä»¥å®šä¹‰ä¸ºï¼š

```
accuracy = (TP + TN) / (TP + TN + FP + FN)
```

- å…¶ä¸­ï¼Œ`TP`è¡¨ç¤ºçœŸæ­£ä¾‹ï¼ˆTrue Positiveï¼Œå³æ¨¡å‹æ­£ç¡®é¢„æµ‹ä¸ºæ­£ä¾‹çš„æ ·æœ¬æ•°ï¼‰ï¼Œ`TN`è¡¨ç¤ºçœŸåä¾‹ï¼ˆTrue Negativeï¼Œå³æ¨¡å‹æ­£ç¡®é¢„æµ‹ä¸ºåä¾‹çš„æ ·æœ¬æ•°ï¼‰ï¼Œ`FP`è¡¨ç¤ºå‡æ­£ä¾‹ï¼ˆFalse Positiveï¼Œå³æ¨¡å‹é”™è¯¯é¢„æµ‹ä¸ºæ­£ä¾‹çš„æ ·æœ¬æ•°ï¼‰ï¼Œ`FN`è¡¨ç¤ºå‡åä¾‹ï¼ˆFalse Negativeï¼Œå³æ¨¡å‹é”™è¯¯é¢„æµ‹ä¸ºåä¾‹çš„æ ·æœ¬æ•°ï¼‰ã€‚

- åœ¨Scikit-learnä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`accuracy_score`å‡½æ•°æ¥è®¡ç®—åˆ†ç±»æ¨¡å‹çš„`accuracy`æŒ‡æ ‡ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼š

```python
from sklearn.metrics import accuracy_score
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

# ç”Ÿæˆä¸€ä¸ªäºŒåˆ†ç±»æ•°æ®é›†
X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=123)

# åˆ’åˆ†æ•°æ®é›†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)

# è®­ç»ƒä¸€ä¸ªé€»è¾‘å›å½’æ¨¡å‹
clf = LogisticRegression(random_state=123)
clf.fit(X_train, y_train)

# åœ¨æµ‹è¯•é›†ä¸Šè®¡ç®—accuracy
y_pred = clf.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print("Accuracy:", acc)
```

åœ¨ä¸Šè¿°ä»£ç ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨`make_classification`å‡½æ•°ç”Ÿæˆäº†ä¸€ä¸ªäºŒåˆ†ç±»æ•°æ®é›†ï¼Œç„¶åä½¿ç”¨`train_test_split`å‡½æ•°å°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚æ¥ç€ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªé€»è¾‘å›å½’æ¨¡å‹ï¼Œå¹¶åœ¨æµ‹è¯•é›†ä¸Šè®¡ç®—äº†æ¨¡å‹çš„`accuracy`æŒ‡æ ‡ã€‚æœ€åï¼Œæˆ‘ä»¬è¾“å‡ºäº†æ¨¡å‹çš„`accuracy`å€¼ã€‚

æ€»ä¹‹ï¼Œåœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œ`accuracy`æ˜¯ä¸€ç§ç”¨äºè¯„ä¼°åˆ†ç±»æ¨¡å‹æ€§èƒ½çš„æŒ‡æ ‡ï¼Œè¡¨ç¤ºæ¨¡å‹æ­£ç¡®åˆ†ç±»çš„æ ·æœ¬æ¯”ä¾‹ã€‚åœ¨Scikit-learnä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`accuracy_score`å‡½æ•°æ¥è®¡ç®—åˆ†ç±»æ¨¡å‹çš„`accuracy`æŒ‡æ ‡ã€‚

#### eg

- ```python
  from sklearn.metrics import accuracy_score
  y_pred = [0, 2, 1, 3]
  y_true = [0, 1, 2, 3]
  res=accuracy_score(y_true, y_pred),accuracy_score(y_true, y_pred, normalize=False)
  #é¢„æµ‹çš„4ä¸ªæ ·æœ¬ä¸­,y_pred[0,3]æ˜¯å¯¹çš„,å…¶ä½™æ˜¯é”™è¯¯çš„
  res_bool=np.array(y_pred)==np.array(y_true)
  print(res,res_bool)
  ```

  - ```bash
    (0.5, 2) [ True False False  True]
    ```

    

- ä¾‹å¦‚ï¼Œä¸‹é¢çš„ä»£ç æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨`metrics`æ¨¡å—ä¸­çš„`accuracy_score`å‡½æ•°è®¡ç®—åˆ†ç±»æ¨¡å‹çš„å‡†ç¡®ç‡ï¼š

  ```python
  from sklearn.datasets import make_classification
  from sklearn.linear_model import LogisticRegression
  from sklearn.model_selection import train_test_split
  from sklearn.metrics import accuracy_score
  
  X, y = make_classification(n_samples=1000, random_state=0)
  X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
  lr = LogisticRegression()
  
  lr.fit(X_train, y_train)
  y_pred = lr.predict(X_test)
  acc = accuracy_score(y_test, y_pred)
  print("Accuracy: {:.2f}".format(acc))
  ```

- ```
  Accuracy: 0.97
  ```

  åœ¨ä¸Šé¢çš„ä»£ç ä¸­ï¼Œä½¿ç”¨`accuracy_score`å‡½æ•°è®¡ç®—æ¨¡å‹çš„å‡†ç¡®ç‡ï¼Œå¹¶è¾“å‡ºç»“æœã€‚å¯ä»¥æ ¹æ®å…·ä½“æƒ…å†µé€‰æ‹©åˆé€‚çš„åº¦é‡æŒ‡æ ‡æ¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚

### accuracy_score vs cross_validate

- `cross_validate`å’Œ`accuracy_score`éƒ½æ˜¯Scikit-learnä¸­ç”¨äºè¯„ä¼°æ¨¡å‹æ€§èƒ½çš„å‡½æ•°ï¼Œä½†å®ƒä»¬çš„åº”ç”¨åœºæ™¯å’Œç”¨æ³•æœ‰æ‰€ä¸åŒã€‚

  - `accuracy_score`å‡½æ•°ç”¨äºè®¡ç®—åˆ†ç±»æ¨¡å‹çš„å‡†ç¡®ç‡ï¼Œå³é¢„æµ‹æ­£ç¡®çš„æ ·æœ¬æ•°å æ€»æ ·æœ¬æ•°çš„æ¯”ä¾‹ã€‚å®ƒåªèƒ½è®¡ç®—æ¨¡å‹çš„å•ä¸€æŒ‡æ ‡ï¼Œä¸è€ƒè™‘æ¨¡å‹çš„å…¶ä»–æ€§èƒ½æŒ‡æ ‡ï¼Œä¹Ÿä¸è€ƒè™‘æ¨¡å‹çš„è®­ç»ƒæ—¶é—´å’Œé¢„æµ‹æ—¶é—´ç­‰å…¶ä»–æ–¹é¢çš„æ€§èƒ½ã€‚

  - `cross_validate`å‡½æ•°åˆ™å¯ä»¥åŒæ—¶è®¡ç®—å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ï¼Œä¾‹å¦‚è®­ç»ƒå¾—åˆ†ã€æµ‹è¯•å¾—åˆ†ã€æ‹Ÿåˆæ—¶é—´å’Œé¢„æµ‹æ—¶é—´ç­‰ã€‚è¿™äº›è¯„ä¼°æŒ‡æ ‡å¯ä»¥å¸®åŠ©æˆ‘ä»¬æ›´å…¨é¢åœ°äº†è§£æ¨¡å‹çš„æ€§èƒ½å’Œç‰¹ç‚¹ã€‚æ­¤å¤–ï¼Œ`cross_validate`å‡½æ•°è¿˜å¯ä»¥æŒ‡å®šå¤šä¸ªè¯„ä¼°æŒ‡æ ‡ï¼Œä¾‹å¦‚å‡†ç¡®ç‡ã€ç²¾åº¦ã€å¬å›ç‡ã€F1åˆ†æ•°ç­‰ï¼Œä»è€Œæ›´å…¨é¢åœ°è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚`cross_validate`è¿˜å¯ä»¥ä½¿ç”¨ä¸åŒçš„äº¤å‰éªŒè¯ç­–ç•¥ï¼Œä¾‹å¦‚ç•™ä¸€äº¤å‰éªŒè¯å’Œåˆ†å±‚KæŠ˜äº¤å‰éªŒè¯ç­‰ï¼Œä»¥æ»¡è¶³ä¸åŒçš„éœ€æ±‚ã€‚

- ç»¼ä¸Šæ‰€è¿°ï¼Œ`accuracy_score`å‡½æ•°é€‚ç”¨äºç®€å•çš„åˆ†ç±»æ¨¡å‹è¯„ä¼°ä»»åŠ¡ï¼Œè€Œ`cross_validate`å‡½æ•°é€‚ç”¨äºæ›´å¤æ‚çš„æ¨¡å‹è¯„ä¼°ä»»åŠ¡ï¼Œå¯ä»¥æä¾›æ›´å…¨é¢çš„æ€§èƒ½æŒ‡æ ‡å’Œæ›´çµæ´»çš„äº¤å‰éªŒè¯ç­–ç•¥ã€‚