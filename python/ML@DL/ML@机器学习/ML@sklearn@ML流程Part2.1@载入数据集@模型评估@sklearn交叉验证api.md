[toc]



- ML@sklearn@MLæµç¨‹Part2.1@è½½å…¥æ•°æ®é›†@æ¨¡å‹è¯„ä¼°@sklearnäº¤å‰éªŒè¯api

# datasets@æ•°æ®é›†çš„è½½å…¥@ç”Ÿæˆ

- [sklearn.datasets](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets)
- The sklearn.datasets module includes utilities to load datasets, including methods to load and fetch popular reference datasets. It also features some artificial data generators.
  - Loaders
  - Samples generator

### sklearnä¸­çš„æ•°æ®é›†

- [ Dataset loading utilities â€” scikit-learn  documentation](https://scikit-learn.org/stable/datasets.html)

- **General dataset API.** There are three main kinds of dataset interfaces that can be used to get datasets depending on the desired type of dataset.

  - **The dataset loaders.** They can be used to load small standard datasets, described in the [Toy datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html#toy-datasets) section.

  - **The dataset fetchers.** They can be used to download and load larger datasets, described in the [Real world datasets](https://scikit-learn.org/stable/datasets/real_world.html#real-world-datasets) section.

    - Both loaders and fetchers functions return a [`Bunch`](https://scikit-learn.org/stable/modules/generated/sklearn.utils.Bunch.html#sklearn.utils.Bunch) object holding at least two items: an array of shape `n_samples` * `n_features` with key `data` (except for 20newsgroups) and a numpy array of length `n_samples`, containing the target values, with key `target`.
    - The Bunch object is a dictionary that exposes its keys as attributes. For more information about Bunch object, see [`Bunch`](https://scikit-learn.org/stable/modules/generated/sklearn.utils.Bunch.html#sklearn.utils.Bunch).
    - Itâ€™s also possible for almost all of these function to constrain the output to be a tuple containing only the data and the target, by setting the `return_X_y` parameter to `True`.
    - The datasets also contain a full description in their `DESCR` attribute and some contain `feature_names` and `target_names`. See the dataset descriptions below for details.

  - **The dataset generation functions.** They can be used to generate controlled synthetic datasets, described in the [Generated datasets](https://scikit-learn.org/stable/datasets/sample_generators.html#sample-generators) section.

    - These functions return a tuple `(X, y)` consisting of a `n_samples` * `n_features` numpy array `X` and an array of length `n_samples` containing the targets `y`.

    - In addition, there are also miscellaneous tools to load datasets of other formats or from other locations, described in the [Loading other datasets](https://scikit-learn.org/stable/datasets/loading_other_datasets.html#loading-other-datasets) section.



- åœ¨`sklearn.datasets`ä¸­ï¼Œæ•°æ®é›†çš„è·å–æ–¹å¼å¯åˆ†ä¸ºä¸¤ç±»ï¼š`load`ç³»åˆ—å’Œ`fetch`ç³»åˆ—ã€‚`load`ç³»åˆ—å‡½æ•°ç”¨äºåŠ è½½å†…ç½®çš„æ•°æ®é›†ï¼Œè€Œ`fetch`ç³»åˆ—å‡½æ•°ç”¨äºä»ç½‘ç»œä¸Šä¸‹è½½æ•°æ®é›†ã€‚å…·ä½“æ¥è¯´ï¼š

  - `load_*`å‡½æ•°ï¼šè¿™äº›å‡½æ•°ç”¨äºåŠ è½½å†…ç½®çš„æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†é€šå¸¸å·²ç»è¢«æ‰“åŒ…åˆ°`sklearn`åº“ä¸­ã€‚è¿™äº›å‡½æ•°è¿”å›çš„æ˜¯ä¸€ä¸ª`Bunch`å¯¹è±¡ï¼Œè¯¥å¯¹è±¡åŒ…å«äº†æ•°æ®é›†çš„å±æ€§å’Œæ•°æ®ã€‚
  - `fetch_*`å‡½æ•°ï¼šè¿™äº›å‡½æ•°ç”¨äºä»ç½‘ç»œä¸Šä¸‹è½½æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†é€šå¸¸æ¯”è¾ƒå¤§æˆ–è€…éœ€è¦ä¸€äº›é¢å¤–çš„å¤„ç†ã€‚è¿™äº›å‡½æ•°è¿”å›çš„æ˜¯ä¸€ä¸ª`Bunch`å¯¹è±¡ï¼Œè¯¥å¯¹è±¡åŒ…å«äº†æ•°æ®é›†çš„å±æ€§å’Œæ•°æ®ã€‚

  ä¸‹é¢æ˜¯ä¸€äº›å¸¸è§çš„`load`ç³»åˆ—å’Œ`fetch`ç³»åˆ—å‡½æ•°ï¼š

  - `load_boston()`å’Œ`fetch_california_housing()`ï¼šåˆ†åˆ«ç”¨äºåŠ è½½æ³¢å£«é¡¿æˆ¿ä»·æ•°æ®é›†å’ŒåŠ åˆ©ç¦å°¼äºšå·ä½æˆ¿ä»·æ ¼æ•°æ®é›†ï¼Œç”¨äºå›å½’é—®é¢˜çš„è®­ç»ƒå’Œæµ‹è¯•ã€‚
  - `load_digits()`å’Œ`fetch_openml('mnist_784')`ï¼šåˆ†åˆ«ç”¨äºåŠ è½½æ‰‹å†™æ•°å­—æ•°æ®é›†å’ŒMNISTæ•°å­—æ•°æ®é›†ï¼Œç”¨äºå›¾åƒåˆ†ç±»é—®é¢˜çš„è®­ç»ƒå’Œæµ‹è¯•ã€‚
  - `load_iris()`å’Œ`fetch_rcv1()`ï¼šåˆ†åˆ«ç”¨äºåŠ è½½é¸¢å°¾èŠ±æ•°æ®é›†å’ŒRCV1æ–°é—»æ–‡æœ¬åˆ†ç±»æ•°æ®é›†ï¼Œç”¨äºåˆ†ç±»é—®é¢˜çš„è®­ç»ƒå’Œæµ‹è¯•ã€‚

  éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œ`fetch`ç³»åˆ—å‡½æ•°éœ€è¦ä»ç½‘ç»œä¸Šä¸‹è½½æ•°æ®é›†ï¼Œå› æ­¤éœ€è¦ä¸€å®šçš„æ—¶é—´å’Œç½‘ç»œå¸¦å®½ã€‚åœ¨ä½¿ç”¨`fetch`ç³»åˆ—å‡½æ•°æ—¶ï¼Œåº”è¯¥æ³¨æ„æ•°æ®é›†çš„å¤§å°å’Œéœ€è¦çš„å¤„ç†æ­¥éª¤ï¼Œä»¥å…å½±å“ç¨‹åºçš„è¿è¡Œæ•ˆç‡ã€‚

### loaders@è‡ªå¸¦ç°æœ‰æ•°æ®é›†@å°æ•°æ®é›†

- [ Dataset loading utilities â€” scikit-learn  documentation](https://scikit-learn.org/stable/datasets.html)

  - [Toy datasets â€” scikit-learn  documentation](https://scikit-learn.org/stable/datasets/toy_dataset.html)

- ä»¥é¸¢å°¾èŠ±æ•°æ®é›†(åˆ†ç±»)ä¸ºä¾‹

  - [sklearn.datasets.load_iris â€” scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html)

  - ```python
    X,y=load_iris(return_X_y=True)
    #æ•ˆæœç­‰åŒäº
    data=load_iris()
    X=data.data
    y=data.target
    #ç”±äºBunchå¯¹è±¡çš„ç‰¹æ€§,å¯ä»¥ç”¨å­—å…¸æ–¹å¼è®¿é—®
    X=data['data']
    y=data['target']
    # 
    # å¯¼å‡ºä¸ºpandas dataframe:
    frame_data=load_iris(as_frame=True)
    X_df=frame_data.data
    y_df=frame_data.target
    
    ```

- è¿™ç±»æ–¹æ³•è¿”å›ä¸€ç§ç‰¹å®šçš„å¯¹è±¡:Bunch:[sklearn.utils.Bunch â€” scikit-learn  documentation](https://scikit-learn.org/stable/modules/generated/sklearn.utils.Bunch.html#sklearn.utils.Bunch)

### featcher

#### æˆ¿ä»·æ•°æ®é›†

- åŠ åˆ©ç¦å°¼äºšæˆ¿å±‹æ•°æ®é›†:[sklearn.datasets.fetch_california_housing â€” scikit-learn  documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html)
- æ³¢å£«é¡¿æ•°æ®é›†:è¯¥æ•°æ®é›†æ¶‰åŠé“å¾·é—®é¢˜,ä¸å†å»ºè®®ä½¿ç”¨,å¯ä»¥è€ƒè™‘ä½¿ç”¨California_housingä»£æ›¿

### Samples generator@ç”Ÿæˆæ•°æ®é›†

#### make_classification

- [sklearn.datasets.make_classification â€” scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html)

- Generate a random n-class classification problem.

  This initially creates clusters of points normally distributed (std=1) about vertices of an `n_informative`-dimensional hypercube with sides of length `2*class_sep` and assigns an equal number of clusters to each class. It introduces interdependence between these features and adds various types of further noise to the data.

  Without shuffling, `X` horizontally stacks features in the following order: the primary `n_informative` features, followed by `n_redundant` linear combinations of the informative features, followed by `n_repeated` duplicates, drawn randomly with replacement from the informative and redundant features. The remaining features are filled with random noise. Thus, without shuffling, all useful features are contained in the columns `X[:, :n_informative + n_redundant + n_repeated]`.

  é¦–å…ˆï¼Œåœ¨ä¸€ä¸ªn_informativeç»´çš„è¶…ç«‹æ–¹ä½“çš„é¡¶ç‚¹å‘¨å›´ç”Ÿæˆæ ‡å‡†å·®ä¸º1çš„é«˜æ–¯åˆ†å¸ƒæ•°æ®ç‚¹ç°‡ï¼Œå¹¶å°†ç›¸ç­‰æ•°é‡çš„ç°‡åˆ†é…ç»™æ¯ä¸ªç±»åˆ«ã€‚å®ƒåœ¨è¿™äº›ç‰¹å¾ä¹‹é—´å¼•å…¥ç›¸äº’ä¾èµ–æ€§ï¼Œå¹¶å‘æ•°æ®æ·»åŠ å„ç§ç±»å‹çš„å™ªå£°ã€‚

  å¦‚æœä¸è¿›è¡Œæ´—ç‰Œï¼Œåˆ™XæŒ‰ç…§ä»¥ä¸‹é¡ºåºæ°´å¹³å †å ç‰¹å¾ï¼šn_informativeä¸ªä¸»è¦ç‰¹å¾ï¼Œåé¢æ˜¯n_redundantä¸ªä¿¡æ¯ç‰¹å¾çš„çº¿æ€§ç»„åˆï¼Œç„¶åæ˜¯éšæœºä»ä¿¡æ¯å’Œå†—ä½™ç‰¹å¾ä¸­é‡å¤é€‰æ‹©çš„n_repeatedä¸ªå‰¯æœ¬ã€‚å…¶ä½™ç‰¹å¾å¡«å……éšæœºå™ªå£°ã€‚å› æ­¤ï¼Œå¦‚æœä¸è¿›è¡Œæ´—ç‰Œï¼Œåˆ™æ‰€æœ‰æœ‰ç”¨ç‰¹å¾éƒ½åŒ…å«åœ¨X [:ï¼Œ: n_informative + n_redundant + n_repeated]åˆ—ä¸­ã€‚

- `make_classification`å‡½æ•°å¯ä»¥ç”Ÿæˆå…·æœ‰ä»¥ä¸‹ç‰¹å¾çš„åˆ†ç±»æ•°æ®é›†ï¼š

  - æ ·æœ¬æ•°å’Œç‰¹å¾æ•°å¯ä»¥è‡ªå®šä¹‰ã€‚
  - å¯ä»¥æŒ‡å®šç±»åˆ«æ•°é‡ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨é»˜è®¤å€¼ï¼ˆå³2ï¼‰ã€‚
  - å¯ä»¥æŒ‡å®šç‰¹å¾ä¹‹é—´çš„å…³è”åº¦ï¼ˆå³ç‰¹å¾ç›¸å…³æ€§ï¼‰ã€‚
  - å¯ä»¥æŒ‡å®šåˆ†ç±»å™¨çš„éš¾åº¦ï¼Œå³æ ·æœ¬æ˜¯å¦å®¹æ˜“åˆ†ç¦»ã€‚
  - å¯ä»¥æŒ‡å®šç”¨äºç”Ÿæˆæ•°æ®çš„éšæœºç§å­ï¼Œä»¥ä¾¿å®ç°å¯é‡å¤æ€§ã€‚

- `make_classification`å‡½æ•°å¯ä»¥ç”Ÿæˆä¸€ä¸ªå…·æœ‰nä¸ªç±»åˆ«çš„éšæœºåˆ†ç±»æ•°æ®é›†ï¼Œå…¶ä¸­æ¯ä¸ªç±»åˆ«åŒ…å«ä¸€ç»„ä»¥n_informativeç»´ä¸ºä¸­å¿ƒçš„é«˜æ–¯åˆ†å¸ƒæ•°æ®ç‚¹ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œå‡½æ•°è¿˜å¯ä»¥æ·»åŠ å™ªå£°å’Œå†—ä½™ç‰¹å¾ä»¥åŠå…¶ä»–ç±»å‹çš„å™ªå£°ã€‚ä¸‹é¢ç»™å‡ºä¸€ä¸ªä½¿ç”¨`make_classification`å‡½æ•°ç”Ÿæˆåˆ†ç±»æ•°æ®é›†çš„ç¤ºä¾‹ä»£ç ï¼š

  ```python
  from sklearn.datasets import make_classification
  
  X, y = make_classification(n_samples=1000, n_features=20, n_classes=5, n_informative=10, n_redundant=5, n_repeated=2, class_sep=2.0, random_state=0)
  
  print("X shape:", X.shape)
  print("y shape:", y.shape)
  ```

  - ```bash
    X shape: (1000, 20)
    y shape: (1000,)
    ```

  - ä¸Šé¢çš„ä»£ç ç”Ÿæˆäº†ä¸€ä¸ªåŒ…å«1000ä¸ªæ ·æœ¬å’Œ20ä¸ªç‰¹å¾çš„åˆ†ç±»æ•°æ®é›†ï¼Œå…¶ä¸­æœ‰5ä¸ªç±»åˆ«ã€‚

    - æ¯ä¸ªç±»åˆ«åŒ…å«ä¸€ç»„ä»¥10ç»´ä¸ºä¸­å¿ƒçš„é«˜æ–¯åˆ†å¸ƒæ•°æ®ç‚¹ï¼ŒåŒæ—¶æ·»åŠ äº†5ä¸ªå†—ä½™ç‰¹å¾å’Œ2ä¸ªé‡å¤ç‰¹å¾ï¼Œå¹¶ä¸”ç±»ä¹‹é—´çš„è·ç¦»ä¸º2.0ã€‚
    - æœ€ç»ˆå¾—åˆ°çš„æ•°æ®é›†åŒ…å«20ä¸ªç‰¹å¾å’Œ1ä¸ªç±»åˆ«æ ‡ç­¾ã€‚

  - å¯ä»¥æ ¹æ®å…·ä½“æƒ…å†µï¼Œè°ƒæ•´`make_classification`å‡½æ•°çš„å„ä¸ªå‚æ•°æ¥ç”Ÿæˆä¸åŒçš„åˆ†ç±»æ•°æ®é›†ã€‚

    - ä¾‹å¦‚ï¼Œå¯ä»¥é€šè¿‡è°ƒæ•´`n_samples`å‚æ•°æ¥æ§åˆ¶æ•°æ®é›†çš„è§„æ¨¡ï¼Œé€šè¿‡è°ƒæ•´`n_informative`å‚æ•°æ¥æ§åˆ¶æœ‰ç”¨ç‰¹å¾çš„æ•°é‡ï¼Œé€šè¿‡è°ƒæ•´`class_sep`å‚æ•°æ¥æ§åˆ¶ç±»ä¹‹é—´çš„è·ç¦»ç­‰ç­‰ã€‚

#### make_regressionğŸˆ

- [sklearn.datasets.make_regression â€” scikit-learn  documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html)

  - Generate a random regression problem.

    The input set can either be well conditioned (by default) or have a low rank-fat tail singular profile. See make_low_rank_matrix for more details.

    The output is generated by applying a (potentially biased) random linear regression model with n_informative nonzero regressors to the previously generated input and some gaussian centered noise with some adjustable scale.

    Read more in the User Guide.

  - [sklearn.datasets.make_regression-scikit-learnä¸­æ–‡ç¤¾åŒº](https://scikit-learn.org.cn/view/595.html)

    è¾“å…¥é›†å¯ä»¥æ˜¯è‰¯å¥½æ¡ä»¶çš„ï¼ˆé»˜è®¤æƒ…å†µä¸‹ï¼‰æˆ–å…·æœ‰ä½ç§©-çŸ­å°¾å¥‡å¼‚è½®å»“ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§make_low_rank_matrixã€‚

    è¾“å‡ºæ˜¯é€šè¿‡å°†ä¸€ä¸ªï¼ˆå¯èƒ½æœ‰åå·®çš„ï¼‰éšæœºçº¿æ€§å›å½’æ¨¡å‹åº”ç”¨äºå…ˆå‰ç”Ÿæˆçš„è¾“å…¥å’Œä¸€äº›å…·æœ‰å¯è°ƒèŠ‚å°ºåº¦çš„é«˜æ–¯å™ªå£°çš„n_informativeéé›¶å›å½’å™¨æ¥ç”Ÿæˆçš„ã€‚

  - `make_regression`æ˜¯Scikit-learnä¸­çš„ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºç”Ÿæˆä¸€ä¸ªéšæœºå›å½’æ•°æ®é›†ã€‚å®ƒçš„è¯­æ³•å¦‚ä¸‹ï¼š

    ```python
    make_regression(n_samples=100, n_features=100, n_informative=10,
                    n_targets=1, bias=0.0, noise=0.0, shuffle=True, 
                    coef=False, random_state=None)
    ```

    å‚æ•°è¯´æ˜ï¼š

    - n_samplesï¼šç”Ÿæˆçš„æ ·æœ¬æ•°ï¼Œé»˜è®¤ä¸º100ã€‚
    - n_featuresï¼šç”Ÿæˆçš„ç‰¹å¾æ•°ï¼Œé»˜è®¤ä¸º100ã€‚
    - n_informativeï¼šç”Ÿæˆçš„æœ‰æ•ˆç‰¹å¾æ•°ï¼Œé»˜è®¤ä¸º10ã€‚è¿™äº›æœ‰æ•ˆç‰¹å¾ä¼šå¯¹ç›®æ ‡å˜é‡æœ‰è´¡çŒ®ã€‚
    - n_targetsï¼šç”Ÿæˆçš„ç›®æ ‡å˜é‡æ•°ï¼Œé»˜è®¤ä¸º1ã€‚
    - biasï¼šåç½®é¡¹ï¼Œé»˜è®¤ä¸º0.0ã€‚
    - noiseï¼šå™ªå£°é¡¹ï¼Œé»˜è®¤ä¸º0.0ã€‚åœ¨ç›®æ ‡å˜é‡ä¸ŠåŠ ä¸Šä¸€äº›å™ªå£°æ¥ä½¿æ•°æ®æ›´çœŸå®ã€‚
    - shuffleï¼šæ˜¯å¦æ‰“ä¹±æ ·æœ¬ï¼Œé»˜è®¤ä¸ºTrueã€‚
    - coefï¼šæ˜¯å¦è¿”å›ç”¨äºç”Ÿæˆæ•°æ®é›†çš„ç³»æ•°ï¼Œé»˜è®¤ä¸ºFalseã€‚
    - random_stateï¼šéšæœºæ•°ç§å­ã€‚

    `make_regression`å‡½æ•°è¿”å›ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«ç”Ÿæˆçš„æ•°æ®é›†å’ŒçœŸå®çš„ç›®æ ‡å˜é‡ã€‚å…¶ä¸­ï¼Œæ•°æ®é›†çš„å½¢çŠ¶ä¸º(n_samples, n_features)ï¼Œç›®æ ‡å˜é‡çš„å½¢çŠ¶ä¸º(n_samples, n_targets)ã€‚

  - make_regressionå‡½æ•°ç”Ÿæˆçš„æ•°æ®å¯ä»¥ç†è§£ä¸ºä¸€ä¸ªå¸¦æœ‰ä¸€å®šè§„å¾‹çš„æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«äº†è¾“å…¥ç‰¹å¾å’Œç›®æ ‡å€¼ã€‚

    è¾“å…¥ç‰¹å¾æ˜¯ä¸€ä¸ªäºŒç»´æ•°ç»„ï¼Œæ¯è¡Œè¡¨ç¤ºä¸€ä¸ªæ ·æœ¬ï¼Œæ¯åˆ—è¡¨ç¤ºä¸€ä¸ªç‰¹å¾ã€‚æˆ‘ä»¬å¯ä»¥å°†å…¶ç†è§£ä¸ºæ•°æ®é›†ä¸­çš„å±æ€§ï¼Œä¾‹å¦‚åœ¨æˆ¿ä»·é¢„æµ‹ä¸­ï¼Œç‰¹å¾å¯ä»¥æ˜¯æˆ¿å±‹çš„é¢ç§¯ã€å§å®¤æ•°é‡ç­‰ç­‰ã€‚

    ç›®æ ‡å€¼æ˜¯ä¸€ä¸ªä¸€ç»´æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ è¡¨ç¤ºå¯¹åº”æ ·æœ¬çš„çœŸå®å€¼ã€‚æˆ‘ä»¬å¯ä»¥å°†å…¶ç†è§£ä¸ºæ•°æ®é›†ä¸­çš„æ ‡ç­¾æˆ–è¾“å‡ºï¼Œä¾‹å¦‚åœ¨æˆ¿ä»·é¢„æµ‹ä¸­ï¼Œç›®æ ‡å€¼å¯ä»¥æ˜¯è¯¥æˆ¿å±‹çš„çœŸå®å”®ä»·ã€‚

    make_regressionå‡½æ•°ç”Ÿæˆçš„æ•°æ®é›†å…·æœ‰ä¸€å®šçš„è§„å¾‹ï¼Œå¯ä»¥æ˜¯çº¿æ€§å…³ç³»æˆ–éçº¿æ€§å…³ç³»ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ç”Ÿæˆä¸€ä¸ªçº¿æ€§å…³ç³»çš„æ•°æ®é›†ï¼Œå…¶ä¸­è¾“å…¥ç‰¹å¾å’Œç›®æ ‡å€¼ä¹‹é—´å­˜åœ¨çº¿æ€§å…³ç³»ï¼Œå¯ä»¥ç”¨çº¿æ€§å›å½’ç®—æ³•æ¥é¢„æµ‹ç›®æ ‡å€¼ï¼›æˆ–è€…ç”Ÿæˆä¸€ä¸ªéçº¿æ€§å…³ç³»çš„æ•°æ®é›†ï¼Œå…¶ä¸­è¾“å…¥ç‰¹å¾å’Œç›®æ ‡å€¼ä¹‹é—´å­˜åœ¨å¤æ‚çš„éçº¿æ€§å…³ç³»ï¼Œéœ€è¦ç”¨æ›´å¤æ‚çš„ç®—æ³•æ¥é¢„æµ‹ç›®æ ‡å€¼ã€‚

    é€šè¿‡ç”Ÿæˆä¸åŒè§„å¾‹çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬å¯ä»¥æµ‹è¯•å’Œè¯„ä¼°ä¸åŒçš„å›å½’ç®—æ³•åœ¨ä¸åŒæƒ…å†µä¸‹çš„æ€§èƒ½å’Œæ•ˆæœï¼Œä»è€Œé€‰æ‹©æœ€é€‚åˆå®é™…åº”ç”¨çš„ç®—æ³•ã€‚

##### eg

- ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨`make_regression`ç”Ÿæˆéšæœºå›å½’æ•°æ®é›†çš„ä¾‹å­ï¼š

  ```python
  from sklearn.datasets import make_regression
  
  # generate a random regression dataset
  X, y = make_regression(n_samples=1000, n_features=5, n_informative=2, noise=0.5, random_state=42)
  
  # print the shape of the dataset and target variable
  print("Shape of dataset:", X.shape)
  print("Shape of target variable:", y.shape)
  ```

- åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨`make_regression`å‡½æ•°ç”Ÿæˆä¸€ä¸ªåŒ…å«1000ä¸ªæ ·æœ¬ã€5ä¸ªç‰¹å¾å’Œ2ä¸ªæœ‰æ•ˆç‰¹å¾çš„éšæœºå›å½’æ•°æ®é›†ã€‚

- æˆ‘ä»¬è¿˜åœ¨ç›®æ ‡å˜é‡ä¸Šæ·»åŠ äº†ä¸€äº›å™ªå£°ã€‚æœ€åï¼Œæˆ‘ä»¬æ‰“å°å‡ºæ•°æ®é›†å’Œç›®æ ‡å˜é‡çš„å½¢çŠ¶ã€‚

- éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œç”±äº`make_regression`æ˜¯ä¸€ä¸ªéšæœºå‡½æ•°ï¼Œæ¯æ¬¡ç”Ÿæˆçš„æ•°æ®é›†éƒ½æ˜¯ä¸åŒçš„ã€‚æ­¤å¤–ï¼Œå¯ä»¥é€šè¿‡è°ƒæ•´å‚æ•°æ¥æ§åˆ¶ç”Ÿæˆæ•°æ®é›†çš„ç‰¹å¾å’Œç›®æ ‡å˜é‡çš„å±æ€§ï¼Œä»¥åŒ¹é…å…·ä½“é—®é¢˜çš„éœ€æ±‚ã€‚

#### visualization

- å½“ç”Ÿæˆçš„æ•°æ®é›†çš„ç‰¹å¾ç»´åº¦è¾ƒä½ï¼ˆå¦‚2ç»´æˆ–3ç»´ï¼‰æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¯è§†åŒ–å·¥å…·æ¥ç›´è§‚åœ°å±•ç¤ºè¾“å‡ºç»“æœã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼Œä½¿ç”¨matplotlibåº“æ¥å¯è§†åŒ–è¾“å‡ºç»“æœï¼š

  ```python
  import numpy as np
  import matplotlib.pyplot as plt
  from sklearn.datasets import make_regression
  
  # ç”Ÿæˆæ•°æ®é›†
  X, y = make_regression(n_samples=100, n_features=1, noise=10)
  
  # ç»˜åˆ¶æ•£ç‚¹å›¾
  plt.scatter(X, y)
  
  # ç»˜åˆ¶å›å½’ç›´çº¿
  x_min, x_max = plt.xlim()
  coef, bias = np.polyfit(X.ravel(), y.ravel(), 1)
  plt.plot([x_min, x_max], [x_min * coef + bias, x_max * coef + bias], 'r')
  
  # æ·»åŠ æ ‡é¢˜å’Œæ ‡ç­¾
  plt.title("Random Regression Problem")
  plt.xlabel("Input Feature")
  plt.ylabel("Target Value")
  
  # æ˜¾ç¤ºå›¾åƒ
  plt.show()
  ```

- è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†ä¸€ä¸ªç®€å•çš„ä¸€ç»´å›å½’é—®é¢˜ï¼Œç„¶åä½¿ç”¨æ•£ç‚¹å›¾å±•ç¤ºäº†è¾“å…¥ç‰¹å¾å’Œç›®æ ‡å€¼ä¹‹é—´çš„å…³ç³»ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨polyfitå‡½æ•°æ‹Ÿåˆäº†ä¸€æ¡å›å½’ç›´çº¿ï¼Œå¹¶å°†å…¶ç»˜åˆ¶åœ¨æ•£ç‚¹å›¾ä¸Šï¼Œä»¥ä¾¿æ›´å¥½åœ°å±•ç¤ºå›å½’é—®é¢˜çš„è§„å¾‹ã€‚

  - Least squares polynomial fit.æœ€å°äºŒä¹˜å¤šé¡¹å¼æ‹Ÿåˆ:numpy.polyfit â€” NumPy v1.24 Manual](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html)

  - `np.polyfit(x, y, deg)`æ˜¯numpyåº“ä¸­çš„ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºå¤šé¡¹å¼æ‹Ÿåˆï¼Œå³å°†ä¸€ç»„æ•°æ®ç‚¹æ‹Ÿåˆæˆä¸€ä¸ªå¤šé¡¹å¼å‡½æ•°ã€‚å®ƒçš„è¾“å…¥å‚æ•°åŒ…æ‹¬ï¼š

    - xï¼šè¦æ‹Ÿåˆçš„æ•°æ®ç‚¹çš„xåæ ‡æ•°ç»„ã€‚
    - yï¼šè¦æ‹Ÿåˆçš„æ•°æ®ç‚¹çš„yåæ ‡æ•°ç»„ã€‚
    - degï¼šæ‹Ÿåˆçš„å¤šé¡¹å¼æ¬¡æ•°ã€‚åœ¨â€œDegree of the fitting polynomialâ€ä¸­ï¼Œâ€œdegreeâ€å¯ä»¥ç¿»è¯‘ä¸ºâ€œæ¬¡æ•°â€æˆ–â€œé˜¶æ•°â€ï¼Œè¡¨ç¤ºæ‹Ÿåˆå¤šé¡¹å¼çš„æ¬¡æ•°æˆ–é˜¶æ•°ã€‚å› æ­¤ï¼Œå¯ä»¥å°†è¿™ä¸ªçŸ­è¯­ç¿»è¯‘ä¸ºâ€œæ‹Ÿåˆå¤šé¡¹å¼çš„æ¬¡æ•°â€ã€‚æœ‰æ—¶ç”¨orderæ¥è¡¨ç¤ºé˜¶æ•°

  - è¯¥å‡½æ•°è¿”å›çš„æ˜¯ä¸€ä¸ªä¸€ç»´æ•°ç»„ï¼ŒåŒ…å«äº†å¤šé¡¹å¼å‡½æ•°çš„ç³»æ•°ï¼Œä»é«˜æ¬¡åˆ°ä½æ¬¡æ’åˆ—ã€‚

  - ä¾‹å¦‚ï¼Œå¯¹äºä¸€ä¸ªäºŒæ¬¡å¤šé¡¹å¼æ‹Ÿåˆï¼Œè¿”å›çš„ç³»æ•°æ•°ç»„ä¸º[a, b, c]ï¼Œè¡¨ç¤ºæ‹Ÿåˆçš„å‡½æ•°ä¸º$y=ax^2+bx+c$ã€‚

  - ä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼Œä½¿ç”¨np.polyfitå‡½æ•°æ‹Ÿåˆä¸€æ¡å›å½’ç›´çº¿ï¼š

    ```python
    import numpy as np
    import matplotlib.pyplot as plt
    
    # ç”Ÿæˆæ•°æ®
    x = np.linspace(-10, 10, 100)
    y = 3 * x + 10 + np.random.randn(100) * 5
    
    # è®¡ç®—æ‹Ÿåˆç›´çº¿çš„ç³»æ•°
    # ç”±äºæ˜¯æ‹Ÿåˆçº¿æ€§å‡½æ•°,è¿”å›çš„ç»“æœå½¢å¦‚array([3.01458133, 9.44181608])çš„æ•°ç»„y=coef*x+bias
    coef, bias = np.polyfit(x, y, 1)
    
    # ç»˜åˆ¶æ•£ç‚¹å›¾å’Œæ‹Ÿåˆç›´çº¿
    plt.scatter(x, y)
    
    y=coef * x + bias
    plt.plot(x, y, color='red')
    
    # æ·»åŠ æ ‡é¢˜å’Œæ ‡ç­¾
    plt.title("Linear Regression Example")
    plt.xlabel("Input Feature")
    plt.ylabel("Target Value")
    
    # æ˜¾ç¤ºå›¾åƒ
    plt.show()
    ```

    è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨np.polyfitå‡½æ•°æ‹Ÿåˆäº†ä¸€æ¡å›å½’ç›´çº¿ï¼Œå°†ä¸€ç»„éšæœºç”Ÿæˆçš„æ•°æ®ç‚¹æ‹Ÿåˆæˆäº†ä¸€ä¸ªä¸€æ¬¡å‡½æ•°ã€‚æˆ‘ä»¬ä½¿ç”¨Matplotlibåº“ç»˜åˆ¶äº†æ•£ç‚¹å›¾å’Œæ‹Ÿåˆç›´çº¿ï¼Œå¹¶æ·»åŠ äº†æ ‡é¢˜å’Œæ ‡ç­¾ï¼Œä½¿å›¾è¡¨æ›´åŠ æ¸…æ™°å’Œæ˜“äºç†è§£ã€‚

- å½“ç‰¹å¾ç»´åº¦è¾ƒé«˜æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨é™ç»´æŠ€æœ¯ï¼ˆå¦‚PCAï¼‰å°†æ•°æ®é›†é™åˆ°2ç»´æˆ–3ç»´ï¼Œç„¶åä½¿ç”¨å¯è§†åŒ–å·¥å…·æ¥å±•ç¤ºè¾“å‡ºç»“æœã€‚

- å¦å¤–ï¼Œè¿˜å¯ä»¥ä½¿ç”¨ä¸€äº›é«˜çº§çš„å¯è§†åŒ–å·¥å…·ï¼ˆå¦‚[seaborn](https://seaborn.pydata.org/)åº“ï¼‰ï¼Œæ¥æ›´å¥½åœ°å±•ç¤ºå¤šç»´æ•°æ®é›†çš„ç‰¹å¾å’Œè§„å¾‹ã€‚



# model_selectionğŸˆğŸ˜

- [model_selection](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection)

  - **User guide:** See the [Cross-validation: evaluating estimator performance](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation), [Tuning the hyper-parameters of an estimator](https://scikit-learn.org/stable/modules/grid_search.html#grid-search) and [Learning curve](https://scikit-learn.org/stable/modules/learning_curve.html#learning-curve) sections for further details.

- Scikit-learnåº“ä¸­çš„`sklearn.model_selection`æ¨¡å—æä¾›äº†ä¸€ç³»åˆ—ç”¨äº**æ¨¡å‹é€‰æ‹©å’Œè¯„ä¼°**çš„å·¥å…·ã€‚è¯¥æ¨¡å—ä¸­åŒ…å«çš„ç±»å’Œå‡½æ•°å¯ä»¥å¸®åŠ©æˆ‘ä»¬è¿›è¡Œ**æ•°æ®é›†çš„åˆ’åˆ†ã€äº¤å‰éªŒè¯ã€è¶…å‚æ•°ä¼˜åŒ–ç­‰æ“ä½œ**ï¼Œä»è€Œé€‰æ‹©æœ€ä¼˜çš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚

- è¯¥æ¨¡å—ä¸­ä¸€äº›å¸¸ç”¨çš„ç±»å’Œå‡½æ•°å¦‚ä¸‹ï¼š

  - `train_test_split`: ç”¨äºå°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚	
  - `KFold`: ç”¨äºè¿›è¡ŒKæŠ˜äº¤å‰éªŒè¯ã€‚
  - `StratifiedKFold`: ç”¨äºè¿›è¡Œåˆ†å±‚KæŠ˜äº¤å‰éªŒè¯ï¼Œå¯ä»¥å¤„ç†ä¸å¹³è¡¡æ•°æ®é›†ã€‚
  - `GridSearchCV`: ç”¨äºè¿›è¡Œç½‘æ ¼æœç´¢å’Œäº¤å‰éªŒè¯ï¼Œå¯»æ‰¾æœ€ä¼˜çš„è¶…å‚æ•°ç»„åˆã€‚
  - `RandomizedSearchCV`: ç”¨äºè¿›è¡Œéšæœºæœç´¢å’Œäº¤å‰éªŒè¯ï¼Œå¯»æ‰¾æœ€ä¼˜çš„è¶…å‚æ•°ç»„åˆã€‚
  - `cross_val_score`: ç”¨äºå¯¹æ¨¡å‹è¿›è¡Œäº¤å‰éªŒè¯å¹¶è®¡ç®—æ€§èƒ½æŒ‡æ ‡çš„å¹³å‡å€¼ã€‚ğŸˆ
  - `learning_curve`: ç”¨äºç»˜åˆ¶å­¦ä¹ æ›²çº¿ï¼Œå¸®åŠ©æˆ‘ä»¬åˆ¤æ–­æ¨¡å‹æ˜¯å¦æ¬ æ‹Ÿåˆæˆ–è¿‡æ‹Ÿåˆã€‚
  - `validation_curve`: ç”¨äºç»˜åˆ¶éªŒè¯æ›²çº¿ï¼Œå¸®åŠ©æˆ‘ä»¬é€‰æ‹©æœ€ä¼˜çš„è¶…å‚æ•°ã€‚

  è¿™äº›ç±»å’Œå‡½æ•°å¯ä»¥æ ¹æ®éœ€è¦è¿›è¡Œç»„åˆä½¿ç”¨ï¼Œä»¥å®Œæˆä¸åŒçš„æ¨¡å‹é€‰æ‹©å’Œè¯„ä¼°ä»»åŠ¡ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`train_test_split`å°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œç„¶åä½¿ç”¨`GridSearchCV`å¯»æ‰¾æœ€ä¼˜çš„è¶…å‚æ•°ç»„åˆï¼Œæœ€åä½¿ç”¨`cross_val_score`å¯¹æ¨¡å‹è¿›è¡Œäº¤å‰éªŒè¯å¹¶è®¡ç®—æ€§èƒ½æŒ‡æ ‡çš„å¹³å‡å€¼ã€‚

## äº¤å‰éªŒè¯ä¸æ€§èƒ½åº¦é‡

- äº¤å‰éªŒè¯æ˜¯ä¸€ç§å®éªŒæ–¹æ³•,åœ¨è¯„ä¼°**åˆ†ç±»å™¨**å’Œ**å›å½’å™¨**æ—¶,å‡å¯ä»¥ä½¿ç”¨äº¤å‰éªŒè¯æ³•æ¥åšå®éªŒ
- åªä¸è¿‡,è¯„ä¼°åˆ†ç±»å™¨çš„æ€§èƒ½åº¦é‡å’Œå›å½’å™¨çš„<u>æ€§èƒ½åº¦é‡æœ‰æ‰€ä¸åŒ</u>
- äº¤å‰éªŒè¯æ˜¯ä¸€ç§å¸¸è§çš„æ¨¡å‹è¯„ä¼°æŠ€æœ¯ï¼Œå®ƒå¯ä»¥å¸®åŠ©æˆ‘ä»¬æ£€æŸ¥æ¨¡å‹åœ¨æœªè§è¿‡çš„æ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œé¿å…è¿‡åº¦æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆã€‚
- åœ¨å›å½’ä»»åŠ¡ä¸­ï¼Œäº¤å‰éªŒè¯çš„å®ç°æ–¹å¼ä¸åˆ†ç±»ä»»åŠ¡ç±»ä¼¼ã€‚
- æˆ‘ä»¬å¯ä»¥ä½¿ç”¨KæŠ˜äº¤å‰éªŒè¯æˆ–å…¶ä»–äº¤å‰éªŒè¯ç­–ç•¥æ¥åˆ†å‰²æ•°æ®é›†å¹¶å¯¹æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚åœ¨æ¯ä¸ªæŠ˜å ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æ‰€æœ‰é™¤äº†è¯¥æŠ˜å ä¹‹å¤–çš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨è¯¥æŠ˜å ä¸Šè¿›è¡Œæµ‹è¯•ã€‚æœ€åï¼Œæˆ‘ä»¬å°†æ¨¡å‹åœ¨æ‰€æœ‰æŠ˜å ä¸Šçš„æµ‹è¯•ç»“æœè¿›è¡Œå¹³å‡ï¼Œå¾—å‡ºä¸€ä¸ªç»¼åˆçš„è¯„ä¼°æŒ‡æ ‡ï¼Œä¾‹å¦‚å‡æ–¹è¯¯å·®ï¼ˆMean Squared Errorï¼‰æˆ–Rå¹³æ–¹ï¼ˆR-squaredï¼‰ã€‚
- åœ¨scikit-learnä¸­ï¼Œå¯ä»¥ä½¿ç”¨`cross_val_score`æˆ–`cross_validate`å‡½æ•°æ¥è¿›è¡Œå›å½’ä»»åŠ¡çš„äº¤å‰éªŒè¯ï¼Œå…·ä½“å®ç°æ–¹å¼ä¸åˆ†ç±»ä»»åŠ¡ç±»ä¼¼ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨å›å½’ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä½¿ç”¨ä¸åŒçš„è¯„ä¼°æŒ‡æ ‡æ¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ï¼Œä¾‹å¦‚å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ã€å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰æˆ–Rå¹³æ–¹ç­‰ï¼Œå…·ä½“é€‰æ‹©å“ªä¸ªæŒ‡æ ‡å–å†³äºå…·ä½“çš„é—®é¢˜å’Œä¸šåŠ¡éœ€æ±‚ã€‚

## sklearnå†…ç½®çš„æ€§èƒ½åº¦é‡

- åœ¨çº¿æ–‡æ¡£:[ the-scoring-parameter-defining-model-evaluation-rules|Metrics and scoring: quantifying the quality of predictions â€” scikit-learn  documentation](https://scikit-learn.org/stable/modules/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules)

- æœ¬åœ°æŸ¥è¯¢

  ```python
  In [24]: import sklearn
      ...: sklearn.metrics.get_scorer_names()
  Out[24]:
  ['accuracy',
   'adjusted_mutual_info_score',
   'adjusted_rand_score',
   'average_precision',
   'balanced_accuracy',
   'completeness_score',
   'explained_variance',
   'f1',
   'f1_macro',
   'f1_micro',
   ...
  ]
  ```

  

## cross_validateğŸˆ

- [sklearn.model_selection.cross_validate â€” scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html)

  - Evaluate metric(s) by cross-validation and also record fit/score times.é€šè¿‡äº¤å‰éªŒè¯è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶è®°å½•æ‹Ÿåˆ/å¾—åˆ†æ—¶é—´ã€‚

### åŠŸèƒ½

- `cross_validate`æ˜¯Scikit-learnåº“ä¸­çš„ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºå¯¹ç»™å®šçš„æ¨¡å‹è¿›è¡Œäº¤å‰éªŒè¯ï¼Œå¹¶è¿”å›äº¤å‰éªŒè¯çš„ç»“æœã€‚

  å…·ä½“æ¥è¯´ï¼Œ`cross_validate`å‡½æ•°çš„åŠŸèƒ½å¦‚ä¸‹ï¼š

  1. å¯¹ç»™å®šçš„æ¨¡å‹è¿›è¡Œäº¤å‰éªŒè¯ï¼Œä»¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚
  2. æ ¹æ®æ•°æ®é›†çš„åˆ’åˆ†æ–¹å¼ï¼Œå°†æ•°æ®é›†åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼Œå¤šæ¬¡è¿›è¡Œè®­ç»ƒå’ŒéªŒè¯ï¼Œå¹¶è®¡ç®—æ¨¡å‹çš„æ€§èƒ½æŒ‡æ ‡ï¼Œ
     - å¦‚ç²¾åº¦ã€F1åˆ†æ•°ã€Rå¹³æ–¹ç­‰ã€‚å¯ç”¨çš„æŒ‡æ ‡å‚çœ‹è¿æ¥:[ Metrics and scoring: quantifying the quality of predictions â€” scikit-learn documentation](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)
  3. è¿”å›äº¤å‰éªŒè¯çš„ç»“æœï¼ŒåŒ…æ‹¬è®­<u>ç»ƒé›†å¾—åˆ†ã€éªŒè¯é›†å¾—åˆ†ã€æ‹Ÿåˆæ—¶é—´ã€é¢„æµ‹æ—¶é—´</u>ç­‰ã€‚

### å‚æ•°

- estimatorï¼šå®ç°äº†â€œfitâ€æ–¹æ³•çš„ä¼°ç®—å™¨å¯¹è±¡ï¼Œç”¨äºè®­ç»ƒæ¨¡å‹ã€‚
- Xï¼šå½¢çŠ¶ä¸ºï¼ˆn_samplesï¼Œn_featuresï¼‰çš„ç±»æ•°ç»„æ•°æ®ï¼Œè¡¨ç¤ºè¦æ‹Ÿåˆçš„æ•°æ®ã€‚å¯ä»¥æ˜¯ä¸€ä¸ªåˆ—è¡¨æˆ–ä¸€ä¸ªæ•°ç»„ã€‚
- yï¼šå½¢çŠ¶ä¸ºï¼ˆn_samplesï¼Œï¼‰æˆ–ï¼ˆn_samplesï¼Œn_outputsï¼‰çš„ç±»æ•°ç»„æ•°æ®ï¼Œé»˜è®¤å€¼ä¸ºNoneã€‚å¯¹äºæœ‰ç›‘ç£å­¦ä¹ é—®é¢˜ï¼Œè¡¨ç¤ºè¦é¢„æµ‹çš„ç›®æ ‡å˜é‡ã€‚
- groupsï¼šå½¢çŠ¶ä¸ºï¼ˆn_samplesï¼Œï¼‰çš„ç±»æ•°ç»„æ•°æ®ï¼Œé»˜è®¤å€¼ä¸ºNoneã€‚åœ¨å°†æ•°æ®é›†åˆ†æˆè®­ç»ƒ/æµ‹è¯•é›†æ—¶ä½¿ç”¨çš„æ ·æœ¬ç»„æ ‡ç­¾ã€‚åªæœ‰åœ¨ä¸â€œGroupâ€äº¤å‰éªŒè¯å®ä¾‹ï¼ˆä¾‹å¦‚GroupKFoldï¼‰ä¸€èµ·ä½¿ç”¨æ—¶æ‰ä¼šä½¿ç”¨ã€‚
- scoringï¼šå­—ç¬¦ä¸²ã€å¯è°ƒç”¨å¯¹è±¡ã€åˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸ï¼Œé»˜è®¤å€¼ä¸ºNoneã€‚ç”¨äºè¯„ä¼°äº¤å‰éªŒè¯æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½çš„ç­–ç•¥ã€‚
  - å¦‚æœscoringè¡¨ç¤ºå•ä¸ªå¾—åˆ†ï¼Œå¯ä»¥ä½¿ç”¨ï¼š
    - å•ä¸ªå­—ç¬¦ä¸²ï¼ˆå‚è§scoringå‚æ•°ï¼šå®šä¹‰æ¨¡å‹è¯„ä¼°è§„åˆ™ï¼‰ã€‚
    - è¿”å›å•ä¸ªå€¼çš„å¯è°ƒç”¨å¯¹è±¡ï¼ˆå‚è§ä»åº¦é‡å‡½æ•°å®šä¹‰æ‚¨çš„è¯„åˆ†ç­–ç•¥ï¼‰ã€‚
  - å¦‚æœscoringè¡¨ç¤ºå¤šä¸ªå¾—åˆ†ï¼Œå¯ä»¥ä½¿ç”¨ï¼š
    - ä¸€ç»„å”¯ä¸€å­—ç¬¦ä¸²çš„åˆ—è¡¨æˆ–å…ƒç»„ã€‚
    - è¿”å›å­—å…¸çš„å¯è°ƒç”¨å¯¹è±¡ï¼Œå…¶ä¸­é”®æ˜¯æŒ‡æ ‡åç§°ï¼Œå€¼æ˜¯æŒ‡æ ‡åˆ†æ•°ã€‚
    - å…·æœ‰æŒ‡æ ‡åç§°ä½œä¸ºé”®å’Œå¯è°ƒç”¨å¯¹è±¡ä½œä¸ºå€¼çš„å­—å…¸ã€‚
  - æœ‰å…³ç¤ºä¾‹ï¼Œè¯·å‚è§æŒ‡å®šå¤šä¸ªè¯„ä¼°æŒ‡æ ‡ã€‚
- cvï¼šæ•´æ•°ã€äº¤å‰éªŒè¯ç”Ÿæˆå™¨æˆ–å¯è¿­ä»£å¯¹è±¡ï¼Œé»˜è®¤å€¼ä¸ºNoneã€‚ç¡®å®šäº¤å‰éªŒè¯æ‹†åˆ†ç­–ç•¥ã€‚cvçš„å¯èƒ½è¾“å…¥æœ‰ï¼š
  - Noneï¼Œä½¿ç”¨é»˜è®¤çš„5æŠ˜äº¤å‰éªŒè¯ï¼›
  - æ•´æ•°ï¼ŒæŒ‡å®šï¼ˆåˆ†å±‚ï¼‰KFoldä¸­çš„æŠ˜æ•°ï¼›
  - CVåˆ†å‰²å™¨ï¼›
  - äº§ç”Ÿï¼ˆtrainï¼Œtestï¼‰æ‹†åˆ†çš„å¯è¿­ä»£å¯¹è±¡ï¼Œä½œä¸ºç´¢å¼•çš„æ•°ç»„ã€‚
  - å¯¹äºint / Noneè¾“å…¥ï¼Œå¦‚æœä¼°ç®—å™¨æ˜¯åˆ†ç±»å™¨å¹¶ä¸”yæ˜¯äºŒè¿›åˆ¶æˆ–å¤šç±»åˆ«ï¼Œåˆ™ä½¿ç”¨StratifiedKFoldã€‚åœ¨æ‰€æœ‰å…¶ä»–æƒ…å†µä¸‹ï¼Œä½¿ç”¨KFoldã€‚è¿™äº›æ‹†åˆ†å™¨æ˜¯ä½¿ç”¨shuffle = Falseå®ä¾‹åŒ–çš„ï¼Œå› æ­¤è·¨è°ƒç”¨æ‹†åˆ†å°†ç›¸åŒã€‚
  - æœ‰å…³å¯ä»¥åœ¨æ­¤å¤„ä½¿ç”¨çš„å„ç§äº¤å‰éªŒè¯ç­–ç•¥ï¼Œè¯·å‚è§ç”¨æˆ·æŒ‡å—ã€‚
  - ç‰ˆæœ¬0.22ä¸­çš„æ›´æ”¹ï¼šå¦‚æœä¸ºNoneï¼Œåˆ™å°†cvé»˜è®¤å€¼ä»3å€æ”¹ä¸º5å€ã€‚
- n_jobsï¼šæ•´æ•°ï¼Œé»˜è®¤å€¼ä¸ºNoneã€‚åœ¨å¹¶è¡Œè¿è¡Œæ—¶è¦è¿è¡Œçš„ä½œä¸šæ•°ã€‚ä¼°ç®—å™¨çš„è®­ç»ƒå’Œè®¡ç®—åˆ†æ•°åœ¨äº¤å‰éªŒè¯æ‹†åˆ†ä¸Šå¹¶è¡ŒåŒ–ã€‚Noneè¡¨ç¤º1ï¼Œé™¤éåœ¨joblib.parallel_backendä¸Šä¸‹æ–‡ä¸­ã€‚-1è¡¨ç¤ºä½¿ç”¨æ‰€æœ‰å¤„ç†å™¨ã€‚è¯·å‚è§æœ¯è¯­è¡¨ä»¥è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚
- verboseï¼šæ•´æ•°ï¼Œé»˜è®¤å€¼ä¸º0ã€‚è¯¦ç»†çº§åˆ«ã€‚
- fit_paramsï¼šå­—å…¸ï¼Œé»˜è®¤å€¼ä¸ºNoneã€‚è¦ä¼ é€’ç»™ä¼°ç®—å™¨çš„fitæ–¹æ³•çš„å‚æ•°ã€‚
- pre_dispatchï¼šæ•´æ•°æˆ–å­—ç¬¦ä¸²ï¼Œé»˜è®¤å€¼ä¸ºâ€œ2*n_jobsâ€ã€‚æ§åˆ¶å¹¶è¡Œæ‰§è¡ŒæœŸé—´åˆ†æ´¾çš„ä½œä¸šæ•°é‡ã€‚å½“åˆ†æ´¾çš„ä½œä¸šå¤šäºCPUå¯ä»¥å¤„ç†çš„ä½œä¸šæ—¶ï¼Œå‡å°‘æ­¤æ•°å­—å¯ä»¥é¿å…å†…å­˜æ¶ˆè€—çˆ†ç‚¸ã€‚è¯¥å‚æ•°å¯ä»¥æ˜¯ï¼š
  - Noneï¼Œæ­¤æ—¶å°†ç«‹å³åˆ›å»ºå’Œç”Ÿæˆæ‰€æœ‰ä½œä¸šã€‚å¯¹äºè½»é‡çº§å’Œå¿«é€Ÿè¿è¡Œçš„ä½œä¸šï¼Œè¯·ä½¿ç”¨æ­¤é€‰é¡¹ï¼Œä»¥é¿å…ç”±äºæŒ‰éœ€ç”Ÿæˆä½œä¸šè€Œå¯¼è‡´çš„å»¶è¿Ÿï¼›
  - ä¸€ä¸ªæ•´æ•°ï¼Œç»™å‡ºè¦ç”Ÿæˆçš„æ€»ä½œä¸šæ•°ï¼›
  - ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œç»™å‡ºn_jobsçš„ä¸€ä¸ªå‡½æ•°è¡¨è¾¾å¼ï¼Œä¾‹å¦‚â€œ2*n_jobsâ€ã€‚
- return_train_scoreï¼šå¸ƒå°”å€¼ï¼Œé»˜è®¤ä¸ºFalseã€‚æ˜¯å¦åŒ…æ‹¬è®­ç»ƒåˆ†æ•°ã€‚è®¡ç®—è®­ç»ƒåˆ†æ•°ç”¨äºäº†è§£ä¸åŒå‚æ•°è®¾ç½®å¦‚ä½•å½±å“è¿‡åº¦æ‹Ÿåˆ/æ¬ æ‹Ÿåˆçš„æƒè¡¡ã€‚ä½†æ˜¯ï¼Œåœ¨è®­ç»ƒé›†ä¸Šè®¡ç®—åˆ†æ•°å¯èƒ½è®¡ç®—é‡å¾ˆå¤§ï¼Œå¹¶ä¸”ä¸ä¸¥æ ¼éœ€è¦é€‰æ‹©äº§ç”Ÿæœ€ä½³æ³›åŒ–æ€§èƒ½çš„å‚æ•°ã€‚
  - ç‰ˆæœ¬0.19ä¸­çš„æ–°åŠŸèƒ½ã€‚
  - ç‰ˆæœ¬0.21ä¸­çš„æ›´æ”¹ï¼šé»˜è®¤å€¼ä»Trueæ›´æ”¹ä¸ºFalseã€‚
- return_estimatorï¼šå¸ƒå°”å€¼ï¼Œé»˜è®¤ä¸ºFalseã€‚æ˜¯å¦è¿”å›åœ¨æ¯ä¸ªæ‹†åˆ†ä¸Šæ‹Ÿåˆçš„ä¼°ç®—å™¨ã€‚
  - ç‰ˆæœ¬0.20ä¸­çš„æ–°åŠŸèƒ½ã€‚
- error_scoreï¼šâ€˜raiseâ€™æˆ–æ•°å­—ï¼Œé»˜è®¤å€¼ä¸ºnp.nanã€‚å¦‚æœåœ¨ä¼°ç®—å™¨æ‹Ÿåˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼Œåˆ™åˆ†æ•°åˆ†é…ç»™å€¼ã€‚å¦‚æœè®¾ç½®ä¸ºâ€œraiseâ€ï¼Œåˆ™ä¼šå¼•å‘é”™è¯¯ã€‚å¦‚æœç»™å‡ºæ•°å­—å€¼ï¼Œåˆ™ä¼šå¼•å‘FitFailedWarningã€‚
  - ç‰ˆæœ¬0.20ä¸­çš„æ–°åŠŸèƒ½ã€‚

### demos

- åŒ…æ‹¬ä¸€ä¸ªåˆ†ç±»ä»»åŠ¡æ•°æ®é›†å’Œå›å½’ä»»åŠ¡æ•°æ®é›†
  - iris(classification)
  - digits(classification)
  - diabetes(<u>regression</u>)

#### åˆ†ç±»ä»»åŠ¡eg

- ```python
  from sklearn import datasets, linear_model
  from sklearn.model_selection import cross_validate,train_test_split as tts
  from sklearn.metrics import make_scorer
  from sklearn.metrics import confusion_matrix
  from sklearn.svm import LinearSVC,SVC,SVR
  from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor
  iris=datasets.load_iris()
  digit=datasets.load_digits()
  db=digit
  
  size=int(len(db.data)*0.5)
  X = db.data[:size]
  y = db.target[:size]
  X_train,X_test,y_train,y_test=tts(X,y)
  # lasso = linear_model.Lasso()
  lsvc=LinearSVC()
  svc=SVC()
  dtc=DecisionTreeClassifier()
  #é€‰æ‹©ä¸€ä¸ªåˆ†ç±»å™¨
  clf=dtc
  
  #å®šä¹‰äº¤å‰éªŒè¯è§„åˆ™
  from sklearn.model_selection import KFold
  kfold=KFold(n_splits=3,shuffle=True,random_state=0)
  #é€šè¿‡scoringå‚æ•°æŒ‡å®šéœ€è¦è¯„ä¼°çš„æŒ‡æ ‡
  # scoring=['accuracy','precision_macro']
  # scoring='precision'
  #ä½¿ç”¨å­—å…¸æ–¹å¼æ¥è®¾ç½®
  scoring_clf = {
      'accuracy': 'accuracy',
      'precision_macro': 'precision_macro',
      'recall_macro': 'recall_macro',
      'f1_macro': 'f1_macro'
  }
  
  
  #è‡ªè¡Œåˆ¤æ–­æ˜¯å›å½’ä»»åŠ¡è¿˜æ˜¯åˆ†ç±»ä»»åŠ¡:
  
  cv_results = cross_validate(clf, X, y, cv=kfold,scoring=scoring_clf)
  cv_results
  ```

  - ```bash
    {'fit_time': array([0.01006103, 0.00803781, 0.00802016]),
     'score_time': array([0.00693703, 0.00400352, 0.00598001]),
     'test_accuracy': array([0.77666667, 0.81270903, 0.84615385]),
     'test_precision_macro': array([0.77240471, 0.80348608, 0.85173747]),
     'test_recall_macro': array([0.77000705, 0.80206389, 0.85180096]),
     'test_f1_macro': array([0.76830664, 0.7997368 , 0.85058593])}
    ```

  

#### å›å½’ä»»åŠ¡eg

- ```python
  from sklearn import datasets, linear_model
  from sklearn.model_selection import cross_validate,train_test_split as tts
  from sklearn.metrics import make_scorer
  from sklearn.metrics import confusion_matrix
  from sklearn.svm import LinearSVC,SVC,SVR
  from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor
  diabetes = datasets.load_diabetes()
  db=diabetes
  
  # è°ƒæ•´æ•°æ®é›†å¤§å°(è¿™é‡Œä»æ€»æ•°æ®é›†åˆ’åˆ†å‡ºæ¥çš„éƒ¨åˆ†ä½œä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†(åŒ…å«sizeä¸ªæ ·ä¾‹),
  # ä½†æ˜¯è¿™ä¸¤ä¸ªé›†åˆäº¤å‰éªŒè¯å™¨æ¥å¤„ç†,æˆ‘ä»¬æ— éœ€æ‰‹åŠ¨å¤„ç†,åªéœ€è¦ä¿ç•™å‡ºä¸€éƒ¨åˆ†ä½œä¸ºæµ‹è¯•é›†å³å¯)
  size=int(len(db.data)*0.5)
  X = db.data[:size]
  y = db.target[:size]
  #åˆ’åˆ†æ•°æ®é›†
  dtr=DecisionTreeRegressor()
  svr=SVR()
  #é€‰æ‹©ä¸€ä¸ªå›å½’å™¨
  clf=dtr
  #å®šä¹‰äº¤å‰éªŒè¯è§„åˆ™
  from sklearn.model_selection import KFold
  kfold=KFold(n_splits=3,shuffle=True,random_state=0)
  #é€šè¿‡scoringå‚æ•°æŒ‡å®šéœ€è¦è¯„ä¼°çš„æŒ‡æ ‡
  #ä½¿ç”¨åˆ—è¡¨çš„æ–¹å¼æ¥è®¾ç½®
  scoring_rgr=[
      'max_error','r2'
  ]
  
  cv_results = cross_validate(clf, X, y, cv=kfold,scoring=scoring_rgr)
  cv_results
  ```

  - ```bash
    {'fit_time': array([0.00199652, 0.00200272, 0.00100017]),
     'score_time': array([0.0010004 , 0.00100183, 0.00100183]),
     'test_max_error': array([-218., -258., -211.]),
     'test_r2': array([-0.300702  , -0.05502733, -0.20256748])}
    ```



#### eg

- `cross_validate` æ˜¯ sklearnï¼ˆScikit-learnï¼‰åº“ä¸­çš„ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚å®ƒé€šè¿‡äº¤å‰éªŒè¯çš„æ–¹æ³•å°†æ•°æ®é›†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œç„¶ååœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œå¹¶åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚è¿™ä¸ªè¿‡ç¨‹ä¼šé‡å¤å¤šæ¬¡ï¼Œä»¥è·å¾—æ›´ç¨³å®šçš„æ€§èƒ½è¯„ä¼°ã€‚
- `scoring` å‚æ•°æ˜¯ `cross_validate` å‡½æ•°çš„ä¸€ä¸ªé‡è¦å‚æ•°ï¼Œç”¨äºæŒ‡å®šè¯„ä¼°æ¨¡å‹æ€§èƒ½çš„æŒ‡æ ‡ã€‚ä½ å¯ä»¥ä¼ é€’ä¸€ä¸ªå­—ç¬¦ä¸²ã€ä¸€ä¸ªå‡½æ•°æˆ–ä¸€ä¸ªå­—å…¸ï¼Œä»¥ä¾¿ä¸ºæ¨¡å‹æŒ‡å®šä¸€ä¸ªæˆ–å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ã€‚Scikit-learn æä¾›äº†è®¸å¤šå†…ç½®çš„è¯„ä¼°æŒ‡æ ‡ï¼Œå¦‚å‡†ç¡®ç‡ã€ç²¾ç¡®åº¦ã€å¬å›ç‡ç­‰ã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨ `cross_validate` å’Œ `scoring` å‚æ•°çš„ Python ä»£ç ç¤ºä¾‹ï¼š

```python
from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_validate

# åŠ è½½æ•°æ®é›†
iris = load_iris()

# åˆ›å»ºæ¨¡å‹
model = LogisticRegression(solver='liblinear', multi_class='ovr')

# å®šä¹‰è¯„ä¼°æŒ‡æ ‡
scoring = {
    'accuracy': 'accuracy',
    'precision_macro': 'precision_macro',
    'recall_macro': 'recall_macro',
    'f1_macro': 'f1_macro'
}

# ä½¿ç”¨ cross_validate è¿›è¡Œäº¤å‰éªŒè¯
cv_results = cross_validate(model, iris.data, iris.target, cv=5, scoring=scoring)

# è¾“å‡ºç»“æœ
for metric, scores in cv_results.items():
    print(f"{metric}: {scores.mean():.2f} (+/- {scores.std() * 2:.2f})")
```

- åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†é¸¢å°¾èŠ±æ•°æ®é›†å’Œé€»è¾‘å›å½’æ¨¡å‹ã€‚æˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªå­—å…¸ `scoring`ï¼ŒåŒ…å«äº†å››ä¸ªè¯„ä¼°æŒ‡æ ‡ï¼šå‡†ç¡®ç‡ã€å®å¹³å‡ç²¾ç¡®åº¦ã€å®å¹³å‡å¬å›ç‡å’Œå®å¹³å‡ F1 åˆ†æ•°ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨ `cross_validate` å‡½æ•°è¿›è¡Œäº† 5 æŠ˜äº¤å‰éªŒè¯ï¼Œå¹¶è¾“å‡ºäº†æ¯ä¸ªè¯„ä¼°æŒ‡æ ‡çš„å¹³å‡å€¼å’Œæ ‡å‡†å·®ã€‚
- æ³¨æ„ï¼šåœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº† `solver='liblinear'` å’Œ `multi_class='ovr'` å‚æ•°æ¥åˆ›å»ºé€»è¾‘å›å½’æ¨¡å‹ï¼Œä»¥ä¾¿åœ¨å¤šåˆ†ç±»é—®é¢˜ä¸Šè·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚



## cross_val_scoreğŸˆ

- [sklearn.model_selection.cross_val_score â€” scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)

- `cross_val_score`æ˜¯Scikit-learnä¸­ç”¨äºæ‰§è¡ŒKæŠ˜äº¤å‰éªŒè¯çš„å‡½æ•°ä¹‹ä¸€ã€‚å®ƒå¯ä»¥å¸®åŠ©æˆ‘ä»¬è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒçš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šçš„æ€§èƒ½ï¼Œå¹¶è®¡ç®—æ‰€é€‰è¯„ä¼°æŒ‡æ ‡çš„å¹³å‡å€¼å’Œæ ‡å‡†å·®ã€‚
- ä½¿ç”¨`cross_val_score`å‡½æ•°éå¸¸ç®€å•ï¼Œåªéœ€è¦æä¾›è¦è¯„ä¼°çš„æ¨¡å‹ã€è¦ä½¿ç”¨çš„æ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡ä»¥åŠäº¤å‰éªŒè¯çš„æŠ˜æ•°å³å¯ã€‚

ä»¥ä¸‹æ˜¯`cross_val_score`å‡½æ•°çš„åŸºæœ¬è¯­æ³•ï¼š

```python
from sklearn.model_selection import cross_val_score

scores = cross_val_score(estimator, X, y, cv=k, scoring=None)
```

å…¶ä¸­ï¼Œå‚æ•°å«ä¹‰å¦‚ä¸‹ï¼š

- `estimator`: è¦è¯„ä¼°çš„æ¨¡å‹æˆ–ç®¡é“å¯¹è±¡ã€‚
- `X`: ç‰¹å¾æ•°æ®é›†ã€‚
- `y`: æ ‡ç­¾æ•°æ®é›†ã€‚
- `cv`: äº¤å‰éªŒè¯çš„æŠ˜æ•°ï¼Œé»˜è®¤ä¸º5ã€‚
- `scoring`: è¦ä½¿ç”¨çš„è¯„ä¼°æŒ‡æ ‡ï¼Œé»˜è®¤ä¸ºæ¨¡å‹çš„é»˜è®¤æŒ‡æ ‡ã€‚

- `cross_val_score`å‡½æ•°è¿”å›ä¸€ä¸ªåŒ…å«æ¯ä¸ªæŠ˜çš„è¯„ä¼°æŒ‡æ ‡å¾—åˆ†çš„æ•°ç»„ï¼Œå¯ä»¥é€šè¿‡è®¡ç®—å…¶å¹³å‡å€¼å’Œæ ‡å‡†å·®æ¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚å¦‚æœæ²¡æœ‰æŒ‡å®šè¯„ä¼°æŒ‡æ ‡ï¼Œåˆ™é»˜è®¤ä½¿ç”¨æ¨¡å‹çš„é»˜è®¤æŒ‡æ ‡ã€‚

#### å…³äºæ•°æ®éšæœºåŒ–

- `cross_val_score`å‡½æ•°åœ¨é»˜è®¤æƒ…å†µä¸‹ä¸æä¾›æ•°æ®éšæœºåŒ–åŠŸèƒ½ï¼ˆå³ä¸ä¼šæ‰“ä¹±æ•°æ®é¡ºåºï¼‰ï¼Œè¿™æ˜¯å› ä¸ºäº¤å‰éªŒè¯çš„ç›®çš„æ˜¯è¯„ä¼°æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œè€Œä¸æ˜¯å­¦ä¹ æ•°æ®é›†ä¸­çš„å…·ä½“ç‰¹å¾ã€‚
- å› æ­¤ï¼Œåœ¨äº¤å‰éªŒè¯è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿æ¯ä¸ªfoldä¸­çš„æ•°æ®æ ·æœ¬éƒ½æ˜¯éšæœºé€‰æ‹©çš„ï¼Œä»¥é¿å…è¿‡åº¦æ‹Ÿåˆæˆ–æ¬ æ‹Ÿåˆçš„æƒ…å†µå‘ç”Ÿã€‚
- `scikit-learn`åº“æä¾›äº†è®¸å¤šç”¨äºæ•°æ®éšæœºåŒ–çš„å‡½æ•°å’Œç±»ï¼Œæ¯”å¦‚`shuffle`ã€`StratifiedShuffleSplit`ç­‰ã€‚å¦‚æœéœ€è¦åœ¨äº¤å‰éªŒè¯è¿‡ç¨‹ä¸­è¿›è¡Œæ•°æ®éšæœºåŒ–ï¼Œå¯ä»¥ä½¿ç”¨è¿™äº›å‡½æ•°æˆ–ç±»æ¥å®ç°ã€‚

#### é»˜è®¤è¯„ä¼°æŒ‡æ ‡

- ä»¥ä¸‹æ˜¯ä¸€ä¸ªä½¿ç”¨`cross_val_score`å‡½æ•°è¯„ä¼°Logisticå›å½’æ¨¡å‹æ€§èƒ½çš„ç®€å•ç¤ºä¾‹ï¼š

- ```python
  In [27]: from sklearn.datasets import make_classification
      ...: from sklearn.linear_model import LogisticRegression
      ...: from sklearn.model_selection import cross_val_score
      ...:
      ...: X, y = make_classification(n_samples=1000, random_state=0)
      ...:
      ...: lr = LogisticRegression()
      ...: scores = cross_val_score(lr, X, y, cv=5)
      ...:
      ...: print(f'{scores=}')
      ...: print("Accuracy: {:.2f} (+/- {:.2f})".format(scores.mean(), scores.std()))
  scores=array([0.93 , 0.94 , 0.935, 0.955, 0.945])
  Accuracy: 0.94 (+/- 0.01)
  In [28]: scores?
  Type:        ndarray
  String form: [0.93  0.94  0.935 0.955 0.945]
  Length:      5
  ```

  - åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨`make_classification`å‡½æ•°ç”Ÿæˆä¸€ä¸ªäºŒåˆ†ç±»æ•°æ®é›†ï¼Œç„¶åä½¿ç”¨`LogisticRegression`ä½œä¸ºè¯„ä¼°æ¨¡å‹ã€‚

  - æˆ‘ä»¬é€šè¿‡ä¼ é€’æ¨¡å‹ã€ç‰¹å¾æ•°æ®é›†å’Œæ ‡ç­¾æ•°æ®é›†ä»¥åŠ5æŠ˜äº¤å‰éªŒè¯æ¥è°ƒç”¨`cross_val_score`å‡½æ•°ã€‚æœ€åï¼Œæˆ‘ä»¬è®¡ç®—è¯„ä¼°æŒ‡æ ‡å¾—åˆ†çš„å¹³å‡å€¼å’Œæ ‡å‡†å·®ï¼Œå¹¶å°†å…¶æ‰“å°å‡ºæ¥ã€‚

  - æ³¨æ„scoresæ˜¯ä¸€ä¸ªnumpyæ•°ç»„,å¯ä»¥åˆ©ç”¨numpyæ•°ç»„çš„æ–¹æ³•è®¡ç®—æ•°ç»„çš„å¹³å‡å€¼æˆ–æ–¹å·®

    - ```python
      scores.mean()
      scores.std()
      ```

  - `cross_val_score`ä¹Ÿæ”¯æŒscoringæŒ‡å®š,ä¸è¿‡å’Œ`cross_validate`ä¸åŒçš„æ˜¯,è¿™é‡Œçš„scoringä»…æ¥æ”¶ä¸€ä¸ªæŒ‡æ ‡è€Œä¸æ”¯æŒå¤šæŒ‡æ ‡è¯„ä¼°

    ```bash
    scoring : str or callable, default=None
        A str (see model evaluation documentation) or
        a scorer callable object / function with signature
        ``scorer(estimator, X, y)`` which should return only
        a single value.
    
        Similar to :func:`cross_validate`
        but only a single metric is permitted.
    
        If `None`, the estimator's default scorer (if available) is used.
    ```

    

#### egäº¤å‰éªŒè¯è¯„ä¼°å›å¬ç‡æŒ‡æ ‡

- ```python
  In [25]: from sklearn import svm, datasets
      ...: from sklearn.model_selection import cross_val_score
      ...: X, y = datasets.load_iris(return_X_y=True)
      ...: clf = svm.SVC(random_state=0)
      ...: cross_val_score(clf, X, y, cv=5, scoring='recall_macro')
      ...:
  Out[25]: array([0.96666667, 0.96666667, 0.96666667, 0.93333333, 1.        ])
  
  In [26]:
      ...: model = svm.SVC()
      ...: #é”™è¯¯(æˆ–ä¸å¯ç”¨çš„)æŒ‡æ ‡ä¼šæŠ›å‡ºé”™è¯¯:
      ...: cross_val_score(model, X, y, cv=5, scoring='wrong_choice')
      ...:
  ---------------------------------------------------------------------------
  KeyError                                  Traceback (most recent call last)
  ...
  ValueError: 'wrong_choice' is not a valid scoring value. Use sklearn.metrics.get_scorer_names() to get valid options.
  ```

  

## cross_val_predictğŸˆ

- [sklearn.model_selection.cross_val_predict â€” scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html)

  - Generate cross-validated **estimates** for each input data point.

  - The data is split according to the cv(äº¤å‰éªŒè¯å™¨) parameter. Each sample belongs to exactly one test set, and its prediction is computed with an estimator fitted on the corresponding training set.

  - Passing these predictions into an **evaluation metric** may not be a valid way to measure generalization performance. 

  - Results can differ from [`cross_validate`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate) and [`cross_val_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score) <u>unless **all tests sets** have equal size and the metric **decomposes** over samples.</u>

  - ä¸ºæ¯ä¸ªè¾“å…¥æ•°æ®ç‚¹ç”Ÿæˆäº¤å‰éªŒè¯ä¼°è®¡ã€‚

    æ•°æ®æ ¹æ®cvå‚æ•°è¿›è¡Œåˆ’åˆ†ã€‚æ¯ä¸ªæ ·æœ¬æ°å¥½å±äºä¸€ä¸ªæµ‹è¯•é›†ï¼Œå…¶é¢„æµ‹æ˜¯ç”¨ç›¸åº”è®­ç»ƒé›†æ‹Ÿåˆçš„ä¼°è®¡å™¨è®¡ç®—çš„ã€‚

    å°†è¿™äº›é¢„æµ‹ä¼ é€’ç»™è¯„ä¼°æŒ‡æ ‡å¯èƒ½ä¸æ˜¯è¡¡é‡æ³›åŒ–æ€§èƒ½çš„æœ‰æ•ˆæ–¹æ³•ã€‚é™¤éæ‰€æœ‰æµ‹è¯•é›†çš„å¤§å°ç›¸ç­‰ä¸”æŒ‡æ ‡åœ¨æ ·æœ¬ä¸Šåˆ†è§£ï¼Œå¦åˆ™ç»“æœå¯èƒ½ä¸cross_validateå’Œcross_val_scoreä¸åŒã€‚

- `sklearn.model_selection.cross_val_predict`å‡½æ•°æ˜¯`scikit-learn`ä¸­çš„ä¸€ä¸ªäº¤å‰éªŒè¯å‡½æ•°ï¼Œå®ƒå¯ä»¥ç”¨äºå¯¹æ¨¡å‹è¿›è¡Œäº¤å‰éªŒè¯ï¼Œå¹¶è¿”å›äº¤å‰éªŒè¯çš„é¢„æµ‹ç»“æœã€‚

- ä¸`cross_val_score`å‡½æ•°ä¸åŒçš„æ˜¯ï¼Œ`cross_val_predict`å‡½æ•°ä¸è¿”å›æ¯æ¬¡äº¤å‰éªŒè¯çš„å¾—åˆ†ï¼Œè€Œæ˜¯è¿”å›æ¯æ¬¡äº¤å‰éªŒè¯çš„é¢„æµ‹ç»“æœã€‚è¿™äº›é¢„æµ‹ç»“æœå¯ä»¥ç”¨äºè®¡ç®—æ¨¡å‹çš„æ€§èƒ½æŒ‡æ ‡ï¼Œæ¯”å¦‚å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1å€¼ç­‰ã€‚

### demo

- ```python
  from sklearn import datasets
  from sklearn.model_selection import cross_val_predict
  from sklearn.linear_model import LogisticRegression
  from sklearn.metrics import classification_report
  
  # åŠ è½½é¸¢å°¾èŠ±æ•°æ®é›†
  iris = datasets.load_iris()
  X = iris.data
  y = iris.target
  
  # åˆ›å»ºé€»è¾‘å›å½’æ¨¡å‹
  lr = LogisticRegression(max_iter=1000)
  
  # ä½¿ç”¨cross_val_predictç”Ÿæˆäº¤å‰éªŒè¯é¢„æµ‹
  predictions = cross_val_predict(lr, X, y, cv=5)
  
  # è¾“å‡ºåˆ†ç±»æŠ¥å‘Š
  print(classification_report(y, predictions))
  
  ```

  - åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†é¸¢å°¾èŠ±æ•°æ®é›†å’Œé€»è¾‘å›å½’æ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨cross_val_predictå‡½æ•°ç”Ÿæˆäº¤å‰éªŒè¯é¢„æµ‹ï¼Œå…¶ä¸­cvå‚æ•°è®¾ç½®ä¸º5ï¼Œè¡¨ç¤ºä½¿ç”¨5æŠ˜äº¤å‰éªŒè¯ã€‚ç„¶åï¼Œæˆ‘ä»¬è¾“å‡ºäº†ä¸€ä¸ªåˆ†ç±»æŠ¥å‘Šï¼Œå±•ç¤ºäº†æ¨¡å‹åœ¨æ¯ä¸ªç±»åˆ«ä¸Šçš„æ€§èƒ½ã€‚

    éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œcross_val_predictè¿”å›çš„é¢„æµ‹å€¼å¹¶ä¸æ˜¯æ¨¡å‹åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šçš„é¢„æµ‹ç»“æœï¼Œè€Œæ˜¯æ¯ä¸ªæ ·æœ¬åœ¨å…¶æ‰€å±æµ‹è¯•é›†ä¸Šçš„é¢„æµ‹ç»“æœã€‚å› æ­¤ï¼Œå°†è¿™äº›é¢„æµ‹å€¼ä¼ é€’ç»™è¯„ä¼°æŒ‡æ ‡æ—¶ï¼Œéœ€è¦ç¡®ä¿æŒ‡æ ‡èƒ½å¤Ÿæ­£ç¡®å¤„ç†è¿™ç§æƒ…å†µã€‚

  - ```bash
                  precision    recall  f1-score   support
          
               0       1.00      1.00      1.00        50
               1       0.98      0.94      0.96        50
               2       0.94      0.98      0.96        50
          
        accuracy                           0.97       150
       macro avg       0.97      0.97      0.97       150
    weighted avg       0.97      0.97      0.97       150
    ```

## FAQ

### 1:å¯åˆ†è§£æŒ‡æ ‡

- `cross_validate_predict`ä¸­<u>ä¸ºä»€ä¹ˆè¯´:é™¤éæ‰€æœ‰æµ‹è¯•é›†çš„å¤§å°ç›¸ç­‰ä¸”æŒ‡æ ‡åœ¨æ ·æœ¬ä¸Šåˆ†è§£ï¼Œå¦åˆ™ç»“æœå¯èƒ½ä¸cross_validateå’Œcross_val_scoreä¸åŒã€‚å…·ä½“ä¾‹å­è¯´æ˜.</u>

- å½“æˆ‘ä»¬ä½¿ç”¨äº¤å‰éªŒè¯æ–¹æ³•ï¼ˆå¦‚cross_val_predictã€cross_validateå’Œcross_val_scoreï¼‰æ—¶ï¼Œæ•°æ®é›†è¢«åˆ†æˆkä¸ªå­é›†ï¼ˆæŠ˜ï¼‰ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œæ¨¡å‹åœ¨k-1ä¸ªå­é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨ä¸‹çš„ä¸€ä¸ªå­é›†ä¸Šè¿›è¡Œæµ‹è¯•ã€‚è¿™ä¸ªè¿‡ç¨‹é‡å¤kæ¬¡ï¼Œæ¯ä¸ªå­é›†éƒ½æœ‰æœºä¼šä½œä¸ºæµ‹è¯•é›†ã€‚

- å½“æ‰€æœ‰æµ‹è¯•é›†çš„å¤§å°ç›¸ä¸”æŒ‡æ ‡åœ¨æ ·æœ¬ä¸Šåˆ†è§£æ—¶ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥å°†æ¯ä¸ªå­é›†çš„è¯„ä¼°ç»“æœç›´æ¥ç›¸åŠ æˆ–å¹³å‡ï¼Œä»¥è·å¾—æ•´ä¸ªæ•°æ®é›†çš„è¯„ä¼°ç»“æœã€‚ç„¶è€Œï¼Œå¦‚æœæµ‹è¯•é›†çš„å¤§å°ä¸ç›¸ç­‰æˆ–æŒ‡æ ‡ä¸èƒ½åœ¨æ ·æœ¬ä¸Šåˆ†è§£ï¼Œé‚£ä¹ˆæˆ‘ä»¬ä¸èƒ½ç®€å•åœ°å°†æ¯ä¸ªå­é›†çš„è¯„ä¼°ç»“æœç›¸åŠ æˆ–å¹³å‡ï¼Œå› ä¸ºè¿™å¯èƒ½å¯¼è‡´å¯¹ä¸ªæ•°æ®é›†æ€§èƒ½çš„é”™è¯¯ä¼°è®¡ã€‚

- è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå…·ä½“çš„ä¾‹å­æ¥è¯´æ˜è¿™ä¸ªé—®é¢˜ï¼š

  - è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªåŒ…å«100ä¸ªæ ·æœ¬çš„æ•°æ®ï¼Œæˆ‘ä»¬ä½¿ç”¨5æŠ˜äº¤å‰éªŒè¯ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¯ä¸ªæµ‹è¯•é›†å°†åŒ…å«20ä¸ªæ ·æœ¬ã€‚å¦‚æœæˆ‘ä»¬ä½¿ç”¨å‡†ç¡®ç‡ä½œä¸ºè¯„ä¼°æŒ‡æ ‡ï¼Œé‚£ä¹ˆè¿™ä¸ªæŒ‡æ ‡æ˜¯å¯ä»¥åœ¨æ ·æœ¬ä¸Šåˆ†è§£çš„ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥<u>ç®€å•åœ°è®¡ç®—æ¯ä¸ªæµ‹è¯•é›†ä¸Šçš„æ­£ç¡®é¢„æ•°é‡ï¼Œç„¶åå°†å®ƒä»¬ç›¸åŠ å¹¶é™¤ä»¥æ€»æ ·æœ¬æ•°ä»¥è·å¾—æ•´ä¸ªæ•°æ®é›†çš„å‡†ç¡®ç‡ã€‚</u>

  - ```python
    from sklearn import datasets
    from sklearn.model_selection import cross_val_score
    from sklearn.linear_model import LogisticRegression
    
    iris = datasets.load_iris()
    X = iris.data
    y = iris.target
    
    lr = LogisticRegression(max_iter=1000)
    
    # ä½¿ç”¨cross_val_scoreè®¡ç®—å‡†ç¡®ç‡
    accuracy_scores = cross_val_score(lr, X, y, cv=5, scoring='accuracy')
    
    # è®¡ç®—æ•´ä¸ªæ•°æ®é›†çš„ç¡®ç‡
    overall_accuracy = accuracy_scores.mean()
    print("Overall accuracy:", overall_accuracy)
    ```

  - ```python
    from sklearn import datasets
    from sklearn.model_selection import cross_val_score
    from sklearn.linear_model import LogisticRegression
    
    iris = datasets.load_iris()
    X = iris.data
    y = iris.target
    
    lr = LogisticRegression(max_iter=1000)
    
    # ä½¿ç”¨cross_val_scoreè®¡ç®—å‡†ç¡®ç‡
    y_predict = cross_val_predict(lr, X, y, cv=5)
    
    # è®¡ç®—æ•´ä¸ªæ•°æ®é›†çš„ç¡®ç‡
    from sklearn.metrics import accuracy_score
    accuracy_score(y, y_predict)
    ```

  - ä¸Šè¿°ä¸¤ä¸ªç‰‡æ®µæ•ˆæœä¸€è‡´

- ç„¶è€Œï¼Œå¦‚æœæˆ‘ä»¬ä½¿ç”¨F1åˆ†æ•°ä½œä¸ºè¯„ä¼°æŒ‡æ ‡ï¼Œè¿™ä¸ªæŒ‡æ ‡ä¸èƒ½åœ¨æ ·æœ¬ä¸Šåˆ†è§£ï¼Œå› ä¸ºå®ƒæ˜¯ç²¾ç¡®åº¦å’Œå¬å›ç‡çš„è°ƒå’Œå‡å€¼ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¸èƒ½ç®€å•åœ°å°†æ¯ä¸ªæµ‹è¯•é›†ä¸Šçš„F1åˆ†æ•°ç›¸åŠ æˆ–å¹³å‡ï¼Œå› ä¸ºè¿™å¯èƒ½å¯¼è‡´å¯¹æ•´ä¸ªæ•°æ®é›†æ€§èƒ½çš„é”™è¯¯ä¼°è®¡ã€‚

  - ```python
    from sklearn.metrics import f1_score
    
    # ä½¿ç”¨cross_val_predictç”Ÿæˆé¢„æµ‹
    predictions = cross_val_predict(lr,X, y, cv=5)
    
    # è®¡ç®—æ•´ä¸ªæ•°æ®é›†çš„F1åˆ†æ•°
    overall_f1 = f1_score(y, predictions, average='macro')
    print("Overall F1 score:", overall_f1)
    ```

  - ```python
    
    # ä½¿ç”¨cross_val_predictç”Ÿæˆé¢„æµ‹
    f1_macro = cross_val_score(lr,X, y, cv=5,scoring='f1_macro')
    f1_macro.mean()
    
    ```

  - åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨cross_val_predictç”Ÿæˆé¢„æµ‹ï¼Œç„¶åä½¿ç”¨f1_scoreè®¡ç®—æ•´ä¸ªæ•°æ®é›†çš„F1åˆ†æ•°ã€‚è¯·æ³¨æ„ï¼Œè¿™ä¸ªç»“æœå¯èƒ½ä¸ä½¿ç”¨cross_val_scoreè®¡ç®—çš„F1åˆ†æ•°ä¸åŒï¼Œå› ä¸ºåè€…æ˜¯å°†æ¯ä¸ªæµ‹è¯•é›†ä¸Šçš„F1åˆ†æ•°å¹³å‡å¾—åˆ°çš„ã€‚ä¸ºäº†è·å¾—å‡†ç¡®çš„æ•´ä½“æ€§èƒ½è¯„ä¼°ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨é€‚å½“çš„æ–¹æ³•æ¥å¤„ç†è¿™ç§æƒ…å†µã€‚


### 2:ä¸æ°å½“çš„è¯„ä¼°æŒ‡æ ‡å¯¼è‡´çš„é”™è¯¯

- `ValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted']. `
- ä¾‹å¦‚$F_1-score$æ˜¯é’ˆå¯¹äºŒåˆ†ç±»çš„æŒ‡æ ‡
- å¤šåˆ†ç±»éœ€è¦ä½¿ç”¨æ¨å¹¿æŒ‡æ ‡(å¸¦æƒå¹³å‡åŒ–æŒ‡æ ‡),ä¾‹å¦‚
  - 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted',

### 3:å¯¹æ¯”:cross_val_score@cross_validate

- [sklearn.model_selection.cross_val_score â€” scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)

- [sklearn.model_selection.cross_validate â€” scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html)

- `cross_val_score`å’Œ`cross_validate`éƒ½æ˜¯Scikit-learnä¸­ç”¨äºæ‰§è¡ŒKæŠ˜äº¤å‰éªŒè¯çš„å‡½æ•°ï¼Œå®ƒä»¬çš„åŒºåˆ«åœ¨äºï¼š

  1. è¿”å›ç»“æœä¸åŒï¼š`cross_val_score`åªè¿”å›ä¸€ä¸ªåŒ…å«æ¯ä¸ªæŠ˜çš„è¯„ä¼°æŒ‡æ ‡å¾—åˆ†çš„æ•°ç»„ï¼Œè€Œ`cross_validate`è¿”å›ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«æ¯ä¸ªæŒ‡æ ‡çš„å¾—åˆ†æ•°ç»„ã€æ¯ä¸ªæ‹Ÿåˆæ—¶é—´çš„æ•°ç»„å’Œæ¯ä¸ªé¢„æµ‹æ—¶é—´çš„æ•°ç»„ã€‚
  2. å¯é€‰å‚æ•°ä¸åŒï¼š`cross_validate`å‡½æ•°æ¯”`cross_val_score`å‡½æ•°å¤šäº†ä¸€äº›å¯é€‰å‚æ•°ï¼Œä¾‹å¦‚è¿”å›è®­ç»ƒå¾—åˆ†ã€æµ‹è¯•å¾—åˆ†ã€æ‹Ÿåˆæ—¶é—´å’Œé¢„æµ‹æ—¶é—´ç­‰ã€‚
  3. é€‚ç”¨åœºåˆä¸åŒï¼š`cross_val_score`é€‚ç”¨äºç®€å•çš„è¯„ä¼°æ¨¡å‹æ€§èƒ½çš„æƒ…å†µï¼Œè€Œ`cross_validate`é€‚ç”¨äºæ›´å¤æ‚çš„æƒ…å†µï¼Œä¾‹å¦‚éœ€è¦åŒæ—¶è¯„ä¼°**å¤šä¸ªæŒ‡æ ‡**å’Œè®°å½•æ¨¡å‹**æ‹Ÿåˆå’Œé¢„æµ‹æ—¶é—´**çš„æƒ…å†µã€‚

  å› æ­¤ï¼Œåœ¨ç®€å•çš„æ¨¡å‹è¯„ä¼°ä»»åŠ¡ä¸­ï¼Œ`cross_val_score`æ˜¯æ›´å¸¸ç”¨å’Œæ›´æ–¹ä¾¿çš„å‡½æ•°ã€‚ä½†åœ¨æ›´å¤æ‚çš„ä»»åŠ¡ä¸­ï¼Œ`cross_validate`å¯èƒ½æ›´é€‚åˆï¼Œå› ä¸ºå®ƒå¯ä»¥æä¾›æ›´å¤šçš„ä¿¡æ¯å’Œçµæ´»æ€§ã€‚


#### demo

- ä»¥ä¸‹æ˜¯ä½¿ç”¨`cross_val_score`å’Œ`cross_validate`å‡½æ•°çš„ç¤ºä¾‹ï¼š

  ```python
  from sklearn.datasets import load_digits
  from sklearn.svm import SVC
  from sklearn.model_selection import cross_val_score, cross_validate
  
  digits = load_digits()
  X, y = digits.data, digits.target # type: ignore
  
  # ä½¿ç”¨cross_val_scoreå‡½æ•°
  clf = SVC(kernel='linear', C=1, random_state=0)
  scores = cross_val_score(clf, X, y, cv=5)
  print("Cross-validation scores@cross_val_score: ", scores)
  print()
  
  # ä½¿ç”¨cross_validateå‡½æ•°
  scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']
  clf = SVC(kernel='linear', C=1, random_state=0)
  scores = cross_validate(clf, X, y, scoring=scoring, cv=5, 
                          # return_train_score=True
                          )
  for key,value in scores.items():
      print(f'{key}:{value}')
  ```

- åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨`load_digits`åŠ è½½äº†ä¸€ä¸ªæ‰‹å†™æ•°å­—åˆ†ç±»æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨`SVC`ä½œä¸ºåˆ†ç±»å™¨ã€‚

- æˆ‘ä»¬ä½¿ç”¨`cross_val_score`å‡½æ•°è®¡ç®—äº†5æŠ˜äº¤å‰éªŒè¯çš„å‡†ç¡®æ€§å¾—åˆ†ï¼Œå¹¶ä½¿ç”¨`cross_validate`å‡½æ•°è®¡ç®—äº†5æŠ˜äº¤å‰éªŒè¯çš„å¤šä¸ªåº¦é‡å’Œæ‹Ÿåˆæ—¶é—´ã€‚

- æ³¨æ„ï¼Œ`cross_validate`å‡½æ•°è¿”å›ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«è®­ç»ƒå¾—åˆ†ã€æµ‹è¯•å¾—åˆ†å’Œæ‹Ÿåˆæ—¶é—´ç­‰ä¿¡æ¯ã€‚

  

# sklearn metricsğŸˆ

- [Metrics and scoring: quantifying the quality of predictions â€” scikit-learn  documentation](https://scikit-learn.org/stable/modules/model_evaluation.html)

- Scikit-learnä¸­çš„`metrics`æ¨¡å—åŒ…å«äº†è®¸å¤šç”¨äºè¯„ä¼°æœºå™¨å­¦ä¹ æ¨¡å‹æ€§èƒ½çš„åº¦é‡æŒ‡æ ‡ï¼Œç”¨äºæ¯”è¾ƒé¢„æµ‹ç»“æœå’ŒçœŸå®ç»“æœä¹‹é—´çš„å·®å¼‚ï¼Œä»¥ç¡®å®šæ¨¡å‹çš„å‡†ç¡®æ€§ã€ç²¾åº¦ã€å¬å›ç‡ã€F1åˆ†æ•°ç­‰æ€§èƒ½ã€‚

`metrics`æ¨¡å—ä¸­å¸¸ç”¨çš„åº¦é‡æŒ‡æ ‡åŒ…æ‹¬ï¼š

- åˆ†ç±»é—®é¢˜åº¦é‡æŒ‡æ ‡ï¼šå¦‚å‡†ç¡®ç‡ï¼ˆaccuracyï¼‰ã€ç²¾ç¡®ç‡ï¼ˆprecisionï¼‰ã€å¬å›ç‡ï¼ˆrecallï¼‰ã€F1åˆ†æ•°ï¼ˆF1 scoreï¼‰ã€ROCæ›²çº¿ï¼ˆROC curveï¼‰ã€AUCå€¼ï¼ˆAUC scoreï¼‰ç­‰ã€‚
- å›å½’é—®é¢˜åº¦é‡æŒ‡æ ‡ï¼šå¦‚å‡æ–¹è¯¯å·®ï¼ˆmean squared errorï¼‰ã€å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆmean absolute errorï¼‰ã€Rå¹³æ–¹ï¼ˆR-squared scoreï¼‰ç­‰ã€‚
- èšç±»é—®é¢˜åº¦é‡æŒ‡æ ‡ï¼šå¦‚è½®å»“ç³»æ•°ï¼ˆsilhouette scoreï¼‰ç­‰ã€‚
- å¤šæ ‡ç­¾åˆ†ç±»é—®é¢˜åº¦é‡æŒ‡æ ‡ï¼šå¦‚æ±‰æ˜æŸå¤±ï¼ˆHamming lossï¼‰ã€Jaccardç›¸ä¼¼åº¦ï¼ˆJaccard similarityï¼‰ç­‰ã€‚

### accuracy_score

- [sklearn.metrics.accuracy_score â€” scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)

- åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œ`accuracy`æ˜¯ä¸€ç§ç”¨äºè¯„ä¼°åˆ†ç±»æ¨¡å‹æ€§èƒ½çš„æŒ‡æ ‡ï¼Œè¡¨ç¤ºæ¨¡å‹æ­£ç¡®åˆ†ç±»çš„æ ·æœ¬æ¯”ä¾‹ã€‚å…·ä½“åœ°ï¼Œ`accuracy`å¯ä»¥å®šä¹‰ä¸ºï¼š

```
accuracy = (TP + TN) / (TP + TN + FP + FN)
```

- å…¶ä¸­ï¼Œ`TP`è¡¨ç¤ºçœŸæ­£ä¾‹ï¼ˆTrue Positiveï¼Œå³æ¨¡å‹æ­£ç¡®é¢„æµ‹ä¸ºæ­£ä¾‹çš„æ ·æœ¬æ•°ï¼‰ï¼Œ`TN`è¡¨ç¤ºçœŸåä¾‹ï¼ˆTrue Negativeï¼Œå³æ¨¡å‹æ­£ç¡®é¢„æµ‹ä¸ºåä¾‹çš„æ ·æœ¬æ•°ï¼‰ï¼Œ`FP`è¡¨ç¤ºå‡æ­£ä¾‹ï¼ˆFalse Positiveï¼Œå³æ¨¡å‹é”™è¯¯é¢„æµ‹ä¸ºæ­£ä¾‹çš„æ ·æœ¬æ•°ï¼‰ï¼Œ`FN`è¡¨ç¤ºå‡åä¾‹ï¼ˆFalse Negativeï¼Œå³æ¨¡å‹é”™è¯¯é¢„æµ‹ä¸ºåä¾‹çš„æ ·æœ¬æ•°ï¼‰ã€‚

- åœ¨Scikit-learnä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`accuracy_score`å‡½æ•°æ¥è®¡ç®—åˆ†ç±»æ¨¡å‹çš„`accuracy`æŒ‡æ ‡ã€‚

- ä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼š

```python
from sklearn.metrics import accuracy_score
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

# ç”Ÿæˆä¸€ä¸ªäºŒåˆ†ç±»æ•°æ®é›†
X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=123)

# åˆ’åˆ†æ•°æ®é›†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)

# è®­ç»ƒä¸€ä¸ªé€»è¾‘å›å½’æ¨¡å‹
clf = LogisticRegression(random_state=123)
clf.fit(X_train, y_train)

# åœ¨æµ‹è¯•é›†ä¸Šè®¡ç®—accuracy
y_pred = clf.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print("Accuracy:", acc)
```

åœ¨ä¸Šè¿°ä»£ç ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨`make_classification`å‡½æ•°ç”Ÿæˆäº†ä¸€ä¸ªäºŒåˆ†ç±»æ•°æ®é›†ï¼Œç„¶åä½¿ç”¨`train_test_split`å‡½æ•°å°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚æ¥ç€ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªé€»è¾‘å›å½’æ¨¡å‹ï¼Œå¹¶åœ¨æµ‹è¯•é›†ä¸Šè®¡ç®—äº†æ¨¡å‹çš„`accuracy`æŒ‡æ ‡ã€‚æœ€åï¼Œæˆ‘ä»¬è¾“å‡ºäº†æ¨¡å‹çš„`accuracy`å€¼ã€‚

æ€»ä¹‹ï¼Œåœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œ`accuracy`æ˜¯ä¸€ç§ç”¨äºè¯„ä¼°åˆ†ç±»æ¨¡å‹æ€§èƒ½çš„æŒ‡æ ‡ï¼Œè¡¨ç¤ºæ¨¡å‹æ­£ç¡®åˆ†ç±»çš„æ ·æœ¬æ¯”ä¾‹ã€‚åœ¨Scikit-learnä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`accuracy_score`å‡½æ•°æ¥è®¡ç®—åˆ†ç±»æ¨¡å‹çš„`accuracy`æŒ‡æ ‡ã€‚

#### eg

- ```python
  from sklearn.metrics import accuracy_score
  y_pred = [0, 2, 1, 3]
  y_true = [0, 1, 2, 3]
  res=accuracy_score(y_true, y_pred),accuracy_score(y_true, y_pred, normalize=False)
  #é¢„æµ‹çš„4ä¸ªæ ·æœ¬ä¸­,y_pred[0,3]æ˜¯å¯¹çš„,å…¶ä½™æ˜¯é”™è¯¯çš„
  res_bool=np.array(y_pred)==np.array(y_true)
  print(res,res_bool)
  ```

  - ```bash
    (0.5, 2) [ True False False  True]
    ```

    

- ä¾‹å¦‚ï¼Œä¸‹é¢çš„ä»£ç æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨`metrics`æ¨¡å—ä¸­çš„`accuracy_score`å‡½æ•°è®¡ç®—åˆ†ç±»æ¨¡å‹çš„å‡†ç¡®ç‡ï¼š

  ```python
  from sklearn.datasets import make_classification
  from sklearn.linear_model import LogisticRegression
  from sklearn.model_selection import train_test_split
  from sklearn.metrics import accuracy_score
  
  X, y = make_classification(n_samples=1000, random_state=0)
  X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
  lr = LogisticRegression()
  
  lr.fit(X_train, y_train)
  y_pred = lr.predict(X_test)
  acc = accuracy_score(y_test, y_pred)
  print("Accuracy: {:.2f}".format(acc))
  ```

- ```
  Accuracy: 0.97
  ```

  åœ¨ä¸Šé¢çš„ä»£ç ä¸­ï¼Œä½¿ç”¨`accuracy_score`å‡½æ•°è®¡ç®—æ¨¡å‹çš„å‡†ç¡®ç‡ï¼Œå¹¶è¾“å‡ºç»“æœã€‚å¯ä»¥æ ¹æ®å…·ä½“æƒ…å†µé€‰æ‹©åˆé€‚çš„åº¦é‡æŒ‡æ ‡æ¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚

#### metrics.accuracy_score vs estimator.score

- sklearnä¸­çš„estimatorç±»é€šå¸¸éƒ½å…·æœ‰score()æ–¹æ³•ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨ç»™å®šæµ‹è¯•æ•°æ®å’Œæ ‡ç­¾ä¸Šçš„æ€§èƒ½ã€‚å…¶ä¸­ï¼Œscore()æ–¹æ³•çš„å‚æ•°å¦‚ä¸‹ï¼š

  - Xï¼šæµ‹è¯•æ•°æ®

  - yï¼šæµ‹è¯•æ ‡ç­¾

  - sample_weightï¼šæ¯ä¸ªæ ·æœ¬çš„æƒé‡ï¼ˆå¯é€‰ï¼‰

- score()æ–¹æ³•çš„è¿”å›å€¼é€šå¸¸æ˜¯æ¨¡å‹åœ¨æµ‹è¯•æ•°æ®ä¸Šçš„æ€§èƒ½è¯„ä¼°æŒ‡æ ‡ï¼Œä¾‹å¦‚åˆ†ç±»é—®é¢˜ä¸­çš„å‡†ç¡®ç‡ã€å›å½’é—®é¢˜ä¸­çš„RÂ²ç­‰ã€‚

- å¯¹äºåˆ†ç±»é—®é¢˜ä¸­çš„score()æ–¹æ³•ï¼Œé»˜è®¤è¿”å›çš„æ˜¯å‡†ç¡®ç‡ï¼ˆaccuracyï¼‰ï¼Œå³æ­£ç¡®åˆ†ç±»çš„æ ·æœ¬æ•°å æ€»æ ·æœ¬æ•°çš„æ¯”ä¾‹ã€‚

- ä¾‹å¦‚ï¼Œå¯¹äºä¸€ä¸ªåˆ†ç±»å™¨`clf`ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹å¼è®¡ç®—å…¶åœ¨æµ‹è¯•é›†X_testå’Œæ ‡ç­¾y_testä¸Šçš„å‡†ç¡®ç‡ï¼š

  ```python
  from sklearn.metrics import accuracy_score
  
  y_pred = clf.predict(X_test)
  acc = accuracy_score(y_test, y_pred)
  ```

  ä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨clfçš„score()æ–¹æ³•è®¡ç®—å‡†ç¡®ç‡ï¼š

  ```python
  acc = clf.score(X_test, y_test)
  ```

  æ€»ä¹‹ï¼Œsklearnä¸­çš„estimatorç±»é€šå¸¸éƒ½æä¾›äº†score()æ–¹æ³•ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨ç»™å®šæµ‹è¯•æ•°æ®å’Œæ ‡ç­¾ä¸Šçš„æ€§èƒ½ï¼Œå¹¶è¿”å›ç›¸åº”çš„æ€§èƒ½è¯„ä¼°æŒ‡æ ‡ã€‚å…¶ä¸­å¯¹äºåˆ†ç±»é—®é¢˜ï¼Œé»˜è®¤è¿”å›çš„æ˜¯å‡†ç¡®ç‡ï¼ˆaccuracyï¼‰ã€‚

- ```python
  from sklearn.datasets import load_iris
  from sklearn.model_selection import train_test_split
  from sklearn.svm import SVC
  X,y=load_iris(return_X_y=True)
  X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=0)
  svc=SVC()
  svc.fit(X_train,y_train)
  
  ```

  - ```bash
    In [12]: svc.score?
    Signature: svc.score(X, y, sample_weight=None)
    Docstring:
    Return the mean accuracy on the given test data and labels.
    
    In multi-label classification, this is the subset accuracy
    which is a harsh metric since you require for each sample that
    each label set be correctly predicted.
    
    Parameters
    ----------
    X : array-like of shape (n_samples, n_features)
        Test samples.
    
    y : array-like of shape (n_samples,) or (n_samples, n_outputs)
        True labels for `X`.
    
    sample_weight : array-like of shape (n_samples,), default=None
        Sample weights.
    
    Returns
    -------
    score : float
        Mean accuracy of ``self.predict(X)`` w.r.t. `y`.
    File:      c:\users\cxxu\miniconda3\lib\site-packages\sklearn\base.py
    Type:      method
    ```

  - ```python
    In [16]: svc.score(X_test,y_test)
    Out[16]: 0.9736842105263158
    
    In [17]: from sklearn.metrics import accuracy_score
        ...: clf=svc
        ...: y_pred = clf.predict(X_test)
        ...: acc = accuracy_score(y_test, y_pred)
    
    In [18]: acc
    Out[18]: 0.9736842105263158
    ```



#### accuracy_score vs cross_validate

- `cross_validate`å’Œ`accuracy_score`éƒ½æ˜¯Scikit-learnä¸­ç”¨äºè¯„ä¼°æ¨¡å‹æ€§èƒ½çš„å‡½æ•°ï¼Œä½†å®ƒä»¬çš„åº”ç”¨åœºæ™¯å’Œç”¨æ³•æœ‰æ‰€ä¸åŒã€‚

  - `accuracy_score`å‡½æ•°ç”¨äºè®¡ç®—åˆ†ç±»æ¨¡å‹çš„å‡†ç¡®ç‡ï¼Œå³é¢„æµ‹æ­£ç¡®çš„æ ·æœ¬æ•°å æ€»æ ·æœ¬æ•°çš„æ¯”ä¾‹ã€‚å®ƒåªèƒ½è®¡ç®—æ¨¡å‹çš„å•ä¸€æŒ‡æ ‡ï¼Œä¸è€ƒè™‘æ¨¡å‹çš„å…¶ä»–æ€§èƒ½æŒ‡æ ‡ï¼Œä¹Ÿä¸è€ƒè™‘æ¨¡å‹çš„è®­ç»ƒæ—¶é—´å’Œé¢„æµ‹æ—¶é—´ç­‰å…¶ä»–æ–¹é¢çš„æ€§èƒ½ã€‚

  - `cross_validate`å‡½æ•°åˆ™å¯ä»¥åŒæ—¶è®¡ç®—å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ï¼Œä¾‹å¦‚è®­ç»ƒå¾—åˆ†ã€æµ‹è¯•å¾—åˆ†ã€æ‹Ÿåˆæ—¶é—´å’Œé¢„æµ‹æ—¶é—´ç­‰ã€‚è¿™äº›è¯„ä¼°æŒ‡æ ‡å¯ä»¥å¸®åŠ©æˆ‘ä»¬æ›´å…¨é¢åœ°äº†è§£æ¨¡å‹çš„æ€§èƒ½å’Œç‰¹ç‚¹ã€‚æ­¤å¤–ï¼Œ`cross_validate`å‡½æ•°è¿˜å¯ä»¥æŒ‡å®šå¤šä¸ªè¯„ä¼°æŒ‡æ ‡ï¼Œä¾‹å¦‚å‡†ç¡®ç‡ã€ç²¾åº¦ã€å¬å›ç‡ã€F1åˆ†æ•°ç­‰ï¼Œä»è€Œæ›´å…¨é¢åœ°è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚`cross_validate`è¿˜å¯ä»¥ä½¿ç”¨ä¸åŒçš„äº¤å‰éªŒè¯ç­–ç•¥ï¼Œä¾‹å¦‚ç•™ä¸€äº¤å‰éªŒè¯å’Œåˆ†å±‚KæŠ˜äº¤å‰éªŒè¯ç­‰ï¼Œä»¥æ»¡è¶³ä¸åŒçš„éœ€æ±‚ã€‚

- ç»¼ä¸Šæ‰€è¿°ï¼Œ`accuracy_score`å‡½æ•°é€‚ç”¨äºç®€å•çš„åˆ†ç±»æ¨¡å‹è¯„ä¼°ä»»åŠ¡ï¼Œè€Œ`cross_validate`å‡½æ•°é€‚ç”¨äºæ›´å¤æ‚çš„æ¨¡å‹è¯„ä¼°ä»»åŠ¡ï¼Œå¯ä»¥æä¾›æ›´å…¨é¢çš„æ€§èƒ½æŒ‡æ ‡å’Œæ›´çµæ´»çš„äº¤å‰éªŒè¯ç­–ç•¥ã€‚



## å¸¸è§å‚æ•°è¯´æ˜ğŸˆ

### cv

- `cv`å‚æ•°æ˜¯`scikit-learn`åº“ä¸­è®¸å¤šæœºå™¨å­¦ä¹ æ¨¡å‹çš„äº¤å‰éªŒè¯ç­–ç•¥å‚æ•°ï¼Œå®ƒå¯ä»¥ç”¨æ¥æŒ‡å®šäº¤å‰éªŒè¯çš„æŠ˜æ•°æˆ–è€…å…·ä½“çš„äº¤å‰éªŒè¯åˆ’åˆ†æ–¹æ³•ã€‚`cv`å‚æ•°å¯ä»¥ä¼ é€’ä»¥ä¸‹å‡ ç§å€¼ï¼š

  - `None`ï¼ˆé»˜è®¤å€¼ï¼‰ï¼šä½¿ç”¨é»˜è®¤çš„5æŠ˜äº¤å‰éªŒè¯æ–¹æ³•ï¼›
  - æ•´æ•°ï¼šæŒ‡å®šKFoldæˆ–StratifiedKFoldçš„æŠ˜æ•°ï¼›
  - `CV splitter`ï¼šè‡ªå®šä¹‰äº¤å‰éªŒè¯ç”Ÿæˆå™¨ï¼›
  - è¿­ä»£å™¨ï¼šç”Ÿæˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ç´¢å¼•ã€‚

  å¯¹äºæ•´æ•°æˆ–`None`ç±»å‹çš„è¾“å…¥ï¼Œå¦‚æœæ¨¡å‹æ˜¯åˆ†ç±»å™¨å¹¶ä¸”yæ˜¯äºŒå…ƒæˆ–å¤šå…ƒåˆ†ç±»çš„ï¼Œå°†ä½¿ç”¨`StratifiedKFold`æ–¹æ³•ã€‚å¦åˆ™ï¼Œå°†ä½¿ç”¨`KFold`æ–¹æ³•ã€‚è¿™äº›æ‹†åˆ†å™¨çš„`shuffle`å‚æ•°é»˜è®¤ä¸º`False`ï¼Œå› æ­¤æ¯æ¬¡æ‹†åˆ†çš„ç»“æœç›¸åŒã€‚

  éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä¸åŒçš„äº¤å‰éªŒè¯ç­–ç•¥å¯èƒ½é€‚ç”¨äºä¸åŒçš„æ•°æ®é›†å’Œæ¨¡å‹ï¼Œå› æ­¤å¯ä»¥æ ¹æ®å…·ä½“çš„éœ€æ±‚é€‰æ‹©ä¸åŒçš„äº¤å‰éªŒè¯æ–¹æ³•ã€‚

### n_informative

åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œ`n_informative`é€šå¸¸æ˜¯ç”Ÿæˆåˆ†ç±»æ•°æ®é›†æ—¶çš„ä¸€ä¸ªå‚æ•°ï¼Œç”¨äºæ§åˆ¶ç”Ÿæˆçš„æ•°æ®é›†ä¸­æœ‰ç”¨ç‰¹å¾çš„æ•°é‡ã€‚

å…·ä½“æ¥è¯´ï¼Œ`n_informative`è¡¨ç¤ºç”Ÿæˆæ•°æ®é›†æ—¶æ¯ä¸ªç±»åˆ«çš„ç‰¹å¾ä¸­æœ‰å¤šå°‘ä¸ªæ˜¯æœ‰ç”¨çš„ï¼Œä¹Ÿå°±æ˜¯èƒ½å¤ŸåŒºåˆ†ä¸åŒç±»åˆ«çš„ç‰¹å¾ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸€ä¸ªäºŒåˆ†ç±»é—®é¢˜ä¸­ï¼Œå¦‚æœè®¾ç½®`n_informative`ä¸º1ï¼Œåˆ™ç”Ÿæˆçš„æ•°æ®é›†ä¸­æ¯ä¸ªç±»åˆ«çš„ç‰¹å¾ä¸­åªæœ‰ä¸€ä¸ªèƒ½å¤ŸåŒºåˆ†ä¸åŒç±»åˆ«ï¼Œå…¶ä½™çš„ç‰¹å¾éƒ½æ˜¯å™ªå£°æˆ–æ— ç”¨ç‰¹å¾ã€‚

é€šè¿‡è°ƒæ•´`n_informative`å‚æ•°ï¼Œå¯ä»¥æ§åˆ¶ç”Ÿæˆçš„æ•°æ®é›†ä¸­æœ‰ç”¨ç‰¹å¾çš„æ•°é‡ï¼Œä»è€Œæ§åˆ¶åˆ†ç±»é—®é¢˜çš„éš¾åº¦ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œè¾ƒå°‘çš„æœ‰ç”¨ç‰¹å¾ä¼šä½¿åˆ†ç±»é—®é¢˜æ›´åŠ å›°éš¾ï¼Œéœ€è¦æ›´å¤æ‚çš„æ¨¡å‹æ‰èƒ½è§£å†³ã€‚è€Œè¾ƒå¤šçš„æœ‰ç”¨ç‰¹å¾åˆ™ä¼šä½¿åˆ†ç±»é—®é¢˜æ›´å®¹æ˜“ï¼Œæ›´ç®€å•çš„æ¨¡å‹å°±èƒ½å¤Ÿå¾—åˆ°è¾ƒå¥½çš„ç»“æœã€‚

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œ`n_informative`ä¸æ˜¯å”¯ä¸€çš„æ§åˆ¶ç”Ÿæˆæ•°æ®é›†éš¾åº¦çš„å‚æ•°ï¼Œå…¶ä»–å‚æ•°å¦‚`n_classes`ã€`n_clusters_per_class`ã€`class_sep`ç­‰ä¹Ÿä¼šå¯¹æ•°æ®é›†çš„éš¾åº¦äº§ç”Ÿå½±å“ã€‚å› æ­¤ï¼Œåœ¨ç”Ÿæˆåˆ†ç±»æ•°æ®é›†æ—¶ï¼Œéœ€è¦ç»¼åˆè€ƒè™‘å¤šä¸ªå‚æ•°çš„å½±å“ï¼Œè°ƒæ•´å‚æ•°æ¥ç”Ÿæˆåˆé€‚çš„æ•°æ®é›†ã€‚







