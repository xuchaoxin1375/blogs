[toc]

# ML@sklearn@MLæµç¨‹Part2@æ•°æ®åˆ’åˆ†@KFoldæŠ˜å äº¤å‰éªŒè¯

## Model evaluation

### æ•°æ®åˆ’åˆ†

- | `æ•°æ®åˆ’åˆ†`    | æ•°æ®é›†X          | æ ‡ç­¾é›†y          |
  | ------------- | ---------------- | ---------------- |
  | è®­ç»ƒé›†X_train | `X[train_index]` | `y[train_index]` |
  | æµ‹è¯•é›†y_train | `X[test_index]`  | `y[test_index]`  |

- åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šå°†æ•°æ®é›†åˆ†æˆè‡ªå˜é‡å’Œç›®æ ‡å˜é‡ä¸¤éƒ¨åˆ†ã€‚

  è‡ªå˜é‡æ˜¯ä¸€ç»„ç”¨äºé¢„æµ‹ç›®æ ‡å˜é‡çš„å˜é‡ï¼Œä¹Ÿè¢«ç§°ä¸ºç‰¹å¾æˆ–è¾“å…¥ã€‚è‡ªå˜é‡é€šå¸¸æ˜¯ä¸€ä¸ªçŸ©é˜µï¼Œå…¶ä¸­æ¯ä¸€è¡Œè¡¨ç¤ºä¸€ä¸ªæ ·æœ¬ï¼Œæ¯ä¸€åˆ—è¡¨ç¤ºä¸€ä¸ªç‰¹å¾ã€‚

  ç›®æ ‡å˜é‡æ˜¯æˆ‘ä»¬éœ€è¦é¢„æµ‹çš„å˜é‡ï¼Œä¹Ÿè¢«ç§°ä¸ºè¾“å‡ºã€‚ç›®æ ‡å˜é‡é€šå¸¸æ˜¯ä¸€ä¸ªå‘é‡ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ å¯¹åº”ä¸€ä¸ªæ ·æœ¬çš„è¾“å‡ºå€¼ã€‚

  ä¾‹å¦‚ï¼Œåœ¨å›å½’é—®é¢˜ä¸­ï¼Œè‡ªå˜é‡é€šå¸¸æ˜¯ä¸€ä¸ªåŒ…å«å¤šä¸ªç‰¹å¾çš„çŸ©é˜µï¼Œä¾‹å¦‚æˆ¿å±‹çš„é¢ç§¯ã€å§å®¤æ•°é‡ã€åœ°ç†ä½ç½®ç­‰ç­‰ï¼›è€Œç›®æ ‡å˜é‡é€šå¸¸æ˜¯ä¸€ä¸ªè¡¨ç¤ºæˆ¿å±‹ä»·æ ¼çš„å‘é‡ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ å¯¹åº”ä¸€ä¸ªæˆ¿å±‹çš„ä»·æ ¼ã€‚

  åœ¨ç›‘ç£å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šå°†æ•°æ®é›†åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå…¶ä¸­è®­ç»ƒé›†ç”¨äºè®­ç»ƒæ¨¡å‹ï¼Œæµ‹è¯•é›†ç”¨äºè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚è®­ç»ƒé›†ä¸­åŒ…å«è‡ªå˜é‡å’Œç›®æ ‡å˜é‡ï¼Œè€Œæµ‹è¯•é›†ä¸­åªåŒ…å«è‡ªå˜é‡ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹ï¼Œå¹¶å°†é¢„æµ‹ç»“æœä¸æµ‹è¯•é›†ä¸­çš„ç›®æ ‡å˜é‡è¿›è¡Œæ¯”è¾ƒï¼Œä»¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚

  éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè‡ªå˜é‡å’Œç›®æ ‡å˜é‡çš„æ•°é‡å’Œç±»å‹å–å†³äºå…·ä½“çš„é—®é¢˜å’Œæ•°æ®é›†ï¼Œä¸åŒçš„é—®é¢˜å¯èƒ½éœ€è¦ä¸åŒæ•°é‡å’Œç±»å‹çš„è‡ªå˜é‡å’Œç›®æ ‡å˜é‡ã€‚

- [Training, validation, and test data sets - Wikipedia](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets)
- In [machine learning](https://en.wikipedia.org/wiki/Machine_learning), a common task is the study and construction of [algorithms](https://en.wikipedia.org/wiki/Algorithm) that can learn from and make predictions on [data](https://en.wikipedia.org/wiki/Data). Such algorithms function by making data-driven predictions or decisions, through building a [mathematical model](https://en.wikipedia.org/wiki/Mathematical_model) from input data. These input data used to build the model are usually divided into multiple [data sets](https://en.wikipedia.org/wiki/Data_set). In particular, three data sets are commonly used in different stages of the creation of the model:
  -  training, validation, and test sets.
- The model is initially fit on a **training data set**, which is a set of examples used to fit the parameters (e.g. weights of connections between neurons in [artificial neural networks](https://en.wikipedia.org/wiki/Artificial_neural_networks)) of the model .The model (e.g. a [naive Bayes classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)) is trained on the training data set using a [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning) method, for example using optimization methods such as [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) or [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent). In practice, the training data set often consists of pairs of an input [vector](https://en.wikipedia.org/wiki/Array_data_structure) (or scalar) and the corresponding output vector (or scalar), where the answer key is commonly denoted as the *target* (or *label*). The current model is run with the training data set and produces a result, which is then compared with the *target*, for each input vector in the training data set. Based on the result of the comparison and the specific learning algorithm being used, the parameters of the model are adjusted. The model fitting can include both [variable selection](https://en.wikipedia.org/wiki/Feature_selection) and parameter [estimation](https://en.wikipedia.org/wiki/Estimation_theory).
- Successively, the fitted model is used to predict the responses for the observations in a second data set called the **validation data set**. The validation data set provides an unbiased evaluation of a model fit on the training data set while tuning the model's [hyperparameters](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)) (e.g. the number of hidden unitsâ€”layers and layer widthsâ€”in a neural network  ). Validation datasets can be used for [regularization](https://en.wikipedia.org/wiki/Regularization_(mathematics)) by [early stopping](https://en.wikipedia.org/wiki/Early_stopping) (stopping training when the error on the validation data set increases, as this is a sign of [over-fitting](https://en.wikipedia.org/wiki/Overfitting) to the training data set). This simple procedure is complicated in practice by the fact that the validation dataset's error may fluctuate during training, producing multiple local minima. This complication has led to the creation of many ad-hoc rules for deciding when over-fitting has truly begun.  
- Finally, the **test data set** is a data set used to provide an unbiased evaluation of a *final* model fit on the training data set.  If the data in the test data set has never been used in training (for example in [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics))), the test data set is also called a **holdout data set**. The term "validation set" is sometimes used instead of "test set" in some literature (e.g., if the original data set was partitioned into only two subsets, the test set might be referred to as the validation set).
- Deciding the sizes and strategies for data set division in training, test and validation sets is very dependent on the problem and data available.

### sklearnä¸­çš„æ¨¡å‹è¯„ä¼°

- Fitting a model to some data does not entail that it will predict well on unseen data.
- This needs to be directly evaluated. We have just seen the [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split) helper that splits a dataset into train and test sets, but `scikit-learn` provides many other tools for model evaluation, in particular for [cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation).
- We here briefly show how to perform a 5-fold cross-validation procedure, using the [`cross_validate`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate) helper. 
  - Note that it is also possible to manually iterate over the folds, use different data splitting strategies, and use custom scoring functions. 
  - Please refer to our [User Guide](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation) for more details:

### demo

- ```python
  from sklearn.datasets import make_regression
  from sklearn.linear_model import LinearRegression
  from sklearn.model_selection import cross_validate
  
  X, y = make_regression(n_samples=1000, noise=50,random_state=0)
  # é»˜è®¤ç»´æ•°ç»´100
  # X.shape,y.shape=1000,100
  
  lr = LinearRegression()
  result = cross_validate(lr, X, y)  # defaults to 5-fold CV
  result['test_score']  # r_squared score is high because dataset is easy
  result = cross_validate(lr, X, y, cv=5, return_train_score=True)
  for v in result.items():
      print(v)
  # print(result)
  ```

  - ```bash
    0.9736842105263158
    ('fit_time', array([0.01099563, 0.00600886, 0.00399899, 0.00663686, 0.00399613]))
    ('score_time', array([0.00099921, 0.0009954 , 0.00100064, 0.        , 0.00100303]))
    ('test_score', array([0.90468242, 0.8604173 , 0.89786489, 0.9145173 , 0.89278255]))
    ('train_score', array([0.92046141, 0.92835467, 0.92186638, 0.91778192, 0.92238646]))
    ```

  - å¦‚æœè®¾ç½®`noise=0`,é‡æ–°è¿è¡Œ,ç»“æœå½¢å¦‚

  - ```bash
    ('fit_time', array([0.00899696, 0.00603986, 0.00499964, 0.07801867, 0.0049994 ]))
    ('score_time', array([0.00099921, 0.        , 0.0010035 , 0.00197124, 0.00100207]))
    ('test_score', array([1., 1., 1., 1., 1.]))
    ('train_score', array([1., 1., 1., 1., 1.]))
    ```

  - åœ¨äººå·¥å›å½’æ•°æ®é›†ä¸­ï¼Œ`noise`å‚æ•°ç”¨äºæ§åˆ¶ç›®æ ‡å˜é‡ä¸­å™ªå£°çš„å¼ºåº¦ã€‚å…·ä½“æ¥è¯´ï¼Œå½“æˆ‘ä»¬ç”Ÿæˆä¸€ä¸ªäººå·¥å›å½’æ•°æ®é›†æ—¶ï¼Œæˆ‘ä»¬ä¼šæŒ‰ç…§ä¸€å®šçš„è§„åˆ™ç”Ÿæˆè‡ªå˜é‡`X`å’Œå› å˜é‡`y`ï¼Œå…¶ä¸­`y`çš„å€¼æ˜¯æ ¹æ®æŸç§å‡½æ•°å…³ç³»è®¡ç®—å‡ºæ¥çš„ã€‚å¦‚æœ`noise`å‚æ•°ä¸º0ï¼Œåˆ™ç”Ÿæˆçš„`y`å€¼å®Œå…¨ç¬¦åˆè¿™ç§å‡½æ•°å…³ç³»ï¼Œæ²¡æœ‰ä»»ä½•éšæœºå™ªå£°ã€‚è€Œå½“`noise`å‚æ•°å¤§äº0æ—¶ï¼Œç”Ÿæˆçš„`y`å€¼ä¼šå—åˆ°ä¸€å®šç¨‹åº¦çš„éšæœºå™ªå£°çš„å½±å“ã€‚

    åœ¨Scikit-learnä¸­ï¼Œ`noise`å‚æ•°çš„å€¼è¢«è§£é‡Šä¸ºç”Ÿæˆçš„æ•°æ®é›†ä¸­ç›®æ ‡å˜é‡ï¼ˆå³`y`ï¼‰çš„æ ‡å‡†å·®ã€‚å…·ä½“æ¥è¯´ï¼Œå½“`noise`å‚æ•°ä¸º0æ—¶ï¼Œç”Ÿæˆçš„æ•°æ®é›†ä¸­ç›®æ ‡å˜é‡çš„æ ‡å‡†å·®ä¸º0ï¼Œå³æ‰€æœ‰çš„ç›®æ ‡å˜é‡å€¼å®Œå…¨ç¬¦åˆå‡½æ•°å…³ç³»ï¼›è€Œå½“`noise`å‚æ•°ä¸ä¸º0æ—¶ï¼Œç”Ÿæˆçš„æ•°æ®é›†ä¸­ç›®æ ‡å˜é‡çš„æ ‡å‡†å·®å°†ä¼šå¤§äº0ï¼Œå³ç›®æ ‡å˜é‡å€¼ä¸å†å®Œå…¨ç¬¦åˆå‡½æ•°å…³ç³»ï¼Œè€Œæ˜¯å—åˆ°ä¸€å®šç¨‹åº¦çš„éšæœºå™ªå£°çš„å½±å“ã€‚

    éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œ`noise`å‚æ•°åªå½±å“ç”Ÿæˆçš„æ•°æ®é›†ä¸­ç›®æ ‡å˜é‡çš„éšæœºå™ªå£°ï¼Œè€Œä¸å½±å“è‡ªå˜é‡ã€‚å› æ­¤ï¼Œå½“æˆ‘ä»¬ç”Ÿæˆäººå·¥å›å½’æ•°æ®é›†æ—¶ï¼Œå¦‚æœéœ€è¦ä½¿æ•°æ®é›†æ›´åŠ çœŸå®ï¼Œå¯ä»¥å¢åŠ `noise`å‚æ•°çš„å€¼ï¼›åä¹‹ï¼Œå¦‚æœéœ€è¦ä½¿æ•°æ®é›†æ›´åŠ è§„å¾‹ï¼Œå¯ä»¥å°†`noise`å‚æ•°è®¾ç½®ä¸ºè¾ƒå°çš„å€¼æˆ–è€…0ã€‚

- è¿™æ®µä»£ç ä½¿ç”¨äº†Scikit-learnåº“ä¸­çš„`make_regression`å‡½æ•°ç”Ÿæˆä¸€ä¸ªåŒ…å«1000ä¸ªæ ·æœ¬çš„å›å½’æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨çº¿æ€§å›å½’æ¨¡å‹å¯¹æ•°æ®é›†è¿›è¡Œæ‹Ÿåˆå’Œè¯„ä¼°ã€‚


- å…·ä½“æ¥è¯´ï¼Œè¿™æ®µä»£ç çš„åŠŸèƒ½å¦‚ä¸‹ï¼š

  1. ä»Scikit-learnåº“ä¸­å¯¼å…¥`make_regression`å‡½æ•°ã€`LinearRegression`æ¨¡å‹å’Œ`cross_validate`å‡½æ•°ã€‚
  2. ä½¿ç”¨`make_regression`å‡½æ•°ç”Ÿæˆä¸€ä¸ªåŒ…å«1000ä¸ªæ ·æœ¬çš„å›å½’æ•°æ®é›†ï¼Œå…¶ä¸­`n_samples=1000`è¡¨ç¤ºæ•°æ®é›†ä¸­åŒ…å«1000ä¸ªæ ·æœ¬ï¼Œ`random_state=0`è¡¨ç¤ºä½¿ç”¨ç›¸åŒçš„éšæœºç§å­ç”Ÿæˆæ•°æ®é›†ï¼Œä»¥ç¡®ä¿ç»“æœçš„å¯é‡å¤æ€§ã€‚
  3. åˆ›å»ºä¸€ä¸ªçº¿æ€§å›å½’æ¨¡å‹ï¼Œå¹¶å°†å…¶èµ‹å€¼ç»™å˜é‡`lr`ã€‚
  4. ä½¿ç”¨`cross_validate`å‡½æ•°å¯¹çº¿æ€§å›å½’æ¨¡å‹è¿›è¡Œäº¤å‰éªŒè¯ï¼Œå…¶ä¸­`X`å’Œ`y`åˆ†åˆ«è¡¨ç¤ºæ•°æ®é›†çš„è‡ªå˜é‡å’Œå› å˜é‡ï¼Œ`result`ä¿å­˜äº†äº¤å‰éªŒè¯çš„ç»“æœã€‚
  5. è¾“å‡ºäº¤å‰éªŒè¯çš„æµ‹è¯•é›†å¾—åˆ†ï¼Œå…¶ä¸­`result['test_score']`è¡¨ç¤ºæµ‹è¯•é›†å¾—åˆ†ï¼Œå› ä¸ºæ•°æ®é›†æ¯”è¾ƒç®€å•ï¼Œæ‰€ä»¥å¾—åˆ†æ¯”è¾ƒé«˜ï¼Œé€šå¸¸ä½¿ç”¨Rå¹³æ–¹ä½œä¸ºè¯„ä¼°æŒ‡æ ‡ã€‚



## K-fold cross-validationğŸˆ

- KæŠ˜äº¤å‰éªŒè¯ï¼ˆK-fold cross-validationï¼‰æ˜¯ä¸€ç§å¸¸ç”¨çš„æ•°æ®é›†åˆ’åˆ†å’Œæ¨¡å‹éªŒè¯æŠ€æœ¯ï¼Œå¯ä»¥ç”¨äºè¯„ä¼°æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½å’Œè¿›è¡Œæ¨¡å‹çš„é€‰æ‹©å’Œè°ƒä¼˜ã€‚
- KæŠ˜äº¤å‰éªŒè¯çš„åŸºæœ¬æ€æƒ³æ˜¯å°†æ•°æ®é›†åˆ†æˆKä¸ªå­é›†ï¼ˆä¸€èˆ¬æ˜¯å‡ç­‰åˆ’åˆ†ï¼‰ï¼Œç„¶åä½¿ç”¨å…¶ä¸­K-1ä¸ªå­é›†ä½œä¸ºè®­ç»ƒé›†ï¼Œä½™ä¸‹çš„1ä¸ªå­é›†ä½œä¸ºéªŒè¯é›†ï¼Œè¿›è¡Œæ¨¡å‹çš„è®­ç»ƒå’ŒéªŒè¯ï¼Œé‡å¤Kæ¬¡ï¼Œæ¯æ¬¡ä½¿ç”¨ä¸åŒçš„éªŒè¯é›†ï¼Œæœ€ç»ˆå°†Kæ¬¡éªŒè¯çš„ç»“æœè¿›è¡Œå¹³å‡æˆ–åŠ æƒå¹³å‡ï¼Œå¾—åˆ°æœ€ç»ˆçš„æ€§èƒ½æŒ‡æ ‡ã€‚

- KæŠ˜äº¤å‰éªŒè¯çš„ä¼˜ç‚¹åœ¨äºï¼š
  1. å¯ä»¥å……åˆ†åˆ©ç”¨æ•°æ®é›†ä¸­çš„ä¿¡æ¯ï¼Œé¿å…è¿‡æ‹Ÿåˆæˆ–æ¬ æ‹Ÿåˆçš„é—®é¢˜ã€‚
  2. å¯ä»¥å¯¹æ¨¡å‹çš„æ€§èƒ½è¿›è¡Œæ›´å‡†ç¡®çš„è¯„ä¼°ï¼Œå‡å°è¯„ä¼°è¯¯å·®ã€‚
  3. å¯ä»¥åœ¨æœ‰é™çš„æ•°æ®é›†ä¸­ï¼Œæ‰©å¤§è®­ç»ƒé›†çš„è§„æ¨¡ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

### K-fold

- [sklearn.model_selection.KFold â€” scikit-learn  documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold)
- [Cross-validation: evaluating estimator performance â€” scikit-learn documentation](https://scikit-learn.org/stable/modules/cross_validation.html#k-fold)
- ä»¥ä¸‹ä»£ç æ¼”ç¤ºäº†KFoldæ˜¯æ€ä¹ˆå·¥ä½œçš„

#### eg

- ```python
  import numpy as np
  from sklearn.model_selection import KFold
  X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
  y = np.array([1, 2, 3, 4])
  kf = KFold(n_splits=2)
  n=kf.get_n_splits(X)
  print(n)
  print(kf)
  
  for i, (train_index, test_index) in enumerate(kf.split(X)):
      print(f"Fold {i}:")
      print(f"  Train: index={train_index}")
      print(f"  Test:  index={test_index}")
  ```

  - ```bash
    2
    KFold(n_splits=2, random_state=None, shuffle=False)
    Fold 0:
      Train: index=[2 3]
      Test:  index=[0 1]
    Fold 1:
      Train: index=[0 1]
      Test:  index=[2 3]
    ```

    

- è¿™æ®µä»£ç ä½¿ç”¨äº†Scikit-learnåº“ä¸­çš„`KFold`ç±»è¿›è¡ŒKæŠ˜äº¤å‰éªŒè¯ã€‚å…·ä½“æµç¨‹å¦‚ä¸‹ï¼š

- é¦–å…ˆï¼Œåˆ›å»ºä¸€ä¸ªåŒ…å«4ä¸ªæ ·æœ¬çš„æ•°æ®é›†`X`å’Œå¯¹åº”çš„æ ‡ç­¾`y`ï¼Œå…¶ä¸­`X`æ˜¯ä¸€ä¸ªäºŒç»´æ•°ç»„

  - æ¯è¡Œè¡¨ç¤ºä¸€ä¸ªæ ·æœ¬ï¼Œæ¯åˆ—è¡¨ç¤ºä¸€ä¸ªç‰¹å¾ã€‚
  - ç„¶åï¼Œåˆ›å»ºä¸€ä¸ª`KFold`å¯¹è±¡`kf`ï¼Œå¹¶å°†æ•°æ®é›†`X`ä¼ é€’ç»™å®ƒã€‚

- åœ¨`KFold`å¯¹è±¡ä¸­ï¼Œè®¾ç½®`n_splits=2`ï¼Œè¡¨ç¤ºå°†æ•°æ®é›†åˆ’åˆ†ä¸º2ä¸ªå­é›†ã€‚

  - è°ƒç”¨`get_n_splits`æ–¹æ³•å¯ä»¥è·å–å­é›†çš„æ•°é‡ã€‚

- æ¥ä¸‹æ¥ï¼Œä½¿ç”¨`kf.split(X)`æ–¹æ³•å¯¹æ•°æ®é›†è¿›è¡Œåˆ’åˆ†ï¼Œå¹¶éå†åˆ’åˆ†çš„ç»“æœã€‚

  - åˆ’åˆ†æ“ä½œåªéœ€è¦åˆ’åˆ†å’Œåˆ†ç»„ç´¢å¼•å³å¯,è®¿é—®æ•°æ®çš„æ—¶å€™æ ¹æ®åˆ†å¥½çš„ç´¢å¼•å»è®¿é—®å³å¯
  - åœ¨æ¯ä¸ªæŠ˜å ä¸­ï¼Œ`KFold`ç±»è¿”å›ä¸€ä¸ªå…ƒç»„`(train_index, test_index)`ï¼Œå…¶ä¸­`train_index`è¡¨ç¤ºç”¨äºè®­ç»ƒçš„æ ·æœ¬ç´¢å¼•ï¼Œ`test_index`è¡¨ç¤ºç”¨äºæµ‹è¯•çš„æ ·æœ¬ç´¢å¼•ã€‚
  - ğŸˆå°†kf.split(X)æŠ½å–çš„æ‰€æœ‰`test_index`åˆå¹¶èµ·æ¥(å¹¶æ’åº),å¾—åˆ°çš„åºåˆ—ç›¸å½“äº`range(len(X))`

- åœ¨å¾ªç¯ä¸­ï¼Œä½¿ç”¨`enumerate`å‡½æ•°è·å–å½“å‰æŠ˜å çš„ç´¢å¼•`i`ï¼Œå¹¶è¾“å‡ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ç´¢å¼•ã€‚å…·ä½“æ¥è¯´ï¼Œå°†`train_index`å’Œ`test_index`æ‰“å°å‡ºæ¥ï¼Œå…¶ä¸­`train_index`å’Œ`test_index`åˆ†åˆ«è¡¨ç¤ºå½“å‰æŠ˜å ä¸­ç”¨äºè®­ç»ƒå’Œæµ‹è¯•çš„æ ·æœ¬ç´¢å¼•ã€‚

  - ```python
    # æ ¹æ®åˆ†ç»„å¥½çš„ç´¢å¼•,ä½œæ•°æ®åˆ’åˆ†:
    # æ ¹æ®ç´¢å¼•åˆ’åˆ†æ•°æ®é›†
    X_train, X_test = X[train_index], X[test_index]
    # æ ¹æ®ç´¢å¼•åˆ’åˆ†æ ‡ç­¾
    y_train, y_test = y[train_index], y[test_index]
    ```

    - å¾—ç›Šäºnumpyæ•°ç»„çš„å…ƒç´ è®¿é—®æ–¹å¼,ç›¸å…³è¯­å¥ååˆ†ç®€æ´

  - train_indexå’Œtest_indexæ˜¯ç”±æŸç§äº¤å‰éªŒè¯æ–¹æ³•ï¼ˆå¦‚KæŠ˜äº¤å‰éªŒè¯ï¼‰ç”Ÿæˆçš„ç´¢å¼•æ•°ç»„ï¼Œç”¨äºå°†**æ•°æ®é›†X**å’Œ**æ ‡ç­¾é›†y**åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚(æ•°æ®é›†å’Œæ ‡ç­¾é›†çš„åˆ’åˆ†ä½¿ç”¨çš„ç´¢å¼•åºåˆ—æ˜¯å¯¹åº”ä¸€è‡´çš„)

  - åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼ŒXå’Œyæ˜¯åŸå§‹æ•°æ®é›†å’Œæ ‡ç­¾é›†ï¼Œtrain_indexå’Œtest_indexæ˜¯ç”±KæŠ˜äº¤å‰éªŒè¯æ–¹æ³•ç”Ÿæˆçš„ç´¢å¼•æ•°ç»„ã€‚

- å¯ä»¥çœ‹åˆ°ï¼Œæ•°æ®é›†è¢«åˆ’åˆ†ä¸ºäº†ä¸¤ä¸ªæŠ˜å ï¼Œæ¯ä¸ªæŠ˜å ä¸­è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ç´¢å¼•æ˜¯ä¸åŒçš„ã€‚ä¹Ÿå¯ä»¥é€šè¿‡`KFold`å¯¹è±¡çš„å…¶ä»–å±æ€§å’Œæ–¹æ³•æ¥æ§åˆ¶äº¤å‰éªŒè¯çš„æ–¹å¼ï¼Œå¦‚è®¾ç½®éšæœºç§å­ã€è¿›è¡Œåˆ†å±‚æŠ½æ ·ç­‰ã€‚

#### eg

- ```python
  X=np.random.randint(5,size=(12,3))
  y=np.random.choice(100,size=len(X))
  X,y
  
  kf2 = KFold(n_splits=3)
  n=kf2.get_n_splits(X)
  print(n)
  print(kf2)
  
  for i, (train_index, test_index) in enumerate(kf2.split(X)):
      print(f"Fold {i}:")
      print(f"  Train: index={train_index}")
      print(f"  Test:  index={test_index}")
      merge=np.concatenate((train_index,test_index))
      merge_sort=merge
      merge_sort.sort()
      print(f'{merge_sort=}')
  ```

  - ```bash
    3
    KFold(n_splits=3, random_state=None, shuffle=False)
    Fold 0:
      Train: index=[ 4  5  6  7  8  9 10 11]
      Test:  index=[0 1 2 3]
    merge_sort=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
    Fold 1:
      Train: index=[ 0  1  2  3  8  9 10 11]
      Test:  index=[4 5 6 7]
    merge_sort=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
    Fold 2:
      Train: index=[0 1 2 3 4 5 6 7]
      Test:  index=[ 8  9 10 11]
    merge_sort=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
    ```

- å†çœ‹ä¸‹shuffleå‚æ•°çš„æ•ˆæœ(éšæœºæ‰“ä¹±é¡ºåºç´¢å¼•,ç„¶ååˆ†ç»„)

  - ```python
    kf2 = KFold(n_splits=3,shuffle=True)
    n=kf2.get_n_splits(X)
    print(n)
    print(kf2)
    
    for i, (train_index, test_index) in enumerate(kf2.split(X)):
        print(f"Fold {i}:")
        print(f"  Train: index={train_index}")
        print(f"  Test:  index={test_index}")
        merge=np.concatenate((train_index,test_index))
        merge_sort=merge
        merge_sort.sort()
        print(f'{merge_sort=}')
    ```

    - ```bash
      3
      KFold(n_splits=3, random_state=None, shuffle=True)
      Fold 0:
        Train: index=[ 3  4  5  6  7  8  9 11]
        Test:  index=[ 0  1  2 10]
      merge_sort=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
      Fold 1:
        Train: index=[ 0  1  2  5  6  8 10 11]
        Test:  index=[3 4 7 9]
      merge_sort=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
      Fold 2:
        Train: index=[ 0  1  2  3  4  7  9 10]
        Test:  index=[ 5  6  8 11]
      merge_sort=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
      ```

      

#### eg:KFold@K-fold cross-validation

- ä¸‹é¢çš„ä»£ç æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨Scikit-learnåº“ä¸­çš„`KFold`ç±»è¿›è¡ŒKæŠ˜äº¤å‰éªŒè¯ï¼š

  - ```python
    from sklearn.datasets import make_classification
    from sklearn.linear_model import LogisticRegression
    from sklearn.model_selection import KFold
    from sklearn.metrics import accuracy_score
    
    X, y = make_classification(n_samples=1000, random_state=0)
    kf = KFold(n_splits=5, shuffle=True)
    
    for train_index, test_index in kf.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]
        lr = LogisticRegression()
        lr.fit(X_train, y_train)
        y_pred = lr.predict(X_test)
        
        acc = accuracy_score(y_test, y_pred)
        print("Accuracy: {:.2f}".format(acc))
    ```

- è¿™æ®µä»£ç ä½¿ç”¨Scikit-learnåº“ç”Ÿæˆä¸€ä¸ªäºŒåˆ†ç±»æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨Logisticå›å½’æ¨¡å‹è¿›è¡Œåˆ†ç±»ã€‚ç„¶åä½¿ç”¨KæŠ˜äº¤å‰éªŒè¯æ–¹æ³•æ¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚å…·ä½“è¿‡ç¨‹å¦‚ä¸‹ï¼š

  1. ä½¿ç”¨ `make_classification` å‡½æ•°ç”Ÿæˆä¸€ä¸ªåŒ…å«1000ä¸ªæ ·æœ¬çš„äºŒåˆ†ç±»æ•°æ®é›†ï¼Œå…¶ä¸­ç‰¹å¾æ•°ä¸ºé»˜è®¤å€¼(20)ï¼Œç±»åˆ«æ•°ä¸º2ï¼ŒéšæœºçŠ¶æ€ä¸º0ã€‚
  2. ä½¿ç”¨ `KFold` å‡½æ•°å°†æ•°æ®é›†åˆ†æˆ5ä¸ªäº’æ–¥çš„å­é›†ï¼Œæ¯ä¸ªå­é›†éƒ½å¯ä»¥ä½œä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ä¸€éƒ¨åˆ†ã€‚
  3. å¯¹äºæ¯ä¸ªå­é›†ï¼Œå°†å…¶ä½œä¸ºæµ‹è¯•é›†ï¼Œä½™ä¸‹çš„æ•°æ®ä½œä¸ºè®­ç»ƒé›†ã€‚
  4. åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒä¸€ä¸ªLogisticå›å½’æ¨¡å‹ã€‚
  5. åœ¨æµ‹è¯•é›†ä¸Šé¢„æµ‹æ¨¡å‹çš„è¾“å‡ºï¼Œå¹¶ä½¿ç”¨ `accuracy_score` å‡½æ•°è®¡ç®—é¢„æµ‹çš„å‡†ç¡®ç‡ã€‚
  6. æ‰“å°æ¯æ¬¡äº¤å‰éªŒè¯çš„å‡†ç¡®ç‡ã€‚
  7. æœ€ç»ˆè¾“å‡º5æ¬¡äº¤å‰éªŒè¯çš„å‡†ç¡®ç‡å‡å€¼å’Œæ ‡å‡†å·®ã€‚

  è¿™æ®µä»£ç å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨KæŠ˜äº¤å‰éªŒè¯æ–¹æ³•æ¥è¯„ä¼°Logisticå›å½’æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶è®¡ç®—æ¨¡å‹çš„å‡†ç¡®ç‡ã€‚

- å¯ä»¥æ ¹æ®å…·ä½“æƒ…å†µé€‰æ‹©åˆé€‚çš„Kå€¼å’ŒéªŒè¯æŒ‡æ ‡æ¥è¿›è¡Œæ¨¡å‹è¯„ä¼°ã€‚

### ShuffleSplit

- [sklearn.model_selection.ShuffleSplit â€” scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html)

- Random permutation cross-validatoréšæœºæ’åˆ—äº¤å‰éªŒè¯å™¨

  Yields indices to split data into training and test sets.

  Note: contrary to other cross-validation strategies, random splits do not guarantee that all folds will be different, although this is still very likely for **sizeable** datasets.

  æ­¤äº¤å‰éªŒè¯å™¨å°†æ•°æ®æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ç´¢å¼•ã€‚

- æ³¨æ„ï¼šä¸å…¶ä»–äº¤å‰éªŒè¯ç­–ç•¥ä¸åŒï¼Œ**éšæœºæ‹†åˆ†ä¸èƒ½ä¿è¯æ‰€æœ‰çš„foldéƒ½æ˜¯ä¸åŒçš„**ï¼Œå°½ç®¡å¯¹äºå¤§å‹æ•°æ®é›†æ¥è¯´è¿™ç§æƒ…å†µä»ç„¶æ˜¯å¾ˆå¯èƒ½å‘ç”Ÿçš„ã€‚

- [  Cross-validation: evaluating estimator performance â€” scikit-learn  documentation](https://scikit-learn.org/stable/modules/cross_validation.html#random-permutations-cross-validation-a-k-a-shuffle-split)

##### eg

- ```python
  from sklearn.model_selection import ShuffleSplit
  X = np.arange(8)
  # X = np.arange(12,24)#ç”Ÿæˆ12ä¸ªæ•°,12~23
  ss = ShuffleSplit(n_splits=5, test_size=0.25, random_state=0)
  # ShuffleSplitä»…åˆ’åˆ†ç´¢å¼•,è€Œä¸æ˜¯æ•°æ®æ ·æœ¬æœ¬èº«
  for train_index, test_index in ss.split(X):
      print("%s %s" % (train_index, test_index))
  ```

  - è¿™æ®µä»£ç é¦–å…ˆä½¿ç”¨`np.arange`å‡½æ•°ç”Ÿæˆä¸€ä¸ªåŒ…å«8ä¸ªæ•°çš„æ•°ç»„`X`ï¼Œç„¶ååˆ›å»ºä¸€ä¸ª`ShuffleSplit`å¯¹è±¡`ss`ï¼Œå°†æ•°æ®é›†åˆ†æˆ`5`ä¸ªä¸åŒçš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œæµ‹è¯•é›†çš„å¤§å°è®¾ç½®ä¸º25%ï¼Œéšæœºç§å­è®¾ç½®ä¸º0ã€‚
  - ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨ssçš„`split`æ–¹æ³•å¯¹æ•°æ®é›†è¿›è¡Œéšæœºæ‹†åˆ†ï¼Œå°†æ¯ä¸ªfoldçš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ç´¢å¼•åˆ†åˆ«å­˜å‚¨åœ¨`train_index`å’Œ`test_index`å˜é‡ä¸­ï¼Œå¹¶è¾“å‡ºè¿™ä¸¤ä¸ªå˜é‡ã€‚

- Here is a visualization of the cross-validation behavior. Note that [`ShuffleSplit`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit) is not affected by classes or groups.

  [![../_images/sphx_glr_plot_cv_indices_008.png](https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_008.png)](https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html)

- [`ShuffleSplit`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit) is thus a good alternative to [`KFold`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold) cross validation that allows a finer control on the number of iterations and the proportion of samples on each side of the train / test split.

- è¯·æ³¨æ„ï¼ŒShuffleSplitä¸å—ç±»åˆ«æˆ–ç»„çš„å½±å“ã€‚å› æ­¤ï¼ŒShuffleSplitæ˜¯KFoldäº¤å‰éªŒè¯çš„ä¸€ä¸ªå¾ˆå¥½çš„æ›¿ä»£æ–¹æ³•ï¼Œå®ƒå…è®¸å¯¹è¿­ä»£æ¬¡æ•°å’Œè®­ç»ƒ/æµ‹è¯•æ‹†åˆ†çš„æ ·æœ¬æ¯”ä¾‹è¿›è¡Œæ›´ç²¾ç»†çš„æ§åˆ¶ã€‚

- å¯¹æ¯”äºKFold

  - ```python
    from sklearn.model_selection import KFold
    X = np.arange(8)
    kf = KFold(n_splits=5)
    for train_index, test_index in kf.split(X):
        print("%s %s" % (train_index, test_index))
    ```

    - ```bash
      [2 3 4 5 6 7] [0 1]
      [0 1 4 5 6 7] [2 3]
      [0 1 2 3 6 7] [4 5]
      [0 1 2 3 4 5 7] [6]
      [0 1 2 3 4 5 6] [7]
      
      ```

  - æ ¹æ®KFoldçš„å®šä¹‰,å¦‚æœå¸Œæœ›å¯¹nä¸ªæ•°æ®åškæŠ˜å ,é‚£ä¹ˆæ¯ä¸€ä¸ªfoldåŒ…å«çš„æ ·æœ¬æ•°$\frac{n}{k}$

    - åœ¨sklearnä¸­,å¯èƒ½å‘ä¸Šå–æ•´,ä¹Ÿå¯èƒ½å‘ä¸‹å–æ•´

  - å¦‚æœæŠŠ`np.arange(8)`æ”¹ä¸º`np.arange(10)`,æ­¤æ—¶5æŠ˜å å¯ä»¥æ•´é™¤`10/5=2`

    - é‚£ä¹ˆæ¯ä¸ªfoldåŒ…å«2ä¸ªæ•°æ®

    - ```
      [2 3 4 5 6 7 8 9] [0 1]
      [0 1 4 5 6 7 8 9] [2 3]
      [0 1 2 3 6 7 8 9] [4 5]
      [0 1 2 3 4 5 8 9] [6 7]
      [0 1 2 3 4 5 6 7] [8 9]
      ```

  - ä¸Šè¿°ä¾‹å­çš„KFoldæ²¡æœ‰ä½¿ç”¨`shuffle=True`å‚æ•°æ‰“ä¹±é¡ºåºæ˜¯ä¸ºäº†æ”¾ä¾¿è§‚å¯Ÿ

  - é€šå¸¸å»ºè®®ä½¿ç”¨`shuffle=True`,ä¸å®¹æ˜“å—åˆ°æ•°æ®é›†æ ·æœ¬é¡ºåºçš„å½±å“!

####  å°ç»“

- ä»ä¸Šé¢çš„ç›´è§‚å¯¹æ¯”ä¸­å¯ä»¥çœ‹å‡º,`ShuffleSplit`å‚æ•°å¯ä»¥æ¥æ”¶`n_splits`å’Œ`test_set`ä¸¤ä¸ªç›¸å¯¹ç‹¬ç«‹çš„å‚æ•°
- æˆ‘ä»¬å¯ä»¥å¯¹åŒä¸€ä¸ªæ•°æ®é›†åš`n_splits`æ¬¡åˆ’åˆ†,åŒæ—¶æ¯æ¬¡åˆ’åˆ†ä¸­`test_size`ä¸å—`n_splits`çš„å½±å“
- è€Œå¯¹äºKFold,`n_splits`å¾€å¾€å°±å†³å®šäº†`test_size`çš„å€¼ä¸º`1/n_splits`

### Stratified Shuffle Split

- [sklearn.model_selection.StratifiedShuffleSplit â€” scikit-learn  documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html)
- [  Cross-validation: evaluating estimator performance â€” scikit-learn  documentation](https://scikit-learn.org/stable/modules/cross_validation.html#stratified-shuffle-split)

- [`StratifiedShuffleSplit`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html#sklearn.model_selection.StratifiedShuffleSplit) is a variation of *ShuffleSplit*, which returns stratified splits, *i.e* which creates splits by preserving t**he same percentage for each target class** as in the complete set.
- Stratified ShuffleSplit cross-validator

  Provides train/test indices to split data in train/test sets.

  This cross-validation object is a merge of StratifiedKFold and ShuffleSplit, which returns stratified randomized folds. The folds are made by preserving the percentage of samples for each class.

  Note: like the ShuffleSplit strategy, stratified random splits do not guarantee that all folds will be different, although this is still very likely for sizeable datasets.

  `StratifiedShuffleSplit`æ˜¯ä¸€ç§äº¤å‰éªŒè¯ç”Ÿæˆå™¨ï¼Œå®ƒé€šè¿‡åˆ›å»ºåˆ†å±‚çš„éšæœºæ‹†åˆ†æ¥æä¾›è®­ç»ƒ/æµ‹è¯•ç´¢å¼•ï¼Œç”¨äºå°†æ•°æ®é›†åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚

  è¿™ä¸ªäº¤å‰éªŒè¯å¯¹è±¡æ˜¯`StratifiedKFold`å’Œ`ShuffleSplit`çš„åˆå¹¶ï¼Œå®ƒè¿”å›åˆ†å±‚çš„éšæœºæŠ˜å ã€‚è¿™äº›æŠ˜å æ˜¯é€šè¿‡ä¿ç•™æ¯ä¸ªç±»åˆ«æ ·æœ¬çš„ç™¾åˆ†æ¯”æ¥æ„å»ºçš„ã€‚

  éœ€è¦æ³¨æ„çš„æ˜¯ï¼šä¸`ShuffleSplit`ç­–ç•¥ä¸€æ ·ï¼Œåˆ†å±‚éšæœºæ‹†åˆ†å¹¶ä¸èƒ½ä¿è¯æ‰€æœ‰æŠ˜å éƒ½æ˜¯ä¸åŒçš„ï¼Œå°½ç®¡å¯¹äºè§„æ¨¡è¾ƒå¤§çš„æ•°æ®é›†ï¼Œè¿™ä»ç„¶æ˜¯éå¸¸å¯èƒ½çš„ã€‚
- `StratifiedShuffleSplit` æ˜¯ `ShuffleSplit` çš„ä¸€ç§å˜ä½“ï¼Œå®ƒé€šè¿‡ä¿ç•™ä¸å®Œæ•´æ•°æ®é›†ç›¸åŒçš„æ¯ä¸ªç›®æ ‡ç±»åˆ«çš„ç™¾åˆ†æ¯”æ¥åˆ›å»ºåˆ†å±‚çš„æ‹†åˆ†ã€‚

  `ShuffleSplit` æ˜¯ä¸€ç§ç®€å•çš„äº¤å‰éªŒè¯ç­–ç•¥ï¼Œå®ƒå°†æ•°æ®é›†éšæœºåˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œä¸è€ƒè™‘ç›®æ ‡å˜é‡çš„ç±»åˆ«åˆ†å¸ƒã€‚

  å¦ä¸€æ–¹é¢ï¼Œ`StratifiedShuffleSplit` åœ¨åˆ›å»ºæ‹†åˆ†æ—¶è€ƒè™‘ç›®æ ‡å˜é‡çš„ç±»åˆ«åˆ†å¸ƒã€‚å®ƒç¡®ä¿åœ¨æ¯ä¸ªæ‹†åˆ†ä¸­ä¿ç•™æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬ç™¾åˆ†æ¯”ï¼Œè¿™åœ¨å¤„ç†ä¸å¹³è¡¡æ•°æ®é›†æ—¶éå¸¸é‡è¦ã€‚
- å› æ­¤ï¼Œå½“ä½ å¤„ç†ä¸å¹³è¡¡æ•°æ®é›†å¹¶ä¸”æƒ³è¦ç¡®ä¿æ¯ä¸ªæ‹†åˆ†ä¸­çš„ç›¸åŒç±»åˆ«åˆ†å¸ƒæ—¶ï¼Œ`StratifiedShuffleSplit` æ˜¯ä¸€ç§éå¸¸æœ‰ç”¨çš„äº¤å‰éªŒè¯ç­–ç•¥ã€‚

- `StratifiedShuffleSplit`æ˜¯`scikit-learn`åº“ä¸­çš„ä¸€ä¸ªäº¤å‰éªŒè¯ç”Ÿæˆå™¨ï¼Œå®ƒå¯ä»¥å°†æ•°æ®é›†éšæœºåˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå¹¶ä¸”**ä¿æŒæ¯ä¸ªç±»åˆ«åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­çš„æ¯”ä¾‹ç›¸ç­‰**ã€‚
- è¿™ä¸ªæ–¹æ³•é€‚ç”¨äºåˆ†ç±»é—®é¢˜ä¸­<u>ç±»åˆ«ä¸å¹³è¡¡çš„æ•°æ®é›†</u>ï¼Œå¯ä»¥ç¡®ä¿è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­çš„æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°é‡éƒ½**å¤§è‡´ç›¸åŒ**ã€‚

`StratifiedShuffleSplit`çš„åˆ›å»ºæ–¹æ³•å¦‚ä¸‹ï¼š



- ```python
  from sklearn.model_selection import StratifiedShuffleSplit
  sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)
  ```

- åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ª`StratifiedShuffleSplit`å¯¹è±¡`sss`ï¼Œå…¶ä¸­`n_splits`å‚æ•°æŒ‡å®šäº†åˆ’åˆ†æŠ˜æ•°ï¼Œ`test_size`å‚æ•°æŒ‡å®šäº†æµ‹è¯•é›†å æ¯”ï¼Œ`random_state`å‚æ•°æŒ‡å®šäº†éšæœºç§å­ï¼Œç”¨äºæ§åˆ¶éšæœºæ€§ã€‚
- ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`split`æ–¹æ³•å¯¹æ•°æ®é›†è¿›è¡Œåˆ’åˆ†ï¼Œå°†æ¯ä¸ªfoldçš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ç´¢å¼•åˆ†åˆ«å­˜å‚¨åœ¨`train_index`å’Œ`test_index`å˜é‡ä¸­ã€‚

#### eg

ä»¥ä¸‹æ˜¯ä¸€ä¸ªä½¿ç”¨`StratifiedShuffleSplit`å¯¹æ•°æ®é›†è¿›è¡Œäº¤å‰éªŒè¯çš„ç¤ºä¾‹ä»£ç ï¼š

- ```python
  from sklearn.datasets import load_iris
  from sklearn.model_selection import StratifiedShuffleSplit
  iris = load_iris()
  X, y = iris.data, iris.target
  sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)
  for train_index, test_index in sss.split(X, y):
      print("%s %s" % (train_index, test_index))
  ```


- åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆåŠ è½½äº†`iris`æ•°æ®é›†ï¼Œç„¶åå°†æ•°æ®é›†åˆ’åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚æˆ‘ä»¬ä½¿ç”¨`StratifiedShuffleSplit`æ–¹æ³•å°†æ•°æ®é›†åˆ†æˆ5ä¸ªä¸åŒçš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå¹¶å°†æ¯ä¸ªfoldçš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ç´¢å¼•åˆ†åˆ«å­˜å‚¨åœ¨`train_index`å’Œ`test_index`å˜é‡ä¸­ã€‚ç”±äº`iris`æ•°æ®é›†æ˜¯ä¸€ä¸ªåˆ†ç±»é—®é¢˜ï¼Œå› æ­¤æˆ‘ä»¬ä½¿ç”¨`StratifiedShuffleSplit`ä¿æŒäº†æ¯ä¸ªç±»åˆ«åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­çš„æ¯”ä¾‹ç›¸ç­‰ã€‚

#### eg

- ä¸‹é¢è¿™ä¸ªä¾‹å­æ›´åŠ è¯¦ç»†çš„æè¿°äº†è¿™ä¸€ç‚¹

  ```python
  
  import numpy as np
  from sklearn.model_selection import StratifiedShuffleSplit
  #ä½¿ç”¨éšæœºç”Ÿæˆæ•°æ®æµ‹è¯•
  rng=np.random.default_rng()
  rng.integers(20,size=(12,2))
  # æ ·æœ¬æ€»æ•°ä¸º12,äºŒåˆ†ç±»,æ ‡ç­¾ä¸º0,1,ä¸¤ç§æ ·æœ¬æ¯”ä¾‹ä¸º1:2
  n=12
  n0,n1=1*n//3,2*n//3
  #éšæœºçš„ä¸ºè¿™äº›æ¨¡æ‹Ÿæ ·æœ¬åˆ†é…æ ‡ç­¾(å› ä¸ºè¿™é‡Œä¸æ¶‰åŠåˆ°è®­ç»ƒ,æ‰€ä»¥éšæœºåˆ†é…æ ‡ç­¾ä¸å½±å“æ•ˆæœ,åœ¨æ•°æ®é›†åˆ’åˆ†çš„é˜¶æ®µ,ä¸ç”¨å…³å¿ƒæ ·æœ¬å’Œæ ‡ç­¾çš„å…³è”è§„å¾‹,å¦‚æœæ˜¯è¦è®­ç»ƒ,é€šå¸¸æ˜¯ä¸èƒ½éšæœºç»™æ ·æœ¬ç‰¹å¾åˆ†é…æ ‡ç­¾)
  
  y=[0]*n0+[1]*n1
  y=np.array(y)
  rng.shuffle(y)
  #ä¸‹é¢ä¸€ç§æ–¹å¼é‡‡ç”¨æ¦‚ç‡çš„æ–¹å¼ç”Ÿæˆæ ‡ç­¾,ä½†æ˜¯å³ä½¿æ ·æœ¬æ€»æ•°nå¯ä»¥è¢«3æ•´é™¤,ç”Ÿæˆçš„æ•°ç»„ä¹Ÿä¸ä¿è¯æ•°é‡æ˜¯1:2
  # y = rng.choice([0, 1], size=12, replace=True, p=[1/3, 2/3])
  # count=np.unique(y,return_counts=True)
  # print(count)
  #ä¸ºä¾‹æ”¾ä¾¿éªŒè¯,è¿™é‡Œå°†æ ‡ç­¾æ•°ç»„å’Œæ ·æœ¬ç´¢å¼•æ‰“å°å‡ºæ¥
  print(np.vstack([y,range(n)]))
  #æ„é€ åˆ†å±‚éšæœºæ‹†åˆ†å¯¹è±¡,æŒ‡å®šåšç‹¬ç«‹çš„5æ¬¡åˆ’åˆ†,æ¯æ¬¡åˆ’åˆ†,æµ‹è¯•é›†çš„æ ·æœ¬æ•°é‡å æ€»æ ·æœ¬æ•°é‡nçš„20%
  #è€ŒStratifiedShuffleSplitä¼šä¿æŒå„ä¸ªç±»åˆ«åœ¨æµ‹è¯•é›†å’Œè®­ç»ƒé›†ä¸Šçš„æ¯”ä¾‹
  # æ˜¯ä¸¤ç§ç‹¬ç«‹çš„çº¦æŸ(ä¾‹å¦‚0ç±»æ ·å’Œ1ç±»æ ·æœ¬æ¯”ä¾‹åœ¨æ•°æ®é›†ä¸­ä¸º1:2,é‚£ä¹ˆåœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­ä¾ç„¶ä¿æŒ(æˆ–æ¥è¿‘)1:2)
  sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)
  # print(sss)
  #æ‰“å°è¿™5æ¬¡
  for i, (train_index, test_index) in enumerate(sss.split(X, y)):
      print(f"Fold {i}:")
      print(f"  Train: index={train_index}")
      print(f"  Test:  index={test_index}")
      print(np.vstack((test_index, y[test_index])))
  ```

  

- ```bash
  [[ 1  1  0  1  1  1  0  0  1  1  0  1]
   [ 0  1  2  3  4  5  6  7  8  9 10 11]]
  Fold 0:
    Train: index=[ 3 10  6  9  5  7  8  1  0]
    Test:  index=[ 2 11  4]
  [[ 2 11  4]
   [ 0  1  1]]
  Fold 1:
    Train: index=[ 4  2  9  0  7 11  5  3  6]
    Test:  index=[10  1  8]
  [[10  1  8]
   [ 0  1  1]]
  Fold 2:
    Train: index=[ 6  9  2 11  7  4  3  8  5]
    Test:  index=[ 1  0 10]
  [[ 1  0 10]
   [ 1  1  0]]
  Fold 3:
    Train: index=[10  8  6  3 11  4  1  7  5]
    Test:  index=[0 2 9]
  [[0 2 9]
   [1 0 1]]
  Fold 4:
    Train: index=[ 7  2  0  6 11  1  5  3  9]
    Test:  index=[ 8  4 10]
  [[ 8  4 10]
   [ 1  1  0]]
  ```

## demo

- ```python
  
  import numpy as np
  from sklearn.datasets import load_iris
  from sklearn.ensemble import RandomForestClassifier
  from sklearn.linear_model import LinearRegression
  from sklearn.model_selection import KFold, cross_val_score
  from sklearn.svm import SVC
  from sklearn.tree import DecisionTreeClassifier
  from sklearn.datasets import load_iris
  from sklearn.model_selection import ShuffleSplit, cross_val_score
  from sklearn.tree import DecisionTreeClassifier
  
  ##
  
  # åŠ è½½iris(é¸¢å°¾èŠ±)æ•°æ®é›†
  X, y = load_iris(return_X_y=True)
  
  #! å®šä¹‰5æŠ˜äº¤å‰éªŒè¯
  kf = KFold(
      n_splits=5,
      shuffle=True,
      random_state=42,
  )
  
  # ä½¿ç”¨çº¿æ€§å›å½’æ¨¡å‹è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•
  model = LinearRegression()
  # model=RandomForestClassifier()
  
  scores_cv = []
  # è¿™é‡Œsplitå‚æ•°å¯ä»¥æ˜¯Xä¹Ÿå¯ä»¥æ˜¯y,å› ä¸ºåªéœ€è¦åˆ’åˆ†æ ·æœ¬çš„ç´¢å¼•,æ‰€ä»¥ä¸¤è€…éƒ½å¯ä»¥
  for train_index, test_index in kf.split(y):
      X_train, X_test = X[train_index], X[test_index]
      y_train, y_test = y[train_index], y[test_index]
      model.fit(X_train, y_train)
      score = model.score(X_test, y_test)
      scores_cv.append(score)
      print("Score:", score)
  mean_score = np.mean(scores_cv)
  print(f"{mean_score=}")
  ##
  #!ä½¿ç”¨cross_val_score
  #æ„é€ cvå™¨çš„æ—¶å€™ä¸éœ€è¦ä¼ å…¥æ•°æ®é›†
  ss_cv = ShuffleSplit(n_splits=3, test_size=0.2, random_state=42)
  kf_cv=KFold(n_splits=3,shuffle=True,random_state=42)
  scores = cross_val_score(
      model,
      X,
      y,
      #cv=5,
      #cv=ss_cv,
      cv=kf_cv,
      verbose=3,
  )
  #cvå–æ•´æ•°æ—¶,é‡‡ç”¨çš„ééšæœºåŒ–çš„kfoldæ–¹æ³•åˆ’åˆ†,ä¸æ˜¯å¾ˆå¯é 
  #cvå»ºè®®é€‰ç”¨éšæœºåŒ–çš„(StratifiedShuffleSplitæœ€ä¸ºé«˜çº§)
  #cvå–kfoldå¯¹è±¡æ—¶,æˆ‘ä»¬å¯ä»¥é€‰æ‹©shuffle=True,ä½¿å¾—æ‰€æœ‰æ ·æœ¬éƒ½èƒ½å¤Ÿå‚ä¸è®­ç»ƒé›†/æµ‹è¯•é›†
  print("Scores:", scores)
  print("Mean score:", scores.mean())
  
  
  ##
  
  # ä½¿ç”¨å†³ç­–æ ‘æ¨¡å‹è¿›è¡Œäº¤å‰éªŒè¯ï¼Œå¹¶å¯¹æ•°æ®é›†è¿›è¡ŒéšæœºåŒ–æ“ä½œ
  model = DecisionTreeClassifier()
  ss_cv = ShuffleSplit(n_splits=3, test_size=0.2, random_state=42)
  
  print("cv: ", ss_cv)
  
  # ssr=ss_cv.split(X,y)
  # for train_index,test_index in ssr:
  #     train_index,test_index=np.array(train_index),np.array(test_index)
  #     print(train_index.shape,test_index.shape)
  
  scores = cross_val_score(model, X, y, cv=ss_cv, verbose=True)
  print("Scores:", scores)
  print("Mean score:", scores.mean())
  ```

  

