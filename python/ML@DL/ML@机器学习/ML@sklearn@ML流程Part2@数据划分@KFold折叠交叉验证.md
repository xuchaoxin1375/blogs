[toc]

# ML@sklearn@MLæµç¨‹Part2@æ•°æ®åˆ’åˆ†@KFoldæŠ˜å äº¤å‰éªŒè¯

## Model evaluation

### æ•°æ®åˆ’åˆ†

- | `æ•°æ®åˆ’åˆ†`    | æ•°æ®é›†X          | æ ‡ç­¾é›†y          |
  | ------------- | ---------------- | ---------------- |
  | è®­ç»ƒé›†X_train | `X[train_index]` | `y[train_index]` |
  | æµ‹è¯•é›†y_train | `X[test_index]`  | `y[test_index]`  |

- åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šå°†æ•°æ®é›†åˆ†æˆè‡ªå˜é‡å’Œç›®æ ‡å˜é‡ä¸¤éƒ¨åˆ†ã€‚

  è‡ªå˜é‡æ˜¯ä¸€ç»„ç”¨äºé¢„æµ‹ç›®æ ‡å˜é‡çš„å˜é‡ï¼Œä¹Ÿè¢«ç§°ä¸ºç‰¹å¾æˆ–è¾“å…¥ã€‚è‡ªå˜é‡é€šå¸¸æ˜¯ä¸€ä¸ªçŸ©é˜µï¼Œå…¶ä¸­æ¯ä¸€è¡Œè¡¨ç¤ºä¸€ä¸ªæ ·æœ¬ï¼Œæ¯ä¸€åˆ—è¡¨ç¤ºä¸€ä¸ªç‰¹å¾ã€‚

  ç›®æ ‡å˜é‡æ˜¯æˆ‘ä»¬éœ€è¦é¢„æµ‹çš„å˜é‡ï¼Œä¹Ÿè¢«ç§°ä¸ºè¾“å‡ºã€‚ç›®æ ‡å˜é‡é€šå¸¸æ˜¯ä¸€ä¸ªå‘é‡ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ å¯¹åº”ä¸€ä¸ªæ ·æœ¬çš„è¾“å‡ºå€¼ã€‚

  ä¾‹å¦‚ï¼Œåœ¨å›å½’é—®é¢˜ä¸­ï¼Œè‡ªå˜é‡é€šå¸¸æ˜¯ä¸€ä¸ªåŒ…å«å¤šä¸ªç‰¹å¾çš„çŸ©é˜µï¼Œä¾‹å¦‚æˆ¿å±‹çš„é¢ç§¯ã€å§å®¤æ•°é‡ã€åœ°ç†ä½ç½®ç­‰ç­‰ï¼›è€Œç›®æ ‡å˜é‡é€šå¸¸æ˜¯ä¸€ä¸ªè¡¨ç¤ºæˆ¿å±‹ä»·æ ¼çš„å‘é‡ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ å¯¹åº”ä¸€ä¸ªæˆ¿å±‹çš„ä»·æ ¼ã€‚

  åœ¨ç›‘ç£å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šå°†æ•°æ®é›†åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå…¶ä¸­è®­ç»ƒé›†ç”¨äºè®­ç»ƒæ¨¡å‹ï¼Œæµ‹è¯•é›†ç”¨äºè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚è®­ç»ƒé›†ä¸­åŒ…å«è‡ªå˜é‡å’Œç›®æ ‡å˜é‡ï¼Œè€Œæµ‹è¯•é›†ä¸­åªåŒ…å«è‡ªå˜é‡ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹ï¼Œå¹¶å°†é¢„æµ‹ç»“æœä¸æµ‹è¯•é›†ä¸­çš„ç›®æ ‡å˜é‡è¿›è¡Œæ¯”è¾ƒï¼Œä»¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚

  éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè‡ªå˜é‡å’Œç›®æ ‡å˜é‡çš„æ•°é‡å’Œç±»å‹å–å†³äºå…·ä½“çš„é—®é¢˜å’Œæ•°æ®é›†ï¼Œä¸åŒçš„é—®é¢˜å¯èƒ½éœ€è¦ä¸åŒæ•°é‡å’Œç±»å‹çš„è‡ªå˜é‡å’Œç›®æ ‡å˜é‡ã€‚

- [Training, validation, and test data sets - Wikipedia](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets)
- In [machine learning](https://en.wikipedia.org/wiki/Machine_learning), a common task is the study and construction of [algorithms](https://en.wikipedia.org/wiki/Algorithm) that can learn from and make predictions on [data](https://en.wikipedia.org/wiki/Data). Such algorithms function by making data-driven predictions or decisions, through building a [mathematical model](https://en.wikipedia.org/wiki/Mathematical_model) from input data. These input data used to build the model are usually divided into multiple [data sets](https://en.wikipedia.org/wiki/Data_set). In particular, three data sets are commonly used in different stages of the creation of the model:
  -  training, validation, and test sets.
- The model is initially fit on a **training data set**, which is a set of examples used to fit the parameters (e.g. weights of connections between neurons in [artificial neural networks](https://en.wikipedia.org/wiki/Artificial_neural_networks)) of the model .The model (e.g. a [naive Bayes classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)) is trained on the training data set using a [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning) method, for example using optimization methods such as [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) or [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent). In practice, the training data set often consists of pairs of an input [vector](https://en.wikipedia.org/wiki/Array_data_structure) (or scalar) and the corresponding output vector (or scalar), where the answer key is commonly denoted as the *target* (or *label*). The current model is run with the training data set and produces a result, which is then compared with the *target*, for each input vector in the training data set. Based on the result of the comparison and the specific learning algorithm being used, the parameters of the model are adjusted. The model fitting can include both [variable selection](https://en.wikipedia.org/wiki/Feature_selection) and parameter [estimation](https://en.wikipedia.org/wiki/Estimation_theory).
- Successively, the fitted model is used to predict the responses for the observations in a second data set called the **validation data set**. The validation data set provides an unbiased evaluation of a model fit on the training data set while tuning the model's [hyperparameters](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)) (e.g. the number of hidden unitsâ€”layers and layer widthsâ€”in a neural network  ). Validation datasets can be used for [regularization](https://en.wikipedia.org/wiki/Regularization_(mathematics)) by [early stopping](https://en.wikipedia.org/wiki/Early_stopping) (stopping training when the error on the validation data set increases, as this is a sign of [over-fitting](https://en.wikipedia.org/wiki/Overfitting) to the training data set). This simple procedure is complicated in practice by the fact that the validation dataset's error may fluctuate during training, producing multiple local minima. This complication has led to the creation of many ad-hoc rules for deciding when over-fitting has truly begun.  
- Finally, the **test data set** is a data set used to provide an unbiased evaluation of a *final* model fit on the training data set.  If the data in the test data set has never been used in training (for example in [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics))), the test data set is also called a **holdout data set**. The term "validation set" is sometimes used instead of "test set" in some literature (e.g., if the original data set was partitioned into only two subsets, the test set might be referred to as the validation set).
- Deciding the sizes and strategies for data set division in training, test and validation sets is very dependent on the problem and data available.

### sklearnä¸­çš„æ¨¡å‹è¯„ä¼°

- Fitting a model to some data does not entail that it will predict well on unseen data.
- This needs to be directly evaluated. We have just seen the [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split) helper that splits a dataset into train and test sets, but `scikit-learn` provides many other tools for model evaluation, in particular for [cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation).
- We here briefly show how to perform a 5-fold cross-validation procedure, using the [`cross_validate`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate) helper. 
  - Note that it is also possible to manually iterate over the folds, use different data splitting strategies, and use custom scoring functions. 
  - Please refer to our [User Guide](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation) for more details:

### demo

- ```python
  from sklearn.datasets import make_regression
  from sklearn.linear_model import LinearRegression
  from sklearn.model_selection import cross_validate
  
  X, y = make_regression(n_samples=1000, noise=50,random_state=0)
  # é»˜è®¤ç»´æ•°ç»´100
  # X.shape,y.shape=1000,100
  
  lr = LinearRegression()
  result = cross_validate(lr, X, y)  # defaults to 5-fold CV
  result['test_score']  # r_squared score is high because dataset is easy
  result = cross_validate(lr, X, y, cv=5, return_train_score=True)
  for v in result.items():
      print(v)
  # print(result)
  ```

  - ```bash
    0.9736842105263158
    ('fit_time', array([0.01099563, 0.00600886, 0.00399899, 0.00663686, 0.00399613]))
    ('score_time', array([0.00099921, 0.0009954 , 0.00100064, 0.        , 0.00100303]))
    ('test_score', array([0.90468242, 0.8604173 , 0.89786489, 0.9145173 , 0.89278255]))
    ('train_score', array([0.92046141, 0.92835467, 0.92186638, 0.91778192, 0.92238646]))
    ```

  - å¦‚æœè®¾ç½®`noise=0`,é‡æ–°è¿è¡Œ,ç»“æœå½¢å¦‚

  - ```bash
    ('fit_time', array([0.00899696, 0.00603986, 0.00499964, 0.07801867, 0.0049994 ]))
    ('score_time', array([0.00099921, 0.        , 0.0010035 , 0.00197124, 0.00100207]))
    ('test_score', array([1., 1., 1., 1., 1.]))
    ('train_score', array([1., 1., 1., 1., 1.]))
    ```

  - åœ¨äººå·¥å›å½’æ•°æ®é›†ä¸­ï¼Œ`noise`å‚æ•°ç”¨äºæ§åˆ¶ç›®æ ‡å˜é‡ä¸­å™ªå£°çš„å¼ºåº¦ã€‚å…·ä½“æ¥è¯´ï¼Œå½“æˆ‘ä»¬ç”Ÿæˆä¸€ä¸ªäººå·¥å›å½’æ•°æ®é›†æ—¶ï¼Œæˆ‘ä»¬ä¼šæŒ‰ç…§ä¸€å®šçš„è§„åˆ™ç”Ÿæˆè‡ªå˜é‡`X`å’Œå› å˜é‡`y`ï¼Œå…¶ä¸­`y`çš„å€¼æ˜¯æ ¹æ®æŸç§å‡½æ•°å…³ç³»è®¡ç®—å‡ºæ¥çš„ã€‚å¦‚æœ`noise`å‚æ•°ä¸º0ï¼Œåˆ™ç”Ÿæˆçš„`y`å€¼å®Œå…¨ç¬¦åˆè¿™ç§å‡½æ•°å…³ç³»ï¼Œæ²¡æœ‰ä»»ä½•éšæœºå™ªå£°ã€‚è€Œå½“`noise`å‚æ•°å¤§äº0æ—¶ï¼Œç”Ÿæˆçš„`y`å€¼ä¼šå—åˆ°ä¸€å®šç¨‹åº¦çš„éšæœºå™ªå£°çš„å½±å“ã€‚

    åœ¨Scikit-learnä¸­ï¼Œ`noise`å‚æ•°çš„å€¼è¢«è§£é‡Šä¸ºç”Ÿæˆçš„æ•°æ®é›†ä¸­ç›®æ ‡å˜é‡ï¼ˆå³`y`ï¼‰çš„æ ‡å‡†å·®ã€‚å…·ä½“æ¥è¯´ï¼Œå½“`noise`å‚æ•°ä¸º0æ—¶ï¼Œç”Ÿæˆçš„æ•°æ®é›†ä¸­ç›®æ ‡å˜é‡çš„æ ‡å‡†å·®ä¸º0ï¼Œå³æ‰€æœ‰çš„ç›®æ ‡å˜é‡å€¼å®Œå…¨ç¬¦åˆå‡½æ•°å…³ç³»ï¼›è€Œå½“`noise`å‚æ•°ä¸ä¸º0æ—¶ï¼Œç”Ÿæˆçš„æ•°æ®é›†ä¸­ç›®æ ‡å˜é‡çš„æ ‡å‡†å·®å°†ä¼šå¤§äº0ï¼Œå³ç›®æ ‡å˜é‡å€¼ä¸å†å®Œå…¨ç¬¦åˆå‡½æ•°å…³ç³»ï¼Œè€Œæ˜¯å—åˆ°ä¸€å®šç¨‹åº¦çš„éšæœºå™ªå£°çš„å½±å“ã€‚

    éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œ`noise`å‚æ•°åªå½±å“ç”Ÿæˆçš„æ•°æ®é›†ä¸­ç›®æ ‡å˜é‡çš„éšæœºå™ªå£°ï¼Œè€Œä¸å½±å“è‡ªå˜é‡ã€‚å› æ­¤ï¼Œå½“æˆ‘ä»¬ç”Ÿæˆäººå·¥å›å½’æ•°æ®é›†æ—¶ï¼Œå¦‚æœéœ€è¦ä½¿æ•°æ®é›†æ›´åŠ çœŸå®ï¼Œå¯ä»¥å¢åŠ `noise`å‚æ•°çš„å€¼ï¼›åä¹‹ï¼Œå¦‚æœéœ€è¦ä½¿æ•°æ®é›†æ›´åŠ è§„å¾‹ï¼Œå¯ä»¥å°†`noise`å‚æ•°è®¾ç½®ä¸ºè¾ƒå°çš„å€¼æˆ–è€…0ã€‚

- è¿™æ®µä»£ç ä½¿ç”¨äº†Scikit-learnåº“ä¸­çš„`make_regression`å‡½æ•°ç”Ÿæˆä¸€ä¸ªåŒ…å«1000ä¸ªæ ·æœ¬çš„å›å½’æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨çº¿æ€§å›å½’æ¨¡å‹å¯¹æ•°æ®é›†è¿›è¡Œæ‹Ÿåˆå’Œè¯„ä¼°ã€‚


- å…·ä½“æ¥è¯´ï¼Œè¿™æ®µä»£ç çš„åŠŸèƒ½å¦‚ä¸‹ï¼š

  1. ä»Scikit-learnåº“ä¸­å¯¼å…¥`make_regression`å‡½æ•°ã€`LinearRegression`æ¨¡å‹å’Œ`cross_validate`å‡½æ•°ã€‚
  2. ä½¿ç”¨`make_regression`å‡½æ•°ç”Ÿæˆä¸€ä¸ªåŒ…å«1000ä¸ªæ ·æœ¬çš„å›å½’æ•°æ®é›†ï¼Œå…¶ä¸­`n_samples=1000`è¡¨ç¤ºæ•°æ®é›†ä¸­åŒ…å«1000ä¸ªæ ·æœ¬ï¼Œ`random_state=0`è¡¨ç¤ºä½¿ç”¨ç›¸åŒçš„éšæœºç§å­ç”Ÿæˆæ•°æ®é›†ï¼Œä»¥ç¡®ä¿ç»“æœçš„å¯é‡å¤æ€§ã€‚
  3. åˆ›å»ºä¸€ä¸ªçº¿æ€§å›å½’æ¨¡å‹ï¼Œå¹¶å°†å…¶èµ‹å€¼ç»™å˜é‡`lr`ã€‚
  4. ä½¿ç”¨`cross_validate`å‡½æ•°å¯¹çº¿æ€§å›å½’æ¨¡å‹è¿›è¡Œäº¤å‰éªŒè¯ï¼Œå…¶ä¸­`X`å’Œ`y`åˆ†åˆ«è¡¨ç¤ºæ•°æ®é›†çš„è‡ªå˜é‡å’Œå› å˜é‡ï¼Œ`result`ä¿å­˜äº†äº¤å‰éªŒè¯çš„ç»“æœã€‚
  5. è¾“å‡ºäº¤å‰éªŒè¯çš„æµ‹è¯•é›†å¾—åˆ†ï¼Œå…¶ä¸­`result['test_score']`è¡¨ç¤ºæµ‹è¯•é›†å¾—åˆ†ï¼Œå› ä¸ºæ•°æ®é›†æ¯”è¾ƒç®€å•ï¼Œæ‰€ä»¥å¾—åˆ†æ¯”è¾ƒé«˜ï¼Œé€šå¸¸ä½¿ç”¨Rå¹³æ–¹ä½œä¸ºè¯„ä¼°æŒ‡æ ‡ã€‚



## K-fold cross-validationğŸˆ

- KæŠ˜äº¤å‰éªŒè¯ï¼ˆK-fold cross-validationï¼‰æ˜¯ä¸€ç§å¸¸ç”¨çš„æ•°æ®é›†åˆ’åˆ†å’Œæ¨¡å‹éªŒè¯æŠ€æœ¯ï¼Œå¯ä»¥ç”¨äºè¯„ä¼°æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½å’Œè¿›è¡Œæ¨¡å‹çš„é€‰æ‹©å’Œè°ƒä¼˜ã€‚
- KæŠ˜äº¤å‰éªŒè¯çš„åŸºæœ¬æ€æƒ³æ˜¯å°†æ•°æ®é›†åˆ†æˆKä¸ªå­é›†ï¼ˆä¸€èˆ¬æ˜¯å‡ç­‰åˆ’åˆ†ï¼‰ï¼Œç„¶åä½¿ç”¨å…¶ä¸­K-1ä¸ªå­é›†ä½œä¸ºè®­ç»ƒé›†ï¼Œä½™ä¸‹çš„1ä¸ªå­é›†ä½œä¸ºéªŒè¯é›†ï¼Œè¿›è¡Œæ¨¡å‹çš„è®­ç»ƒå’ŒéªŒè¯ï¼Œé‡å¤Kæ¬¡ï¼Œæ¯æ¬¡ä½¿ç”¨ä¸åŒçš„éªŒè¯é›†ï¼Œæœ€ç»ˆå°†Kæ¬¡éªŒè¯çš„ç»“æœè¿›è¡Œå¹³å‡æˆ–åŠ æƒå¹³å‡ï¼Œå¾—åˆ°æœ€ç»ˆçš„æ€§èƒ½æŒ‡æ ‡ã€‚

- KæŠ˜äº¤å‰éªŒè¯çš„ä¼˜ç‚¹åœ¨äºï¼š
  1. å¯ä»¥å……åˆ†åˆ©ç”¨æ•°æ®é›†ä¸­çš„ä¿¡æ¯ï¼Œé¿å…è¿‡æ‹Ÿåˆæˆ–æ¬ æ‹Ÿåˆçš„é—®é¢˜ã€‚
  2. å¯ä»¥å¯¹æ¨¡å‹çš„æ€§èƒ½è¿›è¡Œæ›´å‡†ç¡®çš„è¯„ä¼°ï¼Œå‡å°è¯„ä¼°è¯¯å·®ã€‚
  3. å¯ä»¥åœ¨æœ‰é™çš„æ•°æ®é›†ä¸­ï¼Œæ‰©å¤§è®­ç»ƒé›†çš„è§„æ¨¡ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

### K-fold

- [sklearn.model_selection.KFold â€” scikit-learn  documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold)
- ä»¥ä¸‹ä»£ç æ¼”ç¤ºäº†KFoldæ˜¯æ€ä¹ˆå·¥ä½œçš„

#### eg

- ```python
  import numpy as np
  from sklearn.model_selection import KFold
  X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
  y = np.array([1, 2, 3, 4])
  kf = KFold(n_splits=2)
  n=kf.get_n_splits(X)
  print(n)
  print(kf)
  
  for i, (train_index, test_index) in enumerate(kf.split(X)):
      print(f"Fold {i}:")
      print(f"  Train: index={train_index}")
      print(f"  Test:  index={test_index}")
  ```

  - ```bash
    2
    KFold(n_splits=2, random_state=None, shuffle=False)
    Fold 0:
      Train: index=[2 3]
      Test:  index=[0 1]
    Fold 1:
      Train: index=[0 1]
      Test:  index=[2 3]
    ```

    

- è¿™æ®µä»£ç ä½¿ç”¨äº†Scikit-learnåº“ä¸­çš„`KFold`ç±»è¿›è¡ŒKæŠ˜äº¤å‰éªŒè¯ã€‚å…·ä½“æµç¨‹å¦‚ä¸‹ï¼š

- é¦–å…ˆï¼Œåˆ›å»ºä¸€ä¸ªåŒ…å«4ä¸ªæ ·æœ¬çš„æ•°æ®é›†`X`å’Œå¯¹åº”çš„æ ‡ç­¾`y`ï¼Œå…¶ä¸­`X`æ˜¯ä¸€ä¸ªäºŒç»´æ•°ç»„

  - æ¯è¡Œè¡¨ç¤ºä¸€ä¸ªæ ·æœ¬ï¼Œæ¯åˆ—è¡¨ç¤ºä¸€ä¸ªç‰¹å¾ã€‚
  - ç„¶åï¼Œåˆ›å»ºä¸€ä¸ª`KFold`å¯¹è±¡`kf`ï¼Œå¹¶å°†æ•°æ®é›†`X`ä¼ é€’ç»™å®ƒã€‚

- åœ¨`KFold`å¯¹è±¡ä¸­ï¼Œè®¾ç½®`n_splits=2`ï¼Œè¡¨ç¤ºå°†æ•°æ®é›†åˆ’åˆ†ä¸º2ä¸ªå­é›†ã€‚

  - è°ƒç”¨`get_n_splits`æ–¹æ³•å¯ä»¥è·å–å­é›†çš„æ•°é‡ã€‚

- æ¥ä¸‹æ¥ï¼Œä½¿ç”¨`kf.split(X)`æ–¹æ³•å¯¹æ•°æ®é›†è¿›è¡Œåˆ’åˆ†ï¼Œå¹¶éå†åˆ’åˆ†çš„ç»“æœã€‚

  - åˆ’åˆ†æ“ä½œåªéœ€è¦åˆ’åˆ†å’Œåˆ†ç»„ç´¢å¼•å³å¯,è®¿é—®æ•°æ®çš„æ—¶å€™æ ¹æ®åˆ†å¥½çš„ç´¢å¼•å»è®¿é—®å³å¯
  - åœ¨æ¯ä¸ªæŠ˜å ä¸­ï¼Œ`KFold`ç±»è¿”å›ä¸€ä¸ªå…ƒç»„`(train_index, test_index)`ï¼Œå…¶ä¸­`train_index`è¡¨ç¤ºç”¨äºè®­ç»ƒçš„æ ·æœ¬ç´¢å¼•ï¼Œ`test_index`è¡¨ç¤ºç”¨äºæµ‹è¯•çš„æ ·æœ¬ç´¢å¼•ã€‚
  - ğŸˆå°†kf.split(X)æŠ½å–çš„æ‰€æœ‰`test_index`åˆå¹¶èµ·æ¥(å¹¶æ’åº),å¾—åˆ°çš„åºåˆ—ç›¸å½“äº`range(len(X))`

- åœ¨å¾ªç¯ä¸­ï¼Œä½¿ç”¨`enumerate`å‡½æ•°è·å–å½“å‰æŠ˜å çš„ç´¢å¼•`i`ï¼Œå¹¶è¾“å‡ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ç´¢å¼•ã€‚å…·ä½“æ¥è¯´ï¼Œå°†`train_index`å’Œ`test_index`æ‰“å°å‡ºæ¥ï¼Œå…¶ä¸­`train_index`å’Œ`test_index`åˆ†åˆ«è¡¨ç¤ºå½“å‰æŠ˜å ä¸­ç”¨äºè®­ç»ƒå’Œæµ‹è¯•çš„æ ·æœ¬ç´¢å¼•ã€‚

  - ```python
    # æ ¹æ®åˆ†ç»„å¥½çš„ç´¢å¼•,ä½œæ•°æ®åˆ’åˆ†:
    # æ ¹æ®ç´¢å¼•åˆ’åˆ†æ•°æ®é›†
    X_train, X_test = X[train_index], X[test_index]
    # æ ¹æ®ç´¢å¼•åˆ’åˆ†æ ‡ç­¾
    y_train, y_test = y[train_index], y[test_index]
    ```

    - å¾—ç›Šäºnumpyæ•°ç»„çš„å…ƒç´ è®¿é—®æ–¹å¼,ç›¸å…³è¯­å¥ååˆ†ç®€æ´

  - train_indexå’Œtest_indexæ˜¯ç”±æŸç§äº¤å‰éªŒè¯æ–¹æ³•ï¼ˆå¦‚KæŠ˜äº¤å‰éªŒè¯ï¼‰ç”Ÿæˆçš„ç´¢å¼•æ•°ç»„ï¼Œç”¨äºå°†**æ•°æ®é›†X**å’Œ**æ ‡ç­¾é›†y**åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚(æ•°æ®é›†å’Œæ ‡ç­¾é›†çš„åˆ’åˆ†ä½¿ç”¨çš„ç´¢å¼•åºåˆ—æ˜¯å¯¹åº”ä¸€è‡´çš„)

  - åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼ŒXå’Œyæ˜¯åŸå§‹æ•°æ®é›†å’Œæ ‡ç­¾é›†ï¼Œtrain_indexå’Œtest_indexæ˜¯ç”±KæŠ˜äº¤å‰éªŒè¯æ–¹æ³•ç”Ÿæˆçš„ç´¢å¼•æ•°ç»„ã€‚

- å¯ä»¥çœ‹åˆ°ï¼Œæ•°æ®é›†è¢«åˆ’åˆ†ä¸ºäº†ä¸¤ä¸ªæŠ˜å ï¼Œæ¯ä¸ªæŠ˜å ä¸­è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ç´¢å¼•æ˜¯ä¸åŒçš„ã€‚ä¹Ÿå¯ä»¥é€šè¿‡`KFold`å¯¹è±¡çš„å…¶ä»–å±æ€§å’Œæ–¹æ³•æ¥æ§åˆ¶äº¤å‰éªŒè¯çš„æ–¹å¼ï¼Œå¦‚è®¾ç½®éšæœºç§å­ã€è¿›è¡Œåˆ†å±‚æŠ½æ ·ç­‰ã€‚

#### eg

- ```python
  X=np.random.randint(5,size=(12,3))
  y=np.random.choice(100,size=len(X))
  X,y
  
  kf2 = KFold(n_splits=3)
  n=kf2.get_n_splits(X)
  print(n)
  print(kf2)
  
  for i, (train_index, test_index) in enumerate(kf2.split(X)):
      print(f"Fold {i}:")
      print(f"  Train: index={train_index}")
      print(f"  Test:  index={test_index}")
      merge=np.concatenate((train_index,test_index))
      merge_sort=merge
      merge_sort.sort()
      print(f'{merge_sort=}')
  ```

  - ```bash
    3
    KFold(n_splits=3, random_state=None, shuffle=False)
    Fold 0:
      Train: index=[ 4  5  6  7  8  9 10 11]
      Test:  index=[0 1 2 3]
    merge_sort=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
    Fold 1:
      Train: index=[ 0  1  2  3  8  9 10 11]
      Test:  index=[4 5 6 7]
    merge_sort=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
    Fold 2:
      Train: index=[0 1 2 3 4 5 6 7]
      Test:  index=[ 8  9 10 11]
    merge_sort=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
    ```

- å†çœ‹ä¸‹shuffleå‚æ•°çš„æ•ˆæœ(éšæœºæ‰“ä¹±é¡ºåºç´¢å¼•,ç„¶ååˆ†ç»„)

  - ```python
    kf2 = KFold(n_splits=3,shuffle=True)
    n=kf2.get_n_splits(X)
    print(n)
    print(kf2)
    
    for i, (train_index, test_index) in enumerate(kf2.split(X)):
        print(f"Fold {i}:")
        print(f"  Train: index={train_index}")
        print(f"  Test:  index={test_index}")
        merge=np.concatenate((train_index,test_index))
        merge_sort=merge
        merge_sort.sort()
        print(f'{merge_sort=}')
    ```

    - ```bash
      3
      KFold(n_splits=3, random_state=None, shuffle=True)
      Fold 0:
        Train: index=[ 3  4  5  6  7  8  9 11]
        Test:  index=[ 0  1  2 10]
      merge_sort=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
      Fold 1:
        Train: index=[ 0  1  2  5  6  8 10 11]
        Test:  index=[3 4 7 9]
      merge_sort=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
      Fold 2:
        Train: index=[ 0  1  2  3  4  7  9 10]
        Test:  index=[ 5  6  8 11]
      merge_sort=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
      ```

      

#### eg KFold@K-fold cross-validation

- ä¸‹é¢çš„ä»£ç æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨Scikit-learnåº“ä¸­çš„`KFold`ç±»è¿›è¡ŒKæŠ˜äº¤å‰éªŒè¯ï¼š

  - ```python
    from sklearn.datasets import make_classification
    from sklearn.linear_model import LogisticRegression
    from sklearn.model_selection import KFold
    from sklearn.metrics import accuracy_score
    
    X, y = make_classification(n_samples=1000, random_state=0)
    kf = KFold(n_splits=5, shuffle=True)
    
    for train_index, test_index in kf.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]
        lr = LogisticRegression()
        lr.fit(X_train, y_train)
        y_pred = lr.predict(X_test)
        acc = accuracy_score(y_test, y_pred)
        print("Accuracy: {:.2f}".format(acc))
    ```

- è¿™æ®µä»£ç ä½¿ç”¨Scikit-learnåº“ç”Ÿæˆä¸€ä¸ªäºŒåˆ†ç±»æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨Logisticå›å½’æ¨¡å‹è¿›è¡Œåˆ†ç±»ã€‚ç„¶åä½¿ç”¨KæŠ˜äº¤å‰éªŒè¯æ–¹æ³•æ¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚å…·ä½“è¿‡ç¨‹å¦‚ä¸‹ï¼š

  1. ä½¿ç”¨ `make_classification` å‡½æ•°ç”Ÿæˆä¸€ä¸ªåŒ…å«1000ä¸ªæ ·æœ¬çš„äºŒåˆ†ç±»æ•°æ®é›†ï¼Œå…¶ä¸­ç‰¹å¾æ•°ä¸ºé»˜è®¤å€¼(20)ï¼Œç±»åˆ«æ•°ä¸º2ï¼ŒéšæœºçŠ¶æ€ä¸º0ã€‚
  2. ä½¿ç”¨ `KFold` å‡½æ•°å°†æ•°æ®é›†åˆ†æˆ5ä¸ªäº’æ–¥çš„å­é›†ï¼Œæ¯ä¸ªå­é›†éƒ½å¯ä»¥ä½œä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ä¸€éƒ¨åˆ†ã€‚
  3. å¯¹äºæ¯ä¸ªå­é›†ï¼Œå°†å…¶ä½œä¸ºæµ‹è¯•é›†ï¼Œä½™ä¸‹çš„æ•°æ®ä½œä¸ºè®­ç»ƒé›†ã€‚
  4. åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒä¸€ä¸ªLogisticå›å½’æ¨¡å‹ã€‚
  5. åœ¨æµ‹è¯•é›†ä¸Šé¢„æµ‹æ¨¡å‹çš„è¾“å‡ºï¼Œå¹¶ä½¿ç”¨ `accuracy_score` å‡½æ•°è®¡ç®—é¢„æµ‹çš„å‡†ç¡®ç‡ã€‚
  6. æ‰“å°æ¯æ¬¡äº¤å‰éªŒè¯çš„å‡†ç¡®ç‡ã€‚
  7. æœ€ç»ˆè¾“å‡º5æ¬¡äº¤å‰éªŒè¯çš„å‡†ç¡®ç‡å‡å€¼å’Œæ ‡å‡†å·®ã€‚

  è¿™æ®µä»£ç å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨KæŠ˜äº¤å‰éªŒè¯æ–¹æ³•æ¥è¯„ä¼°Logisticå›å½’æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶è®¡ç®—æ¨¡å‹çš„å‡†ç¡®ç‡ã€‚

- å¯ä»¥æ ¹æ®å…·ä½“æƒ…å†µé€‰æ‹©åˆé€‚çš„Kå€¼å’ŒéªŒè¯æŒ‡æ ‡æ¥è¿›è¡Œæ¨¡å‹è¯„ä¼°ã€‚

## model_selectionğŸˆğŸ˜

- [model_selection](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection)

  - **User guide:** See the [Cross-validation: evaluating estimator performance](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation), [Tuning the hyper-parameters of an estimator](https://scikit-learn.org/stable/modules/grid_search.html#grid-search) and [Learning curve](https://scikit-learn.org/stable/modules/learning_curve.html#learning-curve) sections for further details.

- Scikit-learnåº“ä¸­çš„`sklearn.model_selection`æ¨¡å—æä¾›äº†ä¸€ç³»åˆ—ç”¨äº**æ¨¡å‹é€‰æ‹©å’Œè¯„ä¼°**çš„å·¥å…·ã€‚è¯¥æ¨¡å—ä¸­åŒ…å«çš„ç±»å’Œå‡½æ•°å¯ä»¥å¸®åŠ©æˆ‘ä»¬è¿›è¡Œ**æ•°æ®é›†çš„åˆ’åˆ†ã€äº¤å‰éªŒè¯ã€è¶…å‚æ•°ä¼˜åŒ–ç­‰æ“ä½œ**ï¼Œä»è€Œé€‰æ‹©æœ€ä¼˜çš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚

- è¯¥æ¨¡å—ä¸­ä¸€äº›å¸¸ç”¨çš„ç±»å’Œå‡½æ•°å¦‚ä¸‹ï¼š

  - `train_test_split`: ç”¨äºå°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚	
  - `KFold`: ç”¨äºè¿›è¡ŒKæŠ˜äº¤å‰éªŒè¯ã€‚
  - `StratifiedKFold`: ç”¨äºè¿›è¡Œåˆ†å±‚KæŠ˜äº¤å‰éªŒè¯ï¼Œå¯ä»¥å¤„ç†ä¸å¹³è¡¡æ•°æ®é›†ã€‚
  - `GridSearchCV`: ç”¨äºè¿›è¡Œç½‘æ ¼æœç´¢å’Œäº¤å‰éªŒè¯ï¼Œå¯»æ‰¾æœ€ä¼˜çš„è¶…å‚æ•°ç»„åˆã€‚
  - `RandomizedSearchCV`: ç”¨äºè¿›è¡Œéšæœºæœç´¢å’Œäº¤å‰éªŒè¯ï¼Œå¯»æ‰¾æœ€ä¼˜çš„è¶…å‚æ•°ç»„åˆã€‚
  - `cross_val_score`: ç”¨äºå¯¹æ¨¡å‹è¿›è¡Œäº¤å‰éªŒè¯å¹¶è®¡ç®—æ€§èƒ½æŒ‡æ ‡çš„å¹³å‡å€¼ã€‚ğŸˆ
  - `learning_curve`: ç”¨äºç»˜åˆ¶å­¦ä¹ æ›²çº¿ï¼Œå¸®åŠ©æˆ‘ä»¬åˆ¤æ–­æ¨¡å‹æ˜¯å¦æ¬ æ‹Ÿåˆæˆ–è¿‡æ‹Ÿåˆã€‚
  - `validation_curve`: ç”¨äºç»˜åˆ¶éªŒè¯æ›²çº¿ï¼Œå¸®åŠ©æˆ‘ä»¬é€‰æ‹©æœ€ä¼˜çš„è¶…å‚æ•°ã€‚

  è¿™äº›ç±»å’Œå‡½æ•°å¯ä»¥æ ¹æ®éœ€è¦è¿›è¡Œç»„åˆä½¿ç”¨ï¼Œä»¥å®Œæˆä¸åŒçš„æ¨¡å‹é€‰æ‹©å’Œè¯„ä¼°ä»»åŠ¡ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`train_test_split`å°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œç„¶åä½¿ç”¨`GridSearchCV`å¯»æ‰¾æœ€ä¼˜çš„è¶…å‚æ•°ç»„åˆï¼Œæœ€åä½¿ç”¨`cross_val_score`å¯¹æ¨¡å‹è¿›è¡Œäº¤å‰éªŒè¯å¹¶è®¡ç®—æ€§èƒ½æŒ‡æ ‡çš„å¹³å‡å€¼ã€‚

### train_test_split

- [sklearn.model_selection.train_test_split â€” scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)

- Split arrays or matrices into random train and test subsets.

  Quick utility that wraps input validation, `next(ShuffleSplit().split(X, y))`, and application to input data into a single call for splitting (and optionally subsampling) data into a one-liner.

  å°†æ•°ç»„æˆ–çŸ©é˜µéšæœºåˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„å¿«é€Ÿå®ç”¨ç¨‹åºã€‚

  è¿™æ˜¯ä¸€ä¸ªå¿«æ·å®ç”¨ç¨‹åºï¼Œå®ƒå°†è¾“å…¥éªŒè¯ã€next(ShuffleSplit().split(X, y))å’Œå°†å…¶åº”ç”¨äºè¾“å…¥æ•°æ®çš„æ“ä½œå°è£…æˆä¸€ä¸ªå•ç‹¬çš„å‡½æ•°è°ƒç”¨ï¼Œç”¨äºå°†æ•°æ®æ‹†åˆ†ï¼ˆå’Œå¯é€‰åœ°è¿›è¡Œå­é‡‡æ ·ï¼‰ä¸ºä¸€è¡Œä»£ç ã€‚

### train_test_split vs KFold

- `train_test_split`å’Œ`KFold`éƒ½æ˜¯ç”¨äºæœºå™¨å­¦ä¹ ä¸­çš„æ•°æ®æ‹†åˆ†å’Œäº¤å‰éªŒè¯çš„å·¥å…·ï¼Œä½†å®ƒä»¬çš„ä½¿ç”¨åœºæ™¯ç•¥æœ‰ä¸åŒã€‚

- `train_test_split`æ˜¯ç”¨äºå°†æ•°æ®é›†æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„å·¥å…·ã€‚å®ƒå¯ä»¥å°†æ•°æ®é›†éšæœºåœ°æ‹†åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼Œä¸€éƒ¨åˆ†ç”¨äºè®­ç»ƒæ¨¡å‹ï¼Œå¦ä¸€éƒ¨åˆ†ç”¨äºæµ‹è¯•æ¨¡å‹ã€‚`train_test_split`çš„ä¸€ä¸ªä¸»è¦ä¼˜ç‚¹æ˜¯å®ƒéå¸¸å®¹æ˜“ä½¿ç”¨ï¼Œåªéœ€è¦ä¸€è¡Œä»£ç å°±å¯ä»¥å®Œæˆæ•°æ®æ‹†åˆ†ã€‚ä¾‹å¦‚ï¼š

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

- åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œ`train_test_split`å°†æ•°æ®é›†Xå’Œæ ‡ç­¾yæ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå…¶ä¸­æµ‹è¯•é›†å æ€»æ•°æ®é›†çš„20%ã€‚`random_state`å‚æ•°ç”¨äºè®¾ç½®éšæœºæ•°ç§å­ï¼Œä»¥ç¡®ä¿åœ¨å¤šæ¬¡è¿è¡Œä»£ç æ—¶ï¼Œå¾—åˆ°çš„æ‹†åˆ†ç»“æœç›¸åŒã€‚

- `KFold`æ˜¯ä¸€ç§äº¤å‰éªŒè¯æ–¹æ³•ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚å®ƒå°†æ•°æ®é›†æ‹†åˆ†ä¸ºkä¸ªæŠ˜å ï¼ˆfoldï¼‰ï¼Œæ¯ä¸ªæŠ˜å éƒ½ç”¨äºè®­ç»ƒæ¨¡å‹å’Œè¯„ä¼°æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒå°†æ•°æ®é›†åˆ†æˆkä¸ªç­‰å¤§å°çš„å­é›†ï¼Œæ¯ä¸ªå­é›†éƒ½è¢«ç”¨ä½œæµ‹è¯•é›†ä¸€æ¬¡ï¼Œå‰©ä½™çš„k-1ä¸ªå­é›†è¢«ç”¨ä½œè®­ç»ƒé›†ã€‚è¿™ä¸ªè¿‡ç¨‹è¢«é‡å¤kæ¬¡ï¼Œæ¯æ¬¡ä½¿ç”¨ä¸åŒçš„æµ‹è¯•é›†ã€‚æœ€ç»ˆï¼Œæ‰€æœ‰çš„æµ‹è¯•ç»“æœè¢«å¹³å‡å¾—åˆ°ä¸€ä¸ªæ€§èƒ½æŒ‡æ ‡ã€‚
- `KFold`çš„ä¸€ä¸ªä¸»è¦ä¼˜ç‚¹æ˜¯å®ƒå¯ä»¥ä½¿ç”¨æ‰€æœ‰çš„æ•°æ®æ¥è®­ç»ƒå’Œæµ‹è¯•æ¨¡å‹ï¼Œè¿™æ ·å¯ä»¥æ›´å¥½åœ°è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼š

```python
from sklearn.model_selection import KFold
from sklearn.linear_model import LinearRegression

kf = KFold(n_splits=5, shuffle=True, random_state=42)

for train_index, test_index in kf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    model = LinearRegression()
    model.fit(X_train, y_train)
    score = model.score(X_test, y_test)
    
    print("Score:", score)
```

- åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œ`KFold`å°†æ•°æ®é›†Xå’Œæ ‡ç­¾yæ‹†åˆ†ä¸º5ä¸ªæŠ˜å ï¼Œæ¯ä¸ªæŠ˜å éƒ½ç”¨äºè®­ç»ƒæ¨¡å‹å’Œè¯„ä¼°æ¨¡å‹ã€‚åœ¨æ¯ä¸ªå¾ªç¯ä¸­ï¼Œæˆ‘ä»¬è·å–è®­ç»ƒç´¢å¼•å’Œæµ‹è¯•ç´¢å¼•ï¼Œç„¶åä½¿ç”¨å®ƒä»¬æ¥æ‹†åˆ†æ•°æ®é›†ã€‚æ¥ç€ï¼Œæˆ‘ä»¬è®­ç»ƒçº¿æ€§å›å½’æ¨¡å‹ï¼Œå¹¶åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬æ‰“å°å‡ºæ¯ä¸ªæŠ˜å çš„å¾—åˆ†ã€‚
- éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œ`KFold`éœ€è¦æ‰‹åŠ¨ç¼–å†™å¾ªç¯æ¥è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ï¼Œè¿™æ ·éœ€è¦æ›´å¤šçš„ä»£ç å’Œæ—¶é—´ã€‚ä½†å®ƒå¯ä»¥æ›´å¥½åœ°è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚

### cross_validateğŸˆ

- [sklearn.model_selection.cross_validate â€” scikit-learn 1.2.2 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html)

  - Evaluate metric(s) by cross-validation and also record fit/score times.é€šè¿‡äº¤å‰éªŒè¯è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶è®°å½•æ‹Ÿåˆ/å¾—åˆ†æ—¶é—´ã€‚

- `cross_validate`æ˜¯Scikit-learnåº“ä¸­çš„ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºå¯¹ç»™å®šçš„æ¨¡å‹è¿›è¡Œäº¤å‰éªŒè¯ï¼Œå¹¶è¿”å›äº¤å‰éªŒè¯çš„ç»“æœã€‚

  å…·ä½“æ¥è¯´ï¼Œ`cross_validate`å‡½æ•°çš„åŠŸèƒ½å¦‚ä¸‹ï¼š

  1. å¯¹ç»™å®šçš„æ¨¡å‹è¿›è¡Œäº¤å‰éªŒè¯ï¼Œä»¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚
  2. æ ¹æ®æ•°æ®é›†çš„åˆ’åˆ†æ–¹å¼ï¼Œå°†æ•°æ®é›†åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼Œå¤šæ¬¡è¿›è¡Œè®­ç»ƒå’ŒéªŒè¯ï¼Œå¹¶è®¡ç®—æ¨¡å‹çš„æ€§èƒ½æŒ‡æ ‡ï¼Œå¦‚ç²¾åº¦ã€F1åˆ†æ•°ã€Rå¹³æ–¹ç­‰ã€‚
  3. è¿”å›äº¤å‰éªŒè¯çš„ç»“æœï¼ŒåŒ…æ‹¬è®­<u>ç»ƒé›†å¾—åˆ†ã€éªŒè¯é›†å¾—åˆ†ã€æ‹Ÿåˆæ—¶é—´ã€é¢„æµ‹æ—¶é—´</u>ç­‰ã€‚

- `cross_validate`å‡½æ•°çš„å‚æ•°åŒ…æ‹¬ï¼š

  - `estimator`ï¼šè¡¨ç¤ºè¦è¿›è¡Œäº¤å‰éªŒè¯çš„æ¨¡å‹ã€‚
  - `X`ï¼šè¡¨ç¤ºæ•°æ®é›†çš„è‡ªå˜é‡ã€‚
  - `y`ï¼šè¡¨ç¤ºæ•°æ®é›†çš„å› å˜é‡ã€‚
  - `cv`ï¼šè¡¨ç¤ºäº¤å‰éªŒè¯çš„åˆ’åˆ†æ–¹å¼ã€‚
    - åœ¨Scikit-learnåº“ä¸­ï¼Œ`cv`å‚æ•°ç”¨äºæŒ‡å®šäº¤å‰éªŒè¯çš„ç”Ÿæˆå™¨æˆ–æ•´æ•°å€¼ï¼Œä»¥æ§åˆ¶æ•°æ®é›†çš„åˆ’åˆ†æ–¹å¼ã€‚`cv`å‚æ•°çš„ç¼©å†™é€šå¸¸æœ‰ä»¥ä¸‹å‡ ç§ï¼š
      - `cv`ï¼šè¡¨ç¤ºCross-validationï¼Œå³äº¤å‰éªŒè¯ï¼Œæ˜¯é»˜è®¤çš„ç¼©å†™ã€‚
      - `k`ï¼šè¡¨ç¤ºK-foldï¼Œå³KæŠ˜äº¤å‰éªŒè¯ï¼Œå…¶ä¸­Kè¡¨ç¤ºæ•°æ®é›†è¢«åˆ†æˆçš„æŠ˜æ•°ã€‚
      - `loo`ï¼šè¡¨ç¤ºLeave-One-Outï¼Œå³ç•™ä¸€æ³•äº¤å‰éªŒè¯ï¼Œå…¶ä¸­æ¯ä¸ªæ ·æœ¬éƒ½è¢«ç”¨ä½œéªŒè¯é›†ï¼Œå…¶ä½™æ ·æœ¬ç”¨ä½œè®­ç»ƒé›†ã€‚
      - `shuffle`ï¼šè¡¨ç¤ºShuffle-splitï¼Œå³éšæœºåˆ’åˆ†äº¤å‰éªŒè¯ï¼Œå…¶ä¸­æ¯æ¬¡åˆ’åˆ†éƒ½æ˜¯éšæœºçš„ï¼Œå¹¶æŒ‡å®šäº†åˆ’åˆ†çš„æ¬¡æ•°å’Œæµ‹è¯•é›†çš„å¤§å°ã€‚
  - `scoring`ï¼šè¡¨ç¤ºè¯„ä¼°æŒ‡æ ‡ã€‚
  - `return_train_score`ï¼šè¡¨ç¤ºæ˜¯å¦è¿”å›è®­ç»ƒé›†å¾—åˆ†ã€‚
  - `n_jobs`ï¼šè¡¨ç¤ºå¹¶è¡Œè¿ç®—çš„æ•°é‡ã€‚

- è¿”å›å€¼

  - `cross_validate()`å®ƒè¿”å›ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«äº†å¤šä¸ªè¯„ä¼°æŒ‡æ ‡çš„å€¼ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›é‡è¦çš„è¾“å‡ºç»“æœï¼š

    1. `fit_time`ï¼šè¡¨ç¤ºæ¯ä¸ªäº¤å‰éªŒè¯çš„è®­ç»ƒæ—¶é—´ï¼Œå³åœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆæ¨¡å‹æ‰€éœ€çš„æ—¶é—´ã€‚
    2. `score_time`ï¼šè¡¨ç¤ºæ¯ä¸ªäº¤å‰éªŒè¯çš„è¯„ä¼°æ—¶é—´ï¼Œå³åœ¨éªŒè¯é›†ä¸Šå¯¹æ¨¡å‹è¿›è¡Œè¯„ä¼°æ‰€éœ€çš„æ—¶é—´ã€‚
    3. `test_score`ï¼šè¡¨ç¤ºæ¯ä¸ªäº¤å‰éªŒè¯çš„æµ‹è¯•å¾—åˆ†ï¼Œå³åœ¨éªŒè¯é›†ä¸Šçš„æ¨¡å‹æ€§èƒ½è¯„ä¼°å¾—åˆ†ã€‚
       1. å¯¹äºåˆ†ç±»é—®é¢˜ï¼Œé€šå¸¸ä½¿ç”¨å‡†ç¡®ç‡ï¼ˆaccuracyï¼‰æ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼›
       2. å¯¹äºå›å½’é—®é¢˜ï¼Œé€šå¸¸ä½¿ç”¨å‡æ–¹è¯¯å·®ï¼ˆmean squared errorï¼‰æˆ–Rå¹³æ–¹ï¼ˆR-squaredï¼‰æ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚
    4. `train_score`ï¼šè¡¨ç¤ºæ¯ä¸ªäº¤å‰éªŒè¯çš„è®­ç»ƒå¾—åˆ†ï¼Œå³åœ¨è®­ç»ƒé›†ä¸Šçš„æ¨¡å‹æ€§èƒ½è¯„ä¼°å¾—åˆ†ã€‚

  - åœ¨æ¨¡å‹æ²¡æœ‰è¿‡æ‹Ÿåˆçš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šçš„è¡¨ç°åº”è¯¥ä¸åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°ç›¸ä¼¼ã€‚å› ä¸ºæ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šå­¦åˆ°çš„çŸ¥è¯†åº”è¯¥é€‚ç”¨äºæµ‹è¯•é›†ï¼Œå¦‚æœè®­ç»ƒå¾—åˆ†æ¯”æµ‹è¯•å¾—åˆ†é«˜å¾ˆå¤šï¼Œåˆ™è¡¨ç¤ºæ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šè¿‡åº¦æ‹Ÿåˆï¼Œä¸èƒ½å¾ˆå¥½åœ°æ³›åŒ–åˆ°æ–°çš„æ•°æ®ã€‚

    é™¤äº†ä¸Šè¿°æŒ‡æ ‡å¤–ï¼Œ`cross_validate`è¿˜å¯ä»¥è¿”å›å…¶ä»–æŒ‡æ ‡ï¼Œå¦‚å¹³å‡è®­ç»ƒå¾—åˆ†ã€å¹³å‡æµ‹è¯•å¾—åˆ†ã€æµ‹è¯•å¾—åˆ†çš„æ ‡å‡†å·®ç­‰ã€‚è¿™äº›æŒ‡æ ‡å¯ä»¥å¸®åŠ©æˆ‘ä»¬æ›´å…¨é¢åœ°è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½å’Œç¨³å®šæ€§ã€‚

    éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œ`cross_validate`çš„è¾“å‡ºç»“æœæ˜¯é’ˆå¯¹æ¯ä¸ªäº¤å‰éªŒè¯çš„ç»“æœï¼Œå› æ­¤éœ€è¦å¯¹ç»“æœè¿›è¡Œå¹³å‡æˆ–æ±‡æ€»ä»¥å¾—åˆ°æœ€ç»ˆçš„æ¨¡å‹æ€§èƒ½è¯„ä¼°ã€‚

#### eg

ä¾‹å¦‚ï¼Œä¸‹é¢çš„ä»£ç æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨`cross_validate`å‡½æ•°å¯¹çº¿æ€§å›å½’æ¨¡å‹è¿›è¡Œäº¤å‰éªŒè¯ï¼Œå¹¶è¿”å›äº¤å‰éªŒè¯çš„ç»“æœï¼š

- ```python
  from sklearn.datasets import make_regression
  from sklearn.linear_model import LinearRegression
  from sklearn.model_selection import cross_validate
  
  X, y = make_regression(n_samples=1000, random_state=0)
  lr = LinearRegression()
  
  result = cross_validate(lr, X, y, cv=5, return_train_score=True)
  print(result)
  ```

- ```bash
  ('fit_time', array([0.01501846, 0.00799823, 0.00999498, 0.01046562, 0.00799966]))
  ('score_time', array([0.00098014, 0.00175047, 0.00099993, 0.00201273, 0.00199795]))
  ('test_score', array([1., 1., 1., 1., 1.]))
  ('train_score', array([1., 1., 1., 1., 1.]))
  ```

  

- è¾“å‡ºçš„ç»“æœåŒ…æ‹¬è®­ç»ƒé›†å¾—åˆ†ã€éªŒè¯é›†å¾—åˆ†ã€æ‹Ÿåˆæ—¶é—´ã€é¢„æµ‹æ—¶é—´ç­‰ä¿¡æ¯ï¼Œå¯ä»¥ç”¨äºè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½å’Œè¿›è¡Œæ¨¡å‹çš„é€‰æ‹©å’Œè°ƒä¼˜ã€‚

### cross_val_score

- [sklearn.model_selection.cross_val_score â€” scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)

- `cross_val_score`æ˜¯Scikit-learnä¸­ç”¨äºæ‰§è¡ŒKæŠ˜äº¤å‰éªŒè¯çš„å‡½æ•°ä¹‹ä¸€ã€‚å®ƒå¯ä»¥å¸®åŠ©æˆ‘ä»¬è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒçš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šçš„æ€§èƒ½ï¼Œå¹¶è®¡ç®—æ‰€é€‰è¯„ä¼°æŒ‡æ ‡çš„å¹³å‡å€¼å’Œæ ‡å‡†å·®ã€‚
- ä½¿ç”¨`cross_val_score`å‡½æ•°éå¸¸ç®€å•ï¼Œåªéœ€è¦æä¾›è¦è¯„ä¼°çš„æ¨¡å‹ã€è¦ä½¿ç”¨çš„æ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡ä»¥åŠäº¤å‰éªŒè¯çš„æŠ˜æ•°å³å¯ã€‚

ä»¥ä¸‹æ˜¯`cross_val_score`å‡½æ•°çš„åŸºæœ¬è¯­æ³•ï¼š

```python
from sklearn.model_selection import cross_val_score

scores = cross_val_score(estimator, X, y, cv=k, scoring=None)
```

å…¶ä¸­ï¼Œå‚æ•°å«ä¹‰å¦‚ä¸‹ï¼š

- `estimator`: è¦è¯„ä¼°çš„æ¨¡å‹æˆ–ç®¡é“å¯¹è±¡ã€‚
- `X`: ç‰¹å¾æ•°æ®é›†ã€‚
- `y`: æ ‡ç­¾æ•°æ®é›†ã€‚
- `cv`: äº¤å‰éªŒè¯çš„æŠ˜æ•°ï¼Œé»˜è®¤ä¸º5ã€‚
- `scoring`: è¦ä½¿ç”¨çš„è¯„ä¼°æŒ‡æ ‡ï¼Œé»˜è®¤ä¸ºæ¨¡å‹çš„é»˜è®¤æŒ‡æ ‡ã€‚

`cross_val_score`å‡½æ•°è¿”å›ä¸€ä¸ªåŒ…å«æ¯ä¸ªæŠ˜çš„è¯„ä¼°æŒ‡æ ‡å¾—åˆ†çš„æ•°ç»„ï¼Œå¯ä»¥é€šè¿‡è®¡ç®—å…¶å¹³å‡å€¼å’Œæ ‡å‡†å·®æ¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚å¦‚æœæ²¡æœ‰æŒ‡å®šè¯„ä¼°æŒ‡æ ‡ï¼Œåˆ™é»˜è®¤ä½¿ç”¨æ¨¡å‹çš„é»˜è®¤æŒ‡æ ‡ã€‚

- ä»¥ä¸‹æ˜¯ä¸€ä¸ªä½¿ç”¨`cross_val_score`å‡½æ•°è¯„ä¼°Logisticå›å½’æ¨¡å‹æ€§èƒ½çš„ç®€å•ç¤ºä¾‹ï¼š

```python
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score

X, y = make_classification(n_samples=1000, random_state=0)

lr = LogisticRegression()
scores = cross_val_score(lr, X, y, cv=5)

print(f'{scores=}')
print("Accuracy: {:.2f} (+/- {:.2f})".format(scores.mean(), scores.std()))
```

- åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨`make_classification`å‡½æ•°ç”Ÿæˆä¸€ä¸ªäºŒåˆ†ç±»æ•°æ®é›†ï¼Œç„¶åä½¿ç”¨`LogisticRegression`ä½œä¸ºè¯„ä¼°æ¨¡å‹ã€‚æˆ‘ä»¬é€šè¿‡ä¼ é€’æ¨¡å‹ã€ç‰¹å¾æ•°æ®é›†å’Œæ ‡ç­¾æ•°æ®é›†ä»¥åŠ5æŠ˜äº¤å‰éªŒè¯æ¥è°ƒç”¨`cross_val_score`å‡½æ•°ã€‚æœ€åï¼Œæˆ‘ä»¬è®¡ç®—è¯„ä¼°æŒ‡æ ‡å¾—åˆ†çš„å¹³å‡å€¼å’Œæ ‡å‡†å·®ï¼Œå¹¶å°†å…¶æ‰“å°å‡ºæ¥ã€‚

### cross_val_score@cross_validate

- [sklearn.model_selection.cross_val_score â€” scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)

- [sklearn.model_selection.cross_validate â€” scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html)

- `cross_val_score`å’Œ`cross_validate`éƒ½æ˜¯Scikit-learnä¸­ç”¨äºæ‰§è¡ŒKæŠ˜äº¤å‰éªŒè¯çš„å‡½æ•°ï¼Œå®ƒä»¬çš„åŒºåˆ«åœ¨äºï¼š

  1. è¿”å›ç»“æœä¸åŒï¼š`cross_val_score`åªè¿”å›ä¸€ä¸ªåŒ…å«æ¯ä¸ªæŠ˜çš„è¯„ä¼°æŒ‡æ ‡å¾—åˆ†çš„æ•°ç»„ï¼Œè€Œ`cross_validate`è¿”å›ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«æ¯ä¸ªæŒ‡æ ‡çš„å¾—åˆ†æ•°ç»„ã€æ¯ä¸ªæ‹Ÿåˆæ—¶é—´çš„æ•°ç»„å’Œæ¯ä¸ªé¢„æµ‹æ—¶é—´çš„æ•°ç»„ã€‚
  2. å¯é€‰å‚æ•°ä¸åŒï¼š`cross_validate`å‡½æ•°æ¯”`cross_val_score`å‡½æ•°å¤šäº†ä¸€äº›å¯é€‰å‚æ•°ï¼Œä¾‹å¦‚è¿”å›è®­ç»ƒå¾—åˆ†ã€æµ‹è¯•å¾—åˆ†ã€æ‹Ÿåˆæ—¶é—´å’Œé¢„æµ‹æ—¶é—´ç­‰ã€‚
  3. é€‚ç”¨åœºåˆä¸åŒï¼š`cross_val_score`é€‚ç”¨äºç®€å•çš„è¯„ä¼°æ¨¡å‹æ€§èƒ½çš„æƒ…å†µï¼Œè€Œ`cross_validate`é€‚ç”¨äºæ›´å¤æ‚çš„æƒ…å†µï¼Œä¾‹å¦‚éœ€è¦åŒæ—¶è¯„ä¼°å¤šä¸ªæŒ‡æ ‡å’Œè®°å½•æ¨¡å‹æ‹Ÿåˆå’Œé¢„æµ‹æ—¶é—´çš„æƒ…å†µã€‚

  å› æ­¤ï¼Œåœ¨ç®€å•çš„æ¨¡å‹è¯„ä¼°ä»»åŠ¡ä¸­ï¼Œ`cross_val_score`æ˜¯æ›´å¸¸ç”¨å’Œæ›´æ–¹ä¾¿çš„å‡½æ•°ã€‚ä½†åœ¨æ›´å¤æ‚çš„ä»»åŠ¡ä¸­ï¼Œ`cross_validate`å¯èƒ½æ›´é€‚åˆï¼Œå› ä¸ºå®ƒå¯ä»¥æä¾›æ›´å¤šçš„ä¿¡æ¯å’Œçµæ´»æ€§ã€‚

- ä»¥ä¸‹æ˜¯ä½¿ç”¨`cross_val_score`å’Œ`cross_validate`å‡½æ•°çš„ç¤ºä¾‹ï¼š

  ```python
  from sklearn.datasets import load_digits
  from sklearn.svm import SVC
  from sklearn.model_selection import cross_val_score, cross_validate
  
  digits = load_digits()
  X, y = digits.data, digits.target # type: ignore
  
  # ä½¿ç”¨cross_val_scoreå‡½æ•°
  clf = SVC(kernel='linear', C=1, random_state=0)
  scores = cross_val_score(clf, X, y, cv=5)
  print("Cross-validation scores: ", scores)
  
  # ä½¿ç”¨cross_validateå‡½æ•°
  scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']
  clf = SVC(kernel='linear', C=1, random_state=0)
  scores = cross_validate(clf, X, y, scoring=scoring, cv=5, return_train_score=True)
  print("Train scores: ", scores['train_accuracy'])
  print("Test scores: ", scores['test_accuracy'])
  print("Fit time: ", scores['fit_time'])
  print("Score time: ", scores['score_time'])
  ```

- åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨`load_digits`åŠ è½½äº†ä¸€ä¸ªæ‰‹å†™æ•°å­—åˆ†ç±»æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨`SVC`ä½œä¸ºåˆ†ç±»å™¨ã€‚æˆ‘ä»¬ä½¿ç”¨`cross_val_score`å‡½æ•°è®¡ç®—äº†5æŠ˜äº¤å‰éªŒè¯çš„å‡†ç¡®æ€§å¾—åˆ†ï¼Œå¹¶ä½¿ç”¨`cross_validate`å‡½æ•°è®¡ç®—äº†5æŠ˜äº¤å‰éªŒè¯çš„å¤šä¸ªåº¦é‡å’Œæ‹Ÿåˆæ—¶é—´ã€‚æ³¨æ„ï¼Œ`cross_validate`å‡½æ•°è¿”å›ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«è®­ç»ƒå¾—åˆ†ã€æµ‹è¯•å¾—åˆ†å’Œæ‹Ÿåˆæ—¶é—´ç­‰ä¿¡æ¯ã€‚

  

## è½½å…¥@ç”Ÿæˆæ•°æ®é›†

- [sklearn.datasets](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets)
- The sklearn.datasets module includes utilities to load datasets, including methods to load and fetch popular reference datasets. It also features some artificial data generators.
  - Loaders
  - Samples generator

### make_regressionğŸˆ

- [sklearn.datasets.make_regression â€” scikit-learn 1.2.2 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html)

  - Generate a random regression problem.

    The input set can either be well conditioned (by default) or have a low rank-fat tail singular profile. See make_low_rank_matrix for more details.

    The output is generated by applying a (potentially biased) random linear regression model with n_informative nonzero regressors to the previously generated input and some gaussian centered noise with some adjustable scale.

    Read more in the User Guide.

  - [sklearn.datasets.make_regression-scikit-learnä¸­æ–‡ç¤¾åŒº](https://scikit-learn.org.cn/view/595.html)

    è¾“å…¥é›†å¯ä»¥æ˜¯è‰¯å¥½æ¡ä»¶çš„ï¼ˆé»˜è®¤æƒ…å†µä¸‹ï¼‰æˆ–å…·æœ‰ä½ç§©-çŸ­å°¾å¥‡å¼‚è½®å»“ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§make_low_rank_matrixã€‚

    è¾“å‡ºæ˜¯é€šè¿‡å°†ä¸€ä¸ªï¼ˆå¯èƒ½æœ‰åå·®çš„ï¼‰éšæœºçº¿æ€§å›å½’æ¨¡å‹åº”ç”¨äºå…ˆå‰ç”Ÿæˆçš„è¾“å…¥å’Œä¸€äº›å…·æœ‰å¯è°ƒèŠ‚å°ºåº¦çš„é«˜æ–¯å™ªå£°çš„n_informativeéé›¶å›å½’å™¨æ¥ç”Ÿæˆçš„ã€‚

  - `make_regression`æ˜¯Scikit-learnä¸­çš„ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºç”Ÿæˆä¸€ä¸ªéšæœºå›å½’æ•°æ®é›†ã€‚å®ƒçš„è¯­æ³•å¦‚ä¸‹ï¼š

    ```python
    make_regression(n_samples=100, n_features=100, n_informative=10,
                    n_targets=1, bias=0.0, noise=0.0, shuffle=True, 
                    coef=False, random_state=None)
    ```

    å‚æ•°è¯´æ˜ï¼š

    - n_samplesï¼šç”Ÿæˆçš„æ ·æœ¬æ•°ï¼Œé»˜è®¤ä¸º100ã€‚
    - n_featuresï¼šç”Ÿæˆçš„ç‰¹å¾æ•°ï¼Œé»˜è®¤ä¸º100ã€‚
    - n_informativeï¼šç”Ÿæˆçš„æœ‰æ•ˆç‰¹å¾æ•°ï¼Œé»˜è®¤ä¸º10ã€‚è¿™äº›æœ‰æ•ˆç‰¹å¾ä¼šå¯¹ç›®æ ‡å˜é‡æœ‰è´¡çŒ®ã€‚
    - n_targetsï¼šç”Ÿæˆçš„ç›®æ ‡å˜é‡æ•°ï¼Œé»˜è®¤ä¸º1ã€‚
    - biasï¼šåç½®é¡¹ï¼Œé»˜è®¤ä¸º0.0ã€‚
    - noiseï¼šå™ªå£°é¡¹ï¼Œé»˜è®¤ä¸º0.0ã€‚åœ¨ç›®æ ‡å˜é‡ä¸ŠåŠ ä¸Šä¸€äº›å™ªå£°æ¥ä½¿æ•°æ®æ›´çœŸå®ã€‚
    - shuffleï¼šæ˜¯å¦æ‰“ä¹±æ ·æœ¬ï¼Œé»˜è®¤ä¸ºTrueã€‚
    - coefï¼šæ˜¯å¦è¿”å›ç”¨äºç”Ÿæˆæ•°æ®é›†çš„ç³»æ•°ï¼Œé»˜è®¤ä¸ºFalseã€‚
    - random_stateï¼šéšæœºæ•°ç§å­ã€‚

    `make_regression`å‡½æ•°è¿”å›ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«ç”Ÿæˆçš„æ•°æ®é›†å’ŒçœŸå®çš„ç›®æ ‡å˜é‡ã€‚å…¶ä¸­ï¼Œæ•°æ®é›†çš„å½¢çŠ¶ä¸º(n_samples, n_features)ï¼Œç›®æ ‡å˜é‡çš„å½¢çŠ¶ä¸º(n_samples, n_targets)ã€‚

  - make_regressionå‡½æ•°ç”Ÿæˆçš„æ•°æ®å¯ä»¥ç†è§£ä¸ºä¸€ä¸ªå¸¦æœ‰ä¸€å®šè§„å¾‹çš„æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«äº†è¾“å…¥ç‰¹å¾å’Œç›®æ ‡å€¼ã€‚

    è¾“å…¥ç‰¹å¾æ˜¯ä¸€ä¸ªäºŒç»´æ•°ç»„ï¼Œæ¯è¡Œè¡¨ç¤ºä¸€ä¸ªæ ·æœ¬ï¼Œæ¯åˆ—è¡¨ç¤ºä¸€ä¸ªç‰¹å¾ã€‚æˆ‘ä»¬å¯ä»¥å°†å…¶ç†è§£ä¸ºæ•°æ®é›†ä¸­çš„å±æ€§ï¼Œä¾‹å¦‚åœ¨æˆ¿ä»·é¢„æµ‹ä¸­ï¼Œç‰¹å¾å¯ä»¥æ˜¯æˆ¿å±‹çš„é¢ç§¯ã€å§å®¤æ•°é‡ç­‰ç­‰ã€‚

    ç›®æ ‡å€¼æ˜¯ä¸€ä¸ªä¸€ç»´æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ è¡¨ç¤ºå¯¹åº”æ ·æœ¬çš„çœŸå®å€¼ã€‚æˆ‘ä»¬å¯ä»¥å°†å…¶ç†è§£ä¸ºæ•°æ®é›†ä¸­çš„æ ‡ç­¾æˆ–è¾“å‡ºï¼Œä¾‹å¦‚åœ¨æˆ¿ä»·é¢„æµ‹ä¸­ï¼Œç›®æ ‡å€¼å¯ä»¥æ˜¯è¯¥æˆ¿å±‹çš„çœŸå®å”®ä»·ã€‚

    make_regressionå‡½æ•°ç”Ÿæˆçš„æ•°æ®é›†å…·æœ‰ä¸€å®šçš„è§„å¾‹ï¼Œå¯ä»¥æ˜¯çº¿æ€§å…³ç³»æˆ–éçº¿æ€§å…³ç³»ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ç”Ÿæˆä¸€ä¸ªçº¿æ€§å…³ç³»çš„æ•°æ®é›†ï¼Œå…¶ä¸­è¾“å…¥ç‰¹å¾å’Œç›®æ ‡å€¼ä¹‹é—´å­˜åœ¨çº¿æ€§å…³ç³»ï¼Œå¯ä»¥ç”¨çº¿æ€§å›å½’ç®—æ³•æ¥é¢„æµ‹ç›®æ ‡å€¼ï¼›æˆ–è€…ç”Ÿæˆä¸€ä¸ªéçº¿æ€§å…³ç³»çš„æ•°æ®é›†ï¼Œå…¶ä¸­è¾“å…¥ç‰¹å¾å’Œç›®æ ‡å€¼ä¹‹é—´å­˜åœ¨å¤æ‚çš„éçº¿æ€§å…³ç³»ï¼Œéœ€è¦ç”¨æ›´å¤æ‚çš„ç®—æ³•æ¥é¢„æµ‹ç›®æ ‡å€¼ã€‚

    é€šè¿‡ç”Ÿæˆä¸åŒè§„å¾‹çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬å¯ä»¥æµ‹è¯•å’Œè¯„ä¼°ä¸åŒçš„å›å½’ç®—æ³•åœ¨ä¸åŒæƒ…å†µä¸‹çš„æ€§èƒ½å’Œæ•ˆæœï¼Œä»è€Œé€‰æ‹©æœ€é€‚åˆå®é™…åº”ç”¨çš„ç®—æ³•ã€‚

#### eg

- ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨`make_regression`ç”Ÿæˆéšæœºå›å½’æ•°æ®é›†çš„ä¾‹å­ï¼š

  ```python
  from sklearn.datasets import make_regression
  
  # generate a random regression dataset
  X, y = make_regression(n_samples=1000, n_features=5, n_informative=2, noise=0.5, random_state=42)
  
  # print the shape of the dataset and target variable
  print("Shape of dataset:", X.shape)
  print("Shape of target variable:", y.shape)
  ```

- åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨`make_regression`å‡½æ•°ç”Ÿæˆä¸€ä¸ªåŒ…å«1000ä¸ªæ ·æœ¬ã€5ä¸ªç‰¹å¾å’Œ2ä¸ªæœ‰æ•ˆç‰¹å¾çš„éšæœºå›å½’æ•°æ®é›†ã€‚

- æˆ‘ä»¬è¿˜åœ¨ç›®æ ‡å˜é‡ä¸Šæ·»åŠ äº†ä¸€äº›å™ªå£°ã€‚æœ€åï¼Œæˆ‘ä»¬æ‰“å°å‡ºæ•°æ®é›†å’Œç›®æ ‡å˜é‡çš„å½¢çŠ¶ã€‚

- éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œç”±äº`make_regression`æ˜¯ä¸€ä¸ªéšæœºå‡½æ•°ï¼Œæ¯æ¬¡ç”Ÿæˆçš„æ•°æ®é›†éƒ½æ˜¯ä¸åŒçš„ã€‚æ­¤å¤–ï¼Œå¯ä»¥é€šè¿‡è°ƒæ•´å‚æ•°æ¥æ§åˆ¶ç”Ÿæˆæ•°æ®é›†çš„ç‰¹å¾å’Œç›®æ ‡å˜é‡çš„å±æ€§ï¼Œä»¥åŒ¹é…å…·ä½“é—®é¢˜çš„éœ€æ±‚ã€‚

#### visualization

- å½“ç”Ÿæˆçš„æ•°æ®é›†çš„ç‰¹å¾ç»´åº¦è¾ƒä½ï¼ˆå¦‚2ç»´æˆ–3ç»´ï¼‰æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¯è§†åŒ–å·¥å…·æ¥ç›´è§‚åœ°å±•ç¤ºè¾“å‡ºç»“æœã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼Œä½¿ç”¨matplotlibåº“æ¥å¯è§†åŒ–è¾“å‡ºç»“æœï¼š

  ```python
  import numpy as np
  import matplotlib.pyplot as plt
  from sklearn.datasets import make_regression
  
  # ç”Ÿæˆæ•°æ®é›†
  X, y = make_regression(n_samples=100, n_features=1, noise=10)
  
  # ç»˜åˆ¶æ•£ç‚¹å›¾
  plt.scatter(X, y)
  
  # ç»˜åˆ¶å›å½’ç›´çº¿
  x_min, x_max = plt.xlim()
  coef, bias = np.polyfit(X.ravel(), y.ravel(), 1)
  plt.plot([x_min, x_max], [x_min * coef + bias, x_max * coef + bias], 'r')
  
  # æ·»åŠ æ ‡é¢˜å’Œæ ‡ç­¾
  plt.title("Random Regression Problem")
  plt.xlabel("Input Feature")
  plt.ylabel("Target Value")
  
  # æ˜¾ç¤ºå›¾åƒ
  plt.show()
  ```

- è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†ä¸€ä¸ªç®€å•çš„ä¸€ç»´å›å½’é—®é¢˜ï¼Œç„¶åä½¿ç”¨æ•£ç‚¹å›¾å±•ç¤ºäº†è¾“å…¥ç‰¹å¾å’Œç›®æ ‡å€¼ä¹‹é—´çš„å…³ç³»ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨polyfitå‡½æ•°æ‹Ÿåˆäº†ä¸€æ¡å›å½’ç›´çº¿ï¼Œå¹¶å°†å…¶ç»˜åˆ¶åœ¨æ•£ç‚¹å›¾ä¸Šï¼Œä»¥ä¾¿æ›´å¥½åœ°å±•ç¤ºå›å½’é—®é¢˜çš„è§„å¾‹ã€‚

  - Least squares polynomial fit.æœ€å°äºŒä¹˜å¤šé¡¹å¼æ‹Ÿåˆ:numpy.polyfit â€” NumPy v1.24 Manual](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html)

  - `np.polyfit(x, y, deg)`æ˜¯numpyåº“ä¸­çš„ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºå¤šé¡¹å¼æ‹Ÿåˆï¼Œå³å°†ä¸€ç»„æ•°æ®ç‚¹æ‹Ÿåˆæˆä¸€ä¸ªå¤šé¡¹å¼å‡½æ•°ã€‚å®ƒçš„è¾“å…¥å‚æ•°åŒ…æ‹¬ï¼š

    - xï¼šè¦æ‹Ÿåˆçš„æ•°æ®ç‚¹çš„xåæ ‡æ•°ç»„ã€‚
    - yï¼šè¦æ‹Ÿåˆçš„æ•°æ®ç‚¹çš„yåæ ‡æ•°ç»„ã€‚
    - degï¼šæ‹Ÿåˆçš„å¤šé¡¹å¼æ¬¡æ•°ã€‚åœ¨â€œDegree of the fitting polynomialâ€ä¸­ï¼Œâ€œdegreeâ€å¯ä»¥ç¿»è¯‘ä¸ºâ€œæ¬¡æ•°â€æˆ–â€œé˜¶æ•°â€ï¼Œè¡¨ç¤ºæ‹Ÿåˆå¤šé¡¹å¼çš„æ¬¡æ•°æˆ–é˜¶æ•°ã€‚å› æ­¤ï¼Œå¯ä»¥å°†è¿™ä¸ªçŸ­è¯­ç¿»è¯‘ä¸ºâ€œæ‹Ÿåˆå¤šé¡¹å¼çš„æ¬¡æ•°â€ã€‚æœ‰æ—¶ç”¨orderæ¥è¡¨ç¤ºé˜¶æ•°

  - è¯¥å‡½æ•°è¿”å›çš„æ˜¯ä¸€ä¸ªä¸€ç»´æ•°ç»„ï¼ŒåŒ…å«äº†å¤šé¡¹å¼å‡½æ•°çš„ç³»æ•°ï¼Œä»é«˜æ¬¡åˆ°ä½æ¬¡æ’åˆ—ã€‚

  - ä¾‹å¦‚ï¼Œå¯¹äºä¸€ä¸ªäºŒæ¬¡å¤šé¡¹å¼æ‹Ÿåˆï¼Œè¿”å›çš„ç³»æ•°æ•°ç»„ä¸º[a, b, c]ï¼Œè¡¨ç¤ºæ‹Ÿåˆçš„å‡½æ•°ä¸º$y=ax^2+bx+c$ã€‚

  - ä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼Œä½¿ç”¨np.polyfitå‡½æ•°æ‹Ÿåˆä¸€æ¡å›å½’ç›´çº¿ï¼š

    ```python
    import numpy as np
    import matplotlib.pyplot as plt
    
    # ç”Ÿæˆæ•°æ®
    x = np.linspace(-10, 10, 100)
    y = 3 * x + 10 + np.random.randn(100) * 5
    
    # è®¡ç®—æ‹Ÿåˆç›´çº¿çš„ç³»æ•°
    # ç”±äºæ˜¯æ‹Ÿåˆçº¿æ€§å‡½æ•°,è¿”å›çš„ç»“æœå½¢å¦‚array([3.01458133, 9.44181608])çš„æ•°ç»„y=coef*x+bias
    coef, bias = np.polyfit(x, y, 1)
    
    # ç»˜åˆ¶æ•£ç‚¹å›¾å’Œæ‹Ÿåˆç›´çº¿
    plt.scatter(x, y)
    
    y=coef * x + bias
    plt.plot(x, y, color='red')
    
    # æ·»åŠ æ ‡é¢˜å’Œæ ‡ç­¾
    plt.title("Linear Regression Example")
    plt.xlabel("Input Feature")
    plt.ylabel("Target Value")
    
    # æ˜¾ç¤ºå›¾åƒ
    plt.show()
    ```

    è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨np.polyfitå‡½æ•°æ‹Ÿåˆäº†ä¸€æ¡å›å½’ç›´çº¿ï¼Œå°†ä¸€ç»„éšæœºç”Ÿæˆçš„æ•°æ®ç‚¹æ‹Ÿåˆæˆäº†ä¸€ä¸ªä¸€æ¬¡å‡½æ•°ã€‚æˆ‘ä»¬ä½¿ç”¨Matplotlibåº“ç»˜åˆ¶äº†æ•£ç‚¹å›¾å’Œæ‹Ÿåˆç›´çº¿ï¼Œå¹¶æ·»åŠ äº†æ ‡é¢˜å’Œæ ‡ç­¾ï¼Œä½¿å›¾è¡¨æ›´åŠ æ¸…æ™°å’Œæ˜“äºç†è§£ã€‚

- å½“ç‰¹å¾ç»´åº¦è¾ƒé«˜æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨é™ç»´æŠ€æœ¯ï¼ˆå¦‚PCAï¼‰å°†æ•°æ®é›†é™åˆ°2ç»´æˆ–3ç»´ï¼Œç„¶åä½¿ç”¨å¯è§†åŒ–å·¥å…·æ¥å±•ç¤ºè¾“å‡ºç»“æœã€‚

- å¦å¤–ï¼Œè¿˜å¯ä»¥ä½¿ç”¨ä¸€äº›é«˜çº§çš„å¯è§†åŒ–å·¥å…·ï¼ˆå¦‚[seaborn](https://seaborn.pydata.org/)åº“ï¼‰ï¼Œæ¥æ›´å¥½åœ°å±•ç¤ºå¤šç»´æ•°æ®é›†çš„ç‰¹å¾å’Œè§„å¾‹ã€‚

### make_classification

- [sklearn.datasets.make_classification â€” scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html)

- Generate a random n-class classification problem.

  This initially creates clusters of points normally distributed (std=1) about vertices of an `n_informative`-dimensional hypercube with sides of length `2*class_sep` and assigns an equal number of clusters to each class. It introduces interdependence between these features and adds various types of further noise to the data.

  Without shuffling, `X` horizontally stacks features in the following order: the primary `n_informative` features, followed by `n_redundant` linear combinations of the informative features, followed by `n_repeated` duplicates, drawn randomly with replacement from the informative and redundant features. The remaining features are filled with random noise. Thus, without shuffling, all useful features are contained in the columns `X[:, :n_informative + n_redundant + n_repeated]`.

  é¦–å…ˆï¼Œåœ¨ä¸€ä¸ªn_informativeç»´çš„è¶…ç«‹æ–¹ä½“çš„é¡¶ç‚¹å‘¨å›´ç”Ÿæˆæ ‡å‡†å·®ä¸º1çš„é«˜æ–¯åˆ†å¸ƒæ•°æ®ç‚¹ç°‡ï¼Œå¹¶å°†ç›¸ç­‰æ•°é‡çš„ç°‡åˆ†é…ç»™æ¯ä¸ªç±»åˆ«ã€‚å®ƒåœ¨è¿™äº›ç‰¹å¾ä¹‹é—´å¼•å…¥ç›¸äº’ä¾èµ–æ€§ï¼Œå¹¶å‘æ•°æ®æ·»åŠ å„ç§ç±»å‹çš„å™ªå£°ã€‚

  å¦‚æœä¸è¿›è¡Œæ´—ç‰Œï¼Œåˆ™XæŒ‰ç…§ä»¥ä¸‹é¡ºåºæ°´å¹³å †å ç‰¹å¾ï¼šn_informativeä¸ªä¸»è¦ç‰¹å¾ï¼Œåé¢æ˜¯n_redundantä¸ªä¿¡æ¯ç‰¹å¾çš„çº¿æ€§ç»„åˆï¼Œç„¶åæ˜¯éšæœºä»ä¿¡æ¯å’Œå†—ä½™ç‰¹å¾ä¸­é‡å¤é€‰æ‹©çš„n_repeatedä¸ªå‰¯æœ¬ã€‚å…¶ä½™ç‰¹å¾å¡«å……éšæœºå™ªå£°ã€‚å› æ­¤ï¼Œå¦‚æœä¸è¿›è¡Œæ´—ç‰Œï¼Œåˆ™æ‰€æœ‰æœ‰ç”¨ç‰¹å¾éƒ½åŒ…å«åœ¨X [:ï¼Œ: n_informative + n_redundant + n_repeated]åˆ—ä¸­ã€‚

- `make_classification`å‡½æ•°å¯ä»¥ç”Ÿæˆå…·æœ‰ä»¥ä¸‹ç‰¹å¾çš„åˆ†ç±»æ•°æ®é›†ï¼š

  - æ ·æœ¬æ•°å’Œç‰¹å¾æ•°å¯ä»¥è‡ªå®šä¹‰ã€‚
  - å¯ä»¥æŒ‡å®šç±»åˆ«æ•°é‡ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨é»˜è®¤å€¼ï¼ˆå³2ï¼‰ã€‚
  - å¯ä»¥æŒ‡å®šç‰¹å¾ä¹‹é—´çš„å…³è”åº¦ï¼ˆå³ç‰¹å¾ç›¸å…³æ€§ï¼‰ã€‚
  - å¯ä»¥æŒ‡å®šåˆ†ç±»å™¨çš„éš¾åº¦ï¼Œå³æ ·æœ¬æ˜¯å¦å®¹æ˜“åˆ†ç¦»ã€‚
  - å¯ä»¥æŒ‡å®šç”¨äºç”Ÿæˆæ•°æ®çš„éšæœºç§å­ï¼Œä»¥ä¾¿å®ç°å¯é‡å¤æ€§ã€‚

- `make_classification`å‡½æ•°å¯ä»¥ç”Ÿæˆä¸€ä¸ªå…·æœ‰nä¸ªç±»åˆ«çš„éšæœºåˆ†ç±»æ•°æ®é›†ï¼Œå…¶ä¸­æ¯ä¸ªç±»åˆ«åŒ…å«ä¸€ç»„ä»¥n_informativeç»´ä¸ºä¸­å¿ƒçš„é«˜æ–¯åˆ†å¸ƒæ•°æ®ç‚¹ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œå‡½æ•°è¿˜å¯ä»¥æ·»åŠ å™ªå£°å’Œå†—ä½™ç‰¹å¾ä»¥åŠå…¶ä»–ç±»å‹çš„å™ªå£°ã€‚ä¸‹é¢ç»™å‡ºä¸€ä¸ªä½¿ç”¨`make_classification`å‡½æ•°ç”Ÿæˆåˆ†ç±»æ•°æ®é›†çš„ç¤ºä¾‹ä»£ç ï¼š

  ```python
  from sklearn.datasets import make_classification
  
  X, y = make_classification(n_samples=1000, n_features=20, n_classes=5, n_informative=10, n_redundant=5, n_repeated=2, class_sep=2.0, random_state=0)
  
  print("X shape:", X.shape)
  print("y shape:", y.shape)
  ```

  - ```bash
    X shape: (1000, 20)
    y shape: (1000,)
    ```

  - ä¸Šé¢çš„ä»£ç ç”Ÿæˆäº†ä¸€ä¸ªåŒ…å«1000ä¸ªæ ·æœ¬å’Œ20ä¸ªç‰¹å¾çš„åˆ†ç±»æ•°æ®é›†ï¼Œå…¶ä¸­æœ‰5ä¸ªç±»åˆ«ã€‚
    - æ¯ä¸ªç±»åˆ«åŒ…å«ä¸€ç»„ä»¥10ç»´ä¸ºä¸­å¿ƒçš„é«˜æ–¯åˆ†å¸ƒæ•°æ®ç‚¹ï¼ŒåŒæ—¶æ·»åŠ äº†5ä¸ªå†—ä½™ç‰¹å¾å’Œ2ä¸ªé‡å¤ç‰¹å¾ï¼Œå¹¶ä¸”ç±»ä¹‹é—´çš„è·ç¦»ä¸º2.0ã€‚
    - æœ€ç»ˆå¾—åˆ°çš„æ•°æ®é›†åŒ…å«20ä¸ªç‰¹å¾å’Œ1ä¸ªç±»åˆ«æ ‡ç­¾ã€‚

  - å¯ä»¥æ ¹æ®å…·ä½“æƒ…å†µï¼Œè°ƒæ•´`make_classification`å‡½æ•°çš„å„ä¸ªå‚æ•°æ¥ç”Ÿæˆä¸åŒçš„åˆ†ç±»æ•°æ®é›†ã€‚
    - ä¾‹å¦‚ï¼Œå¯ä»¥é€šè¿‡è°ƒæ•´`n_samples`å‚æ•°æ¥æ§åˆ¶æ•°æ®é›†çš„è§„æ¨¡ï¼Œé€šè¿‡è°ƒæ•´`n_informative`å‚æ•°æ¥æ§åˆ¶æœ‰ç”¨ç‰¹å¾çš„æ•°é‡ï¼Œé€šè¿‡è°ƒæ•´`class_sep`å‚æ•°æ¥æ§åˆ¶ç±»ä¹‹é—´çš„è·ç¦»ç­‰ç­‰ã€‚



## å‚æ•°è¯´æ˜

### n_informative

åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œ`n_informative`é€šå¸¸æ˜¯ç”Ÿæˆåˆ†ç±»æ•°æ®é›†æ—¶çš„ä¸€ä¸ªå‚æ•°ï¼Œç”¨äºæ§åˆ¶ç”Ÿæˆçš„æ•°æ®é›†ä¸­æœ‰ç”¨ç‰¹å¾çš„æ•°é‡ã€‚

å…·ä½“æ¥è¯´ï¼Œ`n_informative`è¡¨ç¤ºç”Ÿæˆæ•°æ®é›†æ—¶æ¯ä¸ªç±»åˆ«çš„ç‰¹å¾ä¸­æœ‰å¤šå°‘ä¸ªæ˜¯æœ‰ç”¨çš„ï¼Œä¹Ÿå°±æ˜¯èƒ½å¤ŸåŒºåˆ†ä¸åŒç±»åˆ«çš„ç‰¹å¾ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸€ä¸ªäºŒåˆ†ç±»é—®é¢˜ä¸­ï¼Œå¦‚æœè®¾ç½®`n_informative`ä¸º1ï¼Œåˆ™ç”Ÿæˆçš„æ•°æ®é›†ä¸­æ¯ä¸ªç±»åˆ«çš„ç‰¹å¾ä¸­åªæœ‰ä¸€ä¸ªèƒ½å¤ŸåŒºåˆ†ä¸åŒç±»åˆ«ï¼Œå…¶ä½™çš„ç‰¹å¾éƒ½æ˜¯å™ªå£°æˆ–æ— ç”¨ç‰¹å¾ã€‚

é€šè¿‡è°ƒæ•´`n_informative`å‚æ•°ï¼Œå¯ä»¥æ§åˆ¶ç”Ÿæˆçš„æ•°æ®é›†ä¸­æœ‰ç”¨ç‰¹å¾çš„æ•°é‡ï¼Œä»è€Œæ§åˆ¶åˆ†ç±»é—®é¢˜çš„éš¾åº¦ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œè¾ƒå°‘çš„æœ‰ç”¨ç‰¹å¾ä¼šä½¿åˆ†ç±»é—®é¢˜æ›´åŠ å›°éš¾ï¼Œéœ€è¦æ›´å¤æ‚çš„æ¨¡å‹æ‰èƒ½è§£å†³ã€‚è€Œè¾ƒå¤šçš„æœ‰ç”¨ç‰¹å¾åˆ™ä¼šä½¿åˆ†ç±»é—®é¢˜æ›´å®¹æ˜“ï¼Œæ›´ç®€å•çš„æ¨¡å‹å°±èƒ½å¤Ÿå¾—åˆ°è¾ƒå¥½çš„ç»“æœã€‚

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œ`n_informative`ä¸æ˜¯å”¯ä¸€çš„æ§åˆ¶ç”Ÿæˆæ•°æ®é›†éš¾åº¦çš„å‚æ•°ï¼Œå…¶ä»–å‚æ•°å¦‚`n_classes`ã€`n_clusters_per_class`ã€`class_sep`ç­‰ä¹Ÿä¼šå¯¹æ•°æ®é›†çš„éš¾åº¦äº§ç”Ÿå½±å“ã€‚å› æ­¤ï¼Œåœ¨ç”Ÿæˆåˆ†ç±»æ•°æ®é›†æ—¶ï¼Œéœ€è¦ç»¼åˆè€ƒè™‘å¤šä¸ªå‚æ•°çš„å½±å“ï¼Œè°ƒæ•´å‚æ•°æ¥ç”Ÿæˆåˆé€‚çš„æ•°æ®é›†ã€‚

## sklearn metricsğŸˆ

Scikit-learnä¸­çš„`metrics`æ¨¡å—åŒ…å«äº†è®¸å¤šç”¨äºè¯„ä¼°æœºå™¨å­¦ä¹ æ¨¡å‹æ€§èƒ½çš„åº¦é‡æŒ‡æ ‡ï¼Œç”¨äºæ¯”è¾ƒé¢„æµ‹ç»“æœå’ŒçœŸå®ç»“æœä¹‹é—´çš„å·®å¼‚ï¼Œä»¥ç¡®å®šæ¨¡å‹çš„å‡†ç¡®æ€§ã€ç²¾åº¦ã€å¬å›ç‡ã€F1åˆ†æ•°ç­‰æ€§èƒ½ã€‚

`metrics`æ¨¡å—ä¸­å¸¸ç”¨çš„åº¦é‡æŒ‡æ ‡åŒ…æ‹¬ï¼š

- åˆ†ç±»é—®é¢˜åº¦é‡æŒ‡æ ‡ï¼šå¦‚å‡†ç¡®ç‡ï¼ˆaccuracyï¼‰ã€ç²¾ç¡®ç‡ï¼ˆprecisionï¼‰ã€å¬å›ç‡ï¼ˆrecallï¼‰ã€F1åˆ†æ•°ï¼ˆF1 scoreï¼‰ã€ROCæ›²çº¿ï¼ˆROC curveï¼‰ã€AUCå€¼ï¼ˆAUC scoreï¼‰ç­‰ã€‚
- å›å½’é—®é¢˜åº¦é‡æŒ‡æ ‡ï¼šå¦‚å‡æ–¹è¯¯å·®ï¼ˆmean squared errorï¼‰ã€å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆmean absolute errorï¼‰ã€Rå¹³æ–¹ï¼ˆR-squared scoreï¼‰ç­‰ã€‚
- èšç±»é—®é¢˜åº¦é‡æŒ‡æ ‡ï¼šå¦‚è½®å»“ç³»æ•°ï¼ˆsilhouette scoreï¼‰ç­‰ã€‚
- å¤šæ ‡ç­¾åˆ†ç±»é—®é¢˜åº¦é‡æŒ‡æ ‡ï¼šå¦‚æ±‰æ˜æŸå¤±ï¼ˆHamming lossï¼‰ã€Jaccardç›¸ä¼¼åº¦ï¼ˆJaccard similarityï¼‰ç­‰ã€‚

### accuracy_score

- [sklearn.metrics.accuracy_score â€” scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)

#### eg

- ```python
  from sklearn.metrics import accuracy_score
  y_pred = [0, 2, 1, 3]
  y_true = [0, 1, 2, 3]
  res=accuracy_score(y_true, y_pred),accuracy_score(y_true, y_pred, normalize=False)
  #é¢„æµ‹çš„4ä¸ªæ ·æœ¬ä¸­,y_pred[0,3]æ˜¯å¯¹çš„,å…¶ä½™æ˜¯é”™è¯¯çš„
  res_bool=np.array(y_pred)==np.array(y_true)
  print(res,res_bool)
  ```

  - ```bash
    (0.5, 2) [ True False False  True]
    ```

    

- ä¾‹å¦‚ï¼Œä¸‹é¢çš„ä»£ç æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨`metrics`æ¨¡å—ä¸­çš„`accuracy_score`å‡½æ•°è®¡ç®—åˆ†ç±»æ¨¡å‹çš„å‡†ç¡®ç‡ï¼š

  ```python
  from sklearn.datasets import make_classification
  from sklearn.linear_model import LogisticRegression
  from sklearn.model_selection import train_test_split
  from sklearn.metrics import accuracy_score
  
  X, y = make_classification(n_samples=1000, random_state=0)
  X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
  lr = LogisticRegression()
  
  lr.fit(X_train, y_train)
  y_pred = lr.predict(X_test)
  acc = accuracy_score(y_test, y_pred)
  print("Accuracy: {:.2f}".format(acc))
  ```

- ```
  Accuracy: 0.97
  ```

  åœ¨ä¸Šé¢çš„ä»£ç ä¸­ï¼Œä½¿ç”¨`accuracy_score`å‡½æ•°è®¡ç®—æ¨¡å‹çš„å‡†ç¡®ç‡ï¼Œå¹¶è¾“å‡ºç»“æœã€‚å¯ä»¥æ ¹æ®å…·ä½“æƒ…å†µé€‰æ‹©åˆé€‚çš„åº¦é‡æŒ‡æ ‡æ¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚

### accuracy_score vs cross_validate

- `cross_validate`å’Œ`accuracy_score`éƒ½æ˜¯Scikit-learnä¸­ç”¨äºè¯„ä¼°æ¨¡å‹æ€§èƒ½çš„å‡½æ•°ï¼Œä½†å®ƒä»¬çš„åº”ç”¨åœºæ™¯å’Œç”¨æ³•æœ‰æ‰€ä¸åŒã€‚

  - `accuracy_score`å‡½æ•°ç”¨äºè®¡ç®—åˆ†ç±»æ¨¡å‹çš„å‡†ç¡®ç‡ï¼Œå³é¢„æµ‹æ­£ç¡®çš„æ ·æœ¬æ•°å æ€»æ ·æœ¬æ•°çš„æ¯”ä¾‹ã€‚å®ƒåªèƒ½è®¡ç®—æ¨¡å‹çš„å•ä¸€æŒ‡æ ‡ï¼Œä¸è€ƒè™‘æ¨¡å‹çš„å…¶ä»–æ€§èƒ½æŒ‡æ ‡ï¼Œä¹Ÿä¸è€ƒè™‘æ¨¡å‹çš„è®­ç»ƒæ—¶é—´å’Œé¢„æµ‹æ—¶é—´ç­‰å…¶ä»–æ–¹é¢çš„æ€§èƒ½ã€‚

  - `cross_validate`å‡½æ•°åˆ™å¯ä»¥åŒæ—¶è®¡ç®—å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ï¼Œä¾‹å¦‚è®­ç»ƒå¾—åˆ†ã€æµ‹è¯•å¾—åˆ†ã€æ‹Ÿåˆæ—¶é—´å’Œé¢„æµ‹æ—¶é—´ç­‰ã€‚è¿™äº›è¯„ä¼°æŒ‡æ ‡å¯ä»¥å¸®åŠ©æˆ‘ä»¬æ›´å…¨é¢åœ°äº†è§£æ¨¡å‹çš„æ€§èƒ½å’Œç‰¹ç‚¹ã€‚æ­¤å¤–ï¼Œ`cross_validate`å‡½æ•°è¿˜å¯ä»¥æŒ‡å®šå¤šä¸ªè¯„ä¼°æŒ‡æ ‡ï¼Œä¾‹å¦‚å‡†ç¡®ç‡ã€ç²¾åº¦ã€å¬å›ç‡ã€F1åˆ†æ•°ç­‰ï¼Œä»è€Œæ›´å…¨é¢åœ°è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚`cross_validate`è¿˜å¯ä»¥ä½¿ç”¨ä¸åŒçš„äº¤å‰éªŒè¯ç­–ç•¥ï¼Œä¾‹å¦‚ç•™ä¸€äº¤å‰éªŒè¯å’Œåˆ†å±‚KæŠ˜äº¤å‰éªŒè¯ç­‰ï¼Œä»¥æ»¡è¶³ä¸åŒçš„éœ€æ±‚ã€‚

- ç»¼ä¸Šæ‰€è¿°ï¼Œ`accuracy_score`å‡½æ•°é€‚ç”¨äºç®€å•çš„åˆ†ç±»æ¨¡å‹è¯„ä¼°ä»»åŠ¡ï¼Œè€Œ`cross_validate`å‡½æ•°é€‚ç”¨äºæ›´å¤æ‚çš„æ¨¡å‹è¯„ä¼°ä»»åŠ¡ï¼Œå¯ä»¥æä¾›æ›´å…¨é¢çš„æ€§èƒ½æŒ‡æ ‡å’Œæ›´çµæ´»çš„äº¤å‰éªŒè¯ç­–ç•¥ã€‚

