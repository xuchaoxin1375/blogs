[toc]

# é›†æˆå­¦ä¹ 

## refs

- [1.11. é›†æˆæ–¹æ³• - sklearn (scikitlearn.com.cn)](https://scikitlearn.com.cn/0.21.3/12/)

- [1.11. Ensemble methods â€” scikit-learn documentation](https://scikit-learn.org/stable/modules/ensemble.html)

## abstract

- **é›†æˆå­¦ä¹ **(ensemble learning)é€šè¿‡**æ„å»ºå¹¶ç»“åˆ**å¤šä¸ªå­¦ä¹ å™¨æ¥å®Œæˆå­¦ä¹ ä»»åŠ¡ï¼Œæœ‰æ—¶ä¹Ÿè¢«ç§°ä¸º**å¤šåˆ†ç±»å™¨ç³»ç»Ÿ**(multi-classifier system)ã€åŸºäºå§”å‘˜ä¼šçš„å­¦ä¹ (committee-based learning)ç­‰.

- é›†æˆå­¦ä¹ çš„**ä¸€èˆ¬ç»“æ„**:å…ˆäº§ç”Ÿä¸€ç»„â€œ**ä¸ªä½“å­¦ä¹ å™¨**â€(`individual learner`)ï¼Œå†ç”¨æŸç§**ç­–ç•¥**å°†å®ƒä»¬ç»“åˆèµ·æ¥.

  - ä¸ªä½“å­¦ä¹ å™¨é€šå¸¸ç”±ä¸€ä¸ª<u>ç°æœ‰çš„å­¦ä¹ ç®—æ³•</u>ä»è®­ç»ƒæ•°æ®äº§ç”Ÿ,ä¾‹å¦‚å†³ç­–æ ‘ç®—æ³•ã€BPç¥ç»ç½‘ç»œç®—æ³•ç­‰,æ­¤æ—¶é›†æˆä¸­åªåŒ…å«<u>åŒç§ç±»å‹çš„ä¸ªä½“å­¦ä¹ å™¨</u>
  - ä¾‹å¦‚â€œå†³ç­–æ ‘é›†æˆâ€ä¸­å…¨æ˜¯å†³ç­–æ ‘ï¼Œâ€œç¥ç»ç½‘ç»œé›†æˆâ€ä¸­å…¨æ˜¯ç¥ç»ç½‘ç»œ,è¿™æ ·çš„é›†æˆæ˜¯â€œ**åŒè´¨**â€çš„(homogeneous)ï¼
  - **åŒè´¨é›†æˆ**ä¸­çš„ä¸ªä½“å­¦ä¹ å™¨äº¦ç§°â€œ**åŸºå­¦ä¹ å™¨**â€(`base learner`),ç›¸åº”çš„å­¦ä¹ ç®—æ³•ç§°ä¸ºâ€œ**åŸºå­¦ä¹ ç®—æ³•**â€(base learning algorithm).
  - é›†æˆä¹Ÿå¯åŒ…å«<u>ä¸åŒç±»å‹çš„ä¸ªä½“å­¦ä¹ å™¨</u>,ä¾‹å¦‚åŒæ—¶åŒ…å«å†³ç­–æ ‘å’Œç¥ç»ç½‘ç»œ,è¿™æ ·çš„é›†æˆæ˜¯â€œ**å¼‚è´¨**â€çš„(heterogenous).
  - **å¼‚è´¨é›†æˆ**ä¸­çš„ä¸ªä½“å­¦ä¹ å™¨<u>ç”±ä¸åŒçš„å­¦ä¹ ç®—æ³•ç”Ÿæˆ</u>,è¿™æ—¶å°±<u>ä¸å†æœ‰åŸºå­¦ä¹ ç®—æ³•</u>;
  - ç›¸åº”çš„,ä¸ªä½“å­¦ä¹ å™¨ä¸€èˆ¬ä¸ç§°ä¸ºåŸºå­¦ä¹ å™¨,å¸¸ç§°ä¸ºâ€œ**ç»„ä»¶å­¦ä¹ å™¨**â€(`component learner`)æˆ–ç›´æ¥ç§°ä¸º**ä¸ªä½“å­¦ä¹ å™¨**.

- ```mermaid
  flowchart LR
  	ä¸ªä½“å­¦ä¹ å™¨-->BL["åŸºå­¦ä¹ å™¨(åŒè´¨é›†æˆ)"] & CL["ç»„ä»¶å­¦ä¹ å™¨(å¼‚è´¨é›†æˆ)"]
  ```

- é›†æˆå­¦ä¹ é€šè¿‡å°†å¤šä¸ª**å­¦ä¹ å™¨**è¿›è¡Œç»“åˆ,å¸¸å¯è·å¾—æ¯”å•ä¸€å­¦ä¹ å™¨æ˜¾è‘—ä¼˜è¶Šçš„**æ³›åŒ–æ€§èƒ½**.

  - è¿™å¯¹â€œ**å¼±å­¦ä¹ å™¨**â€(weak learner)å°¤ä¸ºæ˜æ˜¾,å› æ­¤é›†æˆå­¦ä¹ çš„å¾ˆå¤šç†è®ºç ”ç©¶éƒ½æ˜¯é’ˆå¯¹**å¼±å­¦ä¹ å™¨**è¿›è¡Œçš„,
  - åŸºå­¦ä¹ å™¨æœ‰æ—¶ä¹Ÿè¢«ç›´æ¥ç§°ä¸º**å¼±å­¦ä¹ å™¨**

- ä½†éœ€æ³¨æ„çš„æ˜¯,è™½ç„¶ä»ç†è®ºä¸Šæ¥è¯´ä½¿ç”¨å¼±å­¦ä¹ å™¨é›†æˆè¶³ä»¥è·å¾—å¥½çš„æ€§èƒ½,ä½†åœ¨å®è·µä¸­å‡ºäºç§ç§è€ƒè™‘ï¼Œä¾‹å¦‚å¸Œæœ›ä½¿ç”¨è¾ƒå°‘çš„ä¸ªä½“å­¦ä¹ å™¨,æˆ–æ˜¯é‡ç”¨å…³äºå¸¸è§å­¦ä¹ å™¨çš„ä¸€äº›ç»éªŒç­‰ï¼Œäººä»¬<u>å¾€å¾€ä¼šä½¿ç”¨æ¯”è¾ƒå¼ºçš„å­¦ä¹ å™¨.</u>

- åœ¨ä¸€èˆ¬ç»éªŒä¸­,å¦‚æœæŠŠå¥½åä¸ç­‰çš„ä¸œè¥¿æºåˆ°ä¸€èµ·,é‚£ä¹ˆ<u>é€šå¸¸ç»“æœä¼šæ˜¯æ¯”æœ€åçš„è¦å¥½ä¸€äº›,æ¯”æœ€å¥½çš„è¦åä¸€äº›</u>.

  - é›†æˆå­¦ä¹ æŠŠå¤šä¸ªå­¦ä¹ å™¨ç»“åˆèµ·æ¥,å¦‚ä½•èƒ½è·å¾—æ¯”æœ€å¥½çš„å•ä¸€å­¦ä¹ å™¨æ›´å¥½çš„æ€§èƒ½å‘¢?

- è€ƒè™‘ä¸€ä¸ªç®€å•çš„ä¾‹å­:åœ¨äºŒåˆ†ç±»ä»»åŠ¡ä¸­,å‡å®šä¸‰ä¸ªåˆ†ç±»å™¨åœ¨ä¸‰ä¸ªæµ‹è¯•æ ·æœ¬ä¸Šçš„è¡¨ç°:

  - ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/dcaca546bff1419ca173da30fc94a4ca.png)
  - å…¶ä¸­1è¡¨ç¤ºå¯¹åº”çš„åˆ†ç±»å™¨åˆ†ç±»æ­£ç¡®,-1è¡¨ç¤ºåˆ†ç±»é”™è¯¯,å…¶ä¸­$h_i$è¡¨ç¤ºç¬¬$i$ä¸ª'ä¸ªä½“å­¦ä¹ å™¨',Hè¡¨ç¤ºå¯¹ä¸ªä½“å­¦ä¹ å™¨çš„é›†æˆ
  - é›†æˆå­¦ä¹ çš„ç»“æœé€šè¿‡æŠ•ç¥¨æ³•(voting)äº§ç”Ÿ,å³â€œå°‘æ•°æœä»å¤šæ•°â€.
  - åœ¨(a)ä¸­,æ¯ä¸ªåˆ†ç±»å™¨éƒ½åªæœ‰66.6%çš„ç²¾åº¦,ä½†é›†æˆå­¦ä¹ å´è¾¾åˆ°äº†100%;
  - åœ¨(b)ä¸­,ä¸‰ä¸ªåˆ†ç±»å™¨æ²¡æœ‰å·®åˆ«,é›†æˆä¹‹åæ€§èƒ½æ²¡æœ‰æé«˜,ä¾ç„¶ä¸º66.6%;
  - åœ¨(c)ä¸­ï¼Œæ¯ä¸ªåˆ†ç±»å™¨çš„ç²¾åº¦éƒ½åªæœ‰33.3%ï¼Œé›†æˆå­¦ä¹ çš„ç»“æœå˜å¾—æ›´ç³Ÿ.

- ç»¼ä¸Š:è¦è·å¾—å¥½çš„é›†æˆ,ä¸ªä½“å­¦ä¹ å™¨åº”â€œå¥½è€Œä¸åŒâ€

  - ä¸ªä½“å­¦ä¹ å™¨è¦æœ‰ä¸€å®šçš„â€œå‡†ç¡®æ€§(accuracy)â€ï¼Œå³å­¦ä¹ å™¨äºä¸èƒ½å¤ªå,
  - å¹¶ä¸”è¦æœ‰â€œå¤šæ ·æ€§â€(diversity)ï¼Œå³å­¦ä¹ å™¨é—´å…·æœ‰å·®å¼‚.

- ä»¥äºŒåˆ†ç±»ä¸ºä¾‹åˆ†æ:

  - è®¾$y\in\{-1,+1\}$,çœŸå®å‡½æ•°$f$,å‡å®šä¸ªä½“åˆ†ç±»å™¨çš„é”™è¯¯ç‡ä¸º$\epsilon$,å¯¹äºæ¯ä¸ªåŸºåˆ†ç±»å™¨$h_i$:

    - $$
      P(h_i(\mathbf{x})\neq{f(\mathbf{x})})=\epsilon
      $$

  - å‡è®¾é›†æˆæ˜¯é€šè¿‡ç®€å•æŠ•ç¥¨æ³•ç»“åˆTä¸ªåŸºåˆ†ç±»å™¨,è‹¥æœ‰è¶…è¿‡åŠæ•°çš„åŸºåˆ†ç±»å™¨é¢„æµ‹ç»“æœæ˜¯å¯¹çš„,é‚£ä¹ˆé›†æˆåˆ†ç±»å™¨å¯¹æ ·æœ¬çš„é¢„æµ‹ä¹Ÿä¼šæ˜¯å¯¹çš„:

    - $$
      H(\mathbf{x})=\text{sign}\left(
      \sum_{i=1}^{T}h_i(\mathbf{x})
      \right)
      $$

    - å…¶ä¸­,å¦‚æœ$h_i$å¯¹æ ·æœ¬$\boldsymbol{x}$çš„é¢„æµ‹æ˜¯æ­£ç¡®çš„,åˆ™$h_i(\boldsymbol{x})=1$,å¦åˆ™å–$-1$

    - å› æ­¤å¦‚æœè¶…è¿‡åŠæ•°åŸºåˆ†ç±»å™¨æ˜¯æ­£ç¡®çš„,é‚£ä¹ˆ$\sum_{i=1}^{T}h_i(\boldsymbol{x})>0$ä»è€Œ$H(\boldsymbol{x})=+1$å³,Hçš„é¢„æµ‹æ­£ç¡®

- å¦‚æœç†æƒ³çš„å‡è®¾$h_i$çš„é”™è¯¯ç‡ç›¸äº’ç‹¬ç«‹,æ ¹æ®Hoeffdingä¸ç­‰å¼,é›†æˆHçš„é”™è¯¯ç‡:

  - $$
    P(H(x)\neq{f(x)})=\sum_{k=0}^{\lfloor{T/2}\rfloor}
    \binom{T}{k}(1-\epsilon)^{k}\epsilon^{T-k}
    \leqslant
    \exp
    \left(
    -\frac{1}{2}T(1-2\epsilon)^2
    \right)
    $$

  - éšç€Tçš„å¢å¤§,Hçš„é”™è¯¯ç‡å°†æŒ‡æ•°çº§ä¸‹é™,æœ€ç»ˆè¶‹äº0

  - ä¸Šé¢çš„åˆ†ææœ‰ä¸€ä¸ªå…³é”®å‡è®¾:åŸºå­¦ä¹ å™¨çš„è¯¯å·®ç›¸äº’ç‹¬ç«‹.

- åœ¨ç°å®ä»»åŠ¡ä¸­,**ä¸ªä½“å­¦ä¹ å™¨**æ˜¯ä¸ºè§£å†³åŒä¸€ä¸ªé—®é¢˜è®­ç»ƒå‡ºæ¥çš„,å®ƒä»¬æ˜¾ç„¶**ä¸å¯èƒ½ç›¸äº’ç‹¬ç«‹**!

### å‡†ç¡®æ€§å’Œå¤šæ ·æ€§

- ä¸ªä½“å­¦ä¹ å™¨çš„â€œå‡†ç¡®æ€§â€å’Œâ€œå¤šæ ·æ€§â€æœ¬èº«å°±å­˜åœ¨å†²çª,ä¸€èˆ¬çš„
  - å‡†ç¡®æ€§å¾ˆé«˜ä¹‹åï¼Œè¦å¢åŠ **å¤šæ ·æ€§**å°±éœ€ç‰ºç‰²**å‡†ç¡®æ€§**.
- å¦‚ä½•äº§ç”Ÿå¹¶ç»“åˆâ€œ**å¥½è€Œä¸åŒ**â€çš„ä¸ªä½“å­¦ä¹ å™¨ï¼Œæ˜¯é›†æˆå­¦ä¹ ç ”ç©¶çš„æ ¸å¿ƒ.

# ä¸ªä½“å­¦ä¹ å™¨çš„ç”Ÿæˆæ–¹å¼

- æ ¹æ®**ä¸ªä½“å­¦ä¹ å™¨**çš„ç”Ÿæˆæ–¹å¼ï¼Œç›®å‰çš„é›†æˆå­¦ä¹ æ–¹æ³•å¤§è‡´å¯åˆ†ä¸ºä¸¤å¤§ç±»
  - ä¸ªä½“å­¦ä¹ å™¨é—´å­˜åœ¨å¼ºä¾èµ–å…³ç³»ã€å¿…é¡»ä¸²è¡Œç”Ÿæˆçš„åºåˆ—åŒ–æ–¹æ³•ï¼Œä»£è¡¨æ˜¯`Boosting`,
  - ä¸ªä½“å­¦ä¹ å™¨é—´ä¸å­˜åœ¨å¼ºä¾èµ–å…³ç³»ã€å¯åŒæ—¶ç”Ÿæˆçš„å¹¶è¡ŒåŒ–æ–¹æ³•,ä»£è¡¨æ˜¯`Bagging`å’Œâ€œéšæœºæ£®æ—â€(`Random Forest`).

## Boosting

- Boostingæ˜¯ä¸€æ—å¯ä»¥å°†**å¼±å­¦ä¹ å™¨**æå‡ä¸ºå¼ºå­¦ä¹ å™¨çš„ç®—æ³•.
- å·¥ä½œæœºåˆ¶:
  - å…ˆä»åˆå§‹è®­ç»ƒé›†(è®¾åˆ†å¸ƒä¸ºD)è®­ç»ƒå‡ºä¸€ä¸ª**åŸºå­¦ä¹ å™¨**$h_1$
  - æ ¹æ®åŸºå­¦ä¹ å™¨$h_1$çš„è¡¨ç°å¯¹è®­ç»ƒ**æ ·æœ¬åˆ†å¸ƒ**è¿›è¡Œè°ƒæ•´(æ–°åˆ†å¸ƒ$D_1$),ä½¿å¾—å…ˆå‰åŸºå­¦ä¹ å™¨($h_1$)åšé”™çš„è®­ç»ƒæ ·æœ¬åœ¨åç»­å—åˆ°æ›´å¤šå…³æ³¨
  - åŸºäºè°ƒæ•´åçš„æ ·æœ¬åˆ†å¸ƒ$D_1$æ¥è®­ç»ƒä¸‹ä¸€ä¸ªåŸºå­¦ä¹ å™¨
  - åå¤è¿›è¡Œ,æŒ‡å¯¼åŸºå­¦ä¹ å™¨æ•°é‡è¾¾åˆ°äº‹å…ˆæŒ‡å®šçš„å€¼T
  - å¯¹Tä¸ªåŸºå­¦ä¹ å™¨è¿›è¡Œ**åŠ æƒç»“åˆ**

### AdaBoost

- AdaBoostæ˜¯Boostingæ—ç®—æ³•çš„è‘—åä»£è¡¨ã€‚

  - **AdaBoost**ä¸ºè‹±æ–‡"Adaptive Boosting"ï¼ˆè‡ªé€‚åº”å¢å¼ºï¼‰çš„ç¼©å†™ã€‚
  - AdaBoostæ–¹æ³•çš„è‡ªé€‚åº”åœ¨äºï¼šå‰ä¸€ä¸ªåˆ†ç±»å™¨åˆ†é”™çš„æ ·æœ¬ä¼šè¢«ç”¨æ¥è®­ç»ƒä¸‹ä¸€ä¸ªåˆ†ç±»å™¨ã€‚
  - è™½ç„¶AdaBoostæ–¹æ³•å¯¹äºå™ªå£°æ•°æ®å’Œå¼‚å¸¸æ•°æ®å¾ˆæ•æ„Ÿã€‚ä½†åœ¨ä¸€äº›é—®é¢˜ä¸­ï¼ŒAdaBoostæ–¹æ³•ç›¸å¯¹äºå¤§å¤šæ•°å…¶å®ƒå­¦ä¹ ç®—æ³•è€Œè¨€ï¼Œä¸ä¼šå¾ˆå®¹æ˜“å‡ºç°è¿‡æ‹Ÿåˆç°è±¡ã€‚
  - AdaBoostæ–¹æ³•ä¸­ä½¿ç”¨çš„åˆ†ç±»å™¨å¯èƒ½å¾ˆå¼±ï¼ˆæ¯”å¦‚å‡ºç°å¾ˆå¤§é”™è¯¯ç‡ï¼‰ï¼Œ
    - åªè¦å®ƒçš„åˆ†ç±»æ•ˆæœæ¯”éšæœºå¥½ä¸€ç‚¹ï¼ˆæ¯”å¦‚ä¸¤ç±»é—®é¢˜åˆ†ç±»é”™è¯¯ç‡ç•¥å°äº0.5ï¼‰ï¼Œå°±èƒ½å¤Ÿæ”¹å–„æœ€ç»ˆå¾—åˆ°çš„æ¨¡å‹ã€‚
    - é”™è¯¯ç‡é«˜äº**éšæœºåˆ†ç±»å™¨**(å¯¹äºä¸€ä¸ªåˆ†ç±»é—®é¢˜ï¼ŒéšæœºçŒœæµ‹ç­”æ¡ˆçš„åˆ†ç±»å™¨)çš„å¼±åˆ†ç±»å™¨ä¹Ÿæ˜¯æœ‰ç”¨çš„ï¼Œå› ä¸ºåœ¨æœ€ç»ˆå¾—åˆ°çš„å¤šä¸ªåˆ†ç±»å™¨çš„çº¿æ€§ç»„åˆä¸­ï¼Œå¯ä»¥ç»™å®ƒä»¬èµ‹äºˆ**è´Ÿç³»æ•°**ï¼ŒåŒæ ·<u>ä¹Ÿèƒ½æå‡åˆ†ç±»æ•ˆæœ</u>ã€‚
  - AdaBoostæ–¹æ³•æ˜¯ä¸€ç§è¿­ä»£ç®—æ³•ï¼Œåœ¨æ¯ä¸€è½®ä¸­åŠ å…¥ä¸€ä¸ª<u>æ–°çš„å¼±åˆ†ç±»å™¨</u>ï¼Œç›´åˆ°è¾¾åˆ°æŸä¸ªé¢„å®šçš„è¶³å¤Ÿå°çš„é”™è¯¯ç‡ã€‚
    - æ¯ä¸€ä¸ªè®­ç»ƒæ ·æœ¬éƒ½è¢«èµ‹äºˆä¸€ä¸ªæƒé‡ï¼Œè¡¨æ˜å®ƒè¢«æŸä¸ªåˆ†ç±»å™¨é€‰å…¥è®­ç»ƒé›†çš„æ¦‚ç‡ã€‚
      - å¦‚æœæŸä¸ª**æ ·æœ¬ç‚¹**å·²ç»è¢«å‡†ç¡®åœ°åˆ†ç±»ï¼Œé‚£ä¹ˆåœ¨æ„é€ ä¸‹ä¸€ä¸ªè®­ç»ƒé›†ä¸­ï¼Œå®ƒè¢«é€‰ä¸­çš„æ¦‚ç‡å°±è¢«é™ä½ï¼›
      - ç›¸åï¼Œå¦‚æœæŸä¸ªæ ·æœ¬ç‚¹æ²¡æœ‰è¢«å‡†ç¡®åœ°åˆ†ç±»ï¼Œé‚£ä¹ˆå®ƒçš„æƒé‡å°±å¾—åˆ°æé«˜ã€‚
    - é€šè¿‡è¿™æ ·çš„æ–¹å¼ï¼ŒAdaBoostæ–¹æ³•èƒ½â€œèšç„¦äºâ€é‚£äº›è¾ƒéš¾åˆ†ï¼ˆæ›´å¯Œä¿¡æ¯ï¼‰çš„æ ·æœ¬ä¸Šã€‚
    - åœ¨å…·ä½“å®ç°ä¸Šï¼Œ
      - æœ€åˆä»¤æ¯ä¸ªæ ·æœ¬çš„æƒé‡éƒ½ç›¸ç­‰ã€‚
      - å¯¹äºç¬¬kæ¬¡è¿­ä»£æ“ä½œï¼Œæˆ‘ä»¬å°±æ ¹æ®è¿™äº›æƒé‡æ¥é€‰å–æ ·æœ¬ç‚¹ï¼Œè¿›è€Œè®­ç»ƒåˆ†ç±»å™¨$C_k$ã€‚ç„¶åå°±æ ¹æ®è¿™ä¸ªåˆ†ç±»å™¨ï¼Œæ¥æé«˜è¢«å®ƒåˆ†é”™çš„çš„æ ·æœ¬çš„æƒé‡ï¼Œå¹¶é™ä½è¢«æ­£ç¡®åˆ†ç±»çš„æ ·æœ¬æƒé‡ã€‚
      - ç„¶åï¼Œæƒé‡æ›´æ–°è¿‡çš„æ ·æœ¬é›†è¢«ç”¨äºè®­ç»ƒä¸‹ä¸€ä¸ªåˆ†ç±»å™¨ã€‚
      - æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹å¦‚æ­¤è¿­ä»£åœ°è¿›è¡Œä¸‹å»ã€‚

- è®¾$y_i\in\{-1,1\}$,$f$ä¸ºçœŸå®å‡½æ•°:

  - **åŠ æ€§æ¨¡å‹**(additive model):åŸºå­¦ä¹ å™¨çš„çº¿æ€§ç»„åˆ,ä¸€ç§æ˜“äºç†è§£çš„AdaBoostæ¨å¯¼æ–¹å¼:

  - $$
    H(\mathbf{x})=\sum_{t=1}^{T}\alpha_th_{t}(\mathbf{x})
    $$

- $H(\boldsymbol{x})$æ¥æœ€å°åŒ–**æŒ‡æ•°æŸå¤±å‡½æ•°**(exponential loss function)

  - $$
    L_{exp}(H|D)=E_{\mathbf{x}\sim{D}}[\exp(-f(\mathbf{x})H(\mathbf{x}))]
    $$

#### ä¼ªä»£ç 

- input

  - è®­ç»ƒé›†$D=\{(\boldsymbol{x}_1,y_1),(\boldsymbol{x}_2,y_2),\cdots,(\boldsymbol{x}_m,y_m)\}$
  - åŠå­¦ä¹ ç®—æ³•$\mathfrak{L}$
  - è®­ç»ƒè½®æ•°$T$

- $$
  \begin{array}{l}
  \text { 1: } \mathcal{D}_{1}(\boldsymbol{x})=1 / m \text {. } \\
  \text { 2: }\text{for } t=1,2, \ldots, T \text { do } \\
  \text { 3: } \quad h_{t}=\mathfrak{L}\left(D, \mathcal{D}_{t}\right) \text {; } \\
  \text { 4: } \quad \epsilon_{t}=P_{\boldsymbol{x} \sim \mathcal{D}_{t}}\left(h_{t}(\boldsymbol{x}) \neq f(\boldsymbol{x})\right) \text {; } \\
  \text { 5: } \quad \text { if } \epsilon_{t}>0.5 \text { then break } \\
  \text { 6: } \quad \alpha_{t}=\frac{1}{2} \ln \left(\frac{1-\epsilon_{t}}{\epsilon_{t}}\right) \text {; } \\
  \begin{array}{l}
      \text { 7: } \quad \mathcal{D}_{t+1}(\boldsymbol{x})
      &=\frac{\mathcal{D}_{t}(\boldsymbol{x})}{Z_{t}} \times\left\{\begin{array}{ll}
      \exp \left(-\alpha_{t}\right), & \text { if } h_{t}
      (\boldsymbol{x})=f(\boldsymbol{x}) \\
      \exp \left(\alpha_{t}\right), & \text { if } h_{t}(\boldsymbol{x}) 
      \neq f(\boldsymbol{x})
      \end{array}\right. \\
      &=\frac{\mathcal{D}_{t}(\boldsymbol{x}) \exp \left(-\alpha_{t} 
      f(\boldsymbol{x}) h_{t}(\boldsymbol{x})\right)}{Z_{t}} 
  \end{array}
  \\
  8: \text{end for}\\
  \end{array}
  $$

- output:

  - $$
    H(\boldsymbol{x})=\operatorname{sign}\left(\sum_{t=1}^{T} \alpha_{t} h_{t}(\boldsymbol{x})\right)
    $$

- comments:
  - 1:åˆå§‹åŒ–æ ·æœ¬æƒå€¼åˆ†å¸ƒä¸º$\frac{1}{m}$
  - 3:åŸºäºåˆ†å¸ƒ$\mathcal{D}_t$ä»æ•°æ®é›†Dä¸­è®­ç»ƒå‡ºæ¥çš„åˆ†ç±»å™¨$h_t$
  - 4:ä¼°è®¡$h_t$çš„è¯¯å·®$\epsilon_t$
  - 6:ç¡®å®šåˆ†ç±»å™¨$h_t$çš„æƒé‡
  - 7:æ›´æ–°æ ·æœ¬åˆ†å¸ƒ,$Z_t$æ˜¯è§„èŒƒåŒ–å› å­,ç¡®ä¿$\mathcal{D}_{t+1}$æ˜¯ä¸€ä¸ªåˆ†å¸ƒ
  - ä¸Šè¿°ç®—æ³•ä¸­ç”¨åˆ°çš„å‡ ä¸ªå…¬å¼çš„æ¨å¯¼è¾ƒä¸ºå¤æ‚,å¯å‚è€ƒç›¸å…³èµ„æ–™æ–‡çŒ®(è¥¿ç“œä¹¦ç»™å‡ºçš„æ¨å¯¼æ˜¯:åŸºäºåŠ æ€§æ¨¡å‹è¿­ä»£å¼ä¼˜åŒ–æŒ‡æ•°æŸå¤±å‡½æ•°çš„è§’åº¦è¿›è¡Œæ¨å¯¼)

#### Adaboostå°ç»“

- AdaBoostï¼ˆAdaptive Boostingï¼‰ç®—æ³•æ˜¯ä¸€ç§é›†æˆå­¦ä¹ æ–¹æ³•ï¼Œå®ƒé€šè¿‡ç»“åˆå¤šä¸ªå¼±å­¦ä¹ å™¨æ¥æ„å»ºä¸€ä¸ªå¼ºå­¦ä¹ å™¨ã€‚AdaBoostçš„æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨æ¯è½®è¿­ä»£ä¸­ï¼Œæ ¹æ®å‰ä¸€è½®çš„é¢„æµ‹é”™è¯¯è°ƒæ•´æ ·æœ¬æƒé‡å’Œå¼±å­¦ä¹ å™¨æƒé‡ï¼Œå¾—åç»­çš„å¼±å­¦ä¹ å™¨æ›´å…³æ³¨é‚£äº›è¢«å‰ä¸€è½®å¼±å­¦ä¹ å™¨é”™è¯¯åˆ†ç±»çš„æ ·æœ¬ã€‚æœ€åï¼Œå°†æ‰€æœ‰å¼±å­¦ä¹ å™¨çš„é¢„æµ‹ç»“æœåŠ æƒç»“åˆï¼Œå¾—åˆ°ç»ˆçš„é¢„æµ‹ç»“æœã€‚

  AdaBoostç®—æ³•çš„ä¸»è¦æ­¥éª¤å¦‚ä¸‹ï¼š

  1. åˆå§‹åŒ–æ ·æœ¬æƒé‡ï¼šå°†è®­ç»ƒé›†ä¸­æ¯ä¸ªæ ·æœ¬çš„æƒé‡è®¾ç½®ä¸ºç›¸ç­‰çš„å€¼ï¼Œå³1/Nï¼Œå…¶ä¸­Næ˜¯è®­ç»ƒé›†çš„æ ·æœ¬æ•°é‡ã€‚
  2. å¯¹äºæ¯ä¸€è½®è¿­ä»£ï¼š a. ä½¿ç”¨å¸¦é‡çš„è®­ç»ƒé›†è®­ç»ƒä¸€ä¸ªå¼±å­¦ä¹ å™¨ã€‚ b. è®¡ç®—å¼±å­¦ä¹ å™¨åœ¨è®­ç»ƒé›†ä¸Šçš„åˆ†ç±»é”™è¯¯ç‡ã€‚ c. è®¡ç®—å¼±ä¹ å™¨çš„æƒé‡ï¼Œè¯¥æƒé‡ä¸å¼±å­¦ä¹ å™¨çš„åˆ†ç±»é”™è¯¯ç‡æˆåæ¯”ã€‚ d. æ›´æ–°æ ·æœ¬æƒé‡ï¼šå¢åŠ è¢«é”™è¯¯åˆ†ç±»æ ·æœ¬çš„æƒé‡ï¼Œå‡å°‘æ­£ç¡®åˆ†ç±»æ ·æœ¬çš„æƒé‡ã€‚ e. å¯¹æ ·æœ¬æƒé‡è¿›è¡Œå½’ä¸€åŒ–ï¼Œä½¿å…¶æ€»å’Œä¸º1ã€‚
  3. å°†æ‰€æœ‰å¼±å­¦å™¨çš„é¢„æµ‹ç»“æœåŠ æƒç»“åˆï¼Œå¾—åˆ°æœ€ç»ˆçš„é¢„æµ‹ç»“æœã€‚

  AdaBoostç®—æ³•çš„ä¼˜ç‚¹ï¼š

  1. æ˜“å®ç°ï¼šAdaBoostç®—æ³•å®ç°ç›¸å¯¹ç®€å•ï¼Œåªéœ€åœ¨æ¯è½®è¿­ä»£ä¸­è°ƒæ•´æ ·æœ¬æƒé‡å’Œå¼±å­¦ä¹ å™¨æƒé‡å³å¯ã€‚
  2. è‡ªé€‚åº”æ€§ï¼šAdaBoostç®—æ³•èƒ½å¤Ÿè‡ªé€‚åº”åœ°è°ƒæ•´æ ·æœ¬æƒé‡ï¼Œä½¿å¾—åç»­çš„å¼±å­¦ä¹ å™¨æ›´åŠ å…³æ³¨é‚£äº›è¢«å‰ä¸€è½®å¼±å­¦ä¹ å™¨é”™è¯¯åˆ†ç±»çš„æœ¬ã€‚
  3. é˜²æ­¢è¿‡æ‹Ÿåˆï¼šé€šè¿‡ç»“åˆå¤šä¸ªå¼±å­¦ä¹ å™¨ï¼ŒAdaBoostç®—æ³•å¯ä»¥é™ä½å‹çš„æ–¹å·®ï¼Œä»è€Œå‡å°‘è¿‡åˆçš„é£é™©ã€‚

  AdaBoostç®—æ³•çš„ç¼ºç‚¹ï¼š

  1. å¯¹å™ªå£°æ•°æ®å’Œå¼‚å¸¸å€¼æ•æ„Ÿï¼šç”±äºAdaBoostç®—æ³•ä¼šå¢åŠ è¢«é”™è¯¯åˆ†ç±»æ ·æœ¬çš„æƒé‡ï¼Œå› æ­¤å¯¹å™ªå£°æ•°æ®å’Œå¼‚å¸¸å€¼è¾ƒä¸ºæ•æ„Ÿï¼Œå¯èƒ½å¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚
  2. é¡ºåºè®­ç»ƒï¼šç”±AdaBoostç®—æ³•æ˜¯ä¸€ä¸ªè¿­ä»£è¿‡ç¨‹ï¼Œæ¯ä¸€è½®è®­ç»ƒéƒ½ä¾èµ–äºå‰ä¸€è½®çš„ç»“æœï¼Œå› æ­¤æ— æ³•å¹¶è¡Œè®­ç»ƒå¼±å­¦ä¹ å™¨ï¼Œè®­ç»ƒé€Ÿåº¦å¯èƒ½è¾ƒæ…¢ã€‚

### å°ç»“1

- Boosting ç®—æ³•è¦æ±‚åŸºå­¦ä¹ å™¨èƒ½å¯¹ç‰¹å®šçš„æ•°æ®åˆ†å¸ƒè¿›è¡Œå­¦ä¹ ,è¿™å¯é€šè¿‡â€œ**é‡èµ‹æƒæ³•**â€(re-weighting)å®æ–½,å³åœ¨è®­ç»ƒè¿‡ç¨‹çš„æ¯ä¸€è½®ä¸­,æ ¹æ®**æ ·æœ¬åˆ†å¸ƒ**ä¸ºæ¯ä¸ªè®­ç»ƒæ ·æœ¬**é‡æ–°èµ‹äºˆä¸€ä¸ªæƒé‡**ï¼
- å¯¹æ— æ³•æ¥å—å¸¦æƒæ ·æœ¬çš„åŸºå­¦ä¹ ç®—æ³•ï¼Œåˆ™å¯é€šè¿‡â€œ**é‡é‡‡æ ·æ³•**â€(re-sampling)æ¥å¤„ç†ï¼Œå³åœ¨æ¯ä¸€è½®å­¦ä¹ ä¸­,æ ¹æ®æ ·æœ¬åˆ†å¸ƒå¯¹**è®­ç»ƒé›†**é‡æ–°è¿›è¡Œé‡‡æ ·,å†ç”¨é‡é‡‡æ ·è€Œå¾—çš„æ ·æœ¬é›†å¯¹åŸºå­¦ä¹ å™¨è¿›è¡Œè®­ç»ƒ.
- ä¸€èˆ¬è€Œè¨€,è¿™ä¸¤ç§åšæ³•æ²¡æœ‰æ˜¾è‘—çš„ä¼˜åŠ£å·®åˆ«.
- éœ€æ³¨æ„çš„æ˜¯,Boosting ç®—æ³•åœ¨è®­ç»ƒçš„æ¯ä¸€è½®éƒ½è¦æ£€æŸ¥å½“å‰ç”Ÿæˆçš„åŸºå­¦ä¹ å™¨<u>æ˜¯å¦æ»¡è¶³åŸºæœ¬æ¡ä»¶</u>
  - (ç®—æ³•ä¼ªä»£ç ç¬¬5è¡Œ,æ£€æŸ¥å½“å‰åŸºåˆ†ç±»å™¨æ˜¯å¦æ˜¯æ¯”éšæœºçŒœæµ‹å¥½)ï¼Œä¸€æ—¦æ¡ä»¶ä¸æ»¡è¶³,åˆ™å½“å‰åŸºå­¦ä¹ å™¨å³è¢«æŠ›å¼ƒ,ä¸”å­¦ä¹ è¿‡ç¨‹åœæ­¢.
  - åœ¨æ­¤ç§æƒ…å½¢ä¸‹,åˆå§‹è®¾ç½®çš„å­¦ä¹ è½®æ•°Tä¹Ÿè®¸è¿˜è¿œæœªè¾¾åˆ°,å¯èƒ½å¯¼è‡´æœ€ç»ˆé›†æˆä¸­åªåŒ…å«å¾ˆå°‘çš„åŸºå­¦ä¹ å™¨è€Œæ€§èƒ½ä¸ä½³.
- è‹¥é‡‡ç”¨â€œé‡é‡‡æ ·æ³•â€,åˆ™å¯è·å¾—â€œé‡å¯åŠ¨â€æœºä¼šä»¥é¿å…è®­ç»ƒè¿‡ç¨‹è¿‡æ—©åœæ­¢[Kohavi and Wolpert,1996],å³åœ¨æŠ›å¼ƒä¸æ»¡è¶³æ¡ä»¶çš„å½“å‰åŸºå­¦ä¹ å™¨ä¹‹å,å¯æ ¹æ®**å½“å‰åˆ†å¸ƒ**é‡æ–°å¯¹è®­ç»ƒæ ·æœ¬è¿›è¡Œ**é‡‡æ ·**,å†åŸºäºæ–°çš„é‡‡æ ·ç»“æœé‡æ–°è®­ç»ƒå‡ºåŸºå­¦ä¹ å™¨,ä»è€Œä½¿å¾—å­¦ä¹ è¿‡ç¨‹å¯ä»¥æŒç»­åˆ°é¢„è®¾çš„Tè½®å®Œæˆ.
- ä»åå·®-æ–¹å·®åˆ†è§£çš„è§’åº¦çœ‹ï¼ŒBoostingä¸»è¦å…³æ³¨**é™ä½åå·®**,å› æ­¤Boostingå¯¹èƒ½åŸºäºæ³›åŒ–æ€§èƒ½ç›¸å½“å¼±çš„å­¦ä¹ å™¨æ„å»ºå‡ºå¾ˆå¼ºçš„é›†æˆï¼

### å°ç»“2

- Boostingæ˜¯ä¸€ç§é›†æˆå­¦ä¹ æ–¹æ³•ï¼Œå®ƒé€šè¿‡ç»“åˆå¤šä¸ªå¼±å­¦ä¹ å™¨æ¥æ„å»ºä¸€ä¸ªå¼ºå­¦ä¹ å™¨ã€‚Boostingçš„æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨æ¯ä¸€è½®è¿­ä»£ä¸­ï¼Œæ ¹æ®å‰ä¸€è½®çš„é¢„æµ‹é”™è¯¯è°ƒæ•´æ ·æœ¬æƒé‡ï¼Œä½¿å¾—åç»­çš„å¼±å­¦ä¹ å™¨æ›´åŠ å…³æ³¨é‚£äº›è¢«å‰ä¸€è½®å¼±å­¦ä¹ å™¨é”™è¯¯åˆ†ç±»çš„æ ·æœ¬ã€‚æœ€åï¼Œå°†æ‰€æœ‰å¼±å­¦ä¹ å™¨çš„é¢„æµ‹ç»“æœåŠ æƒç»“åˆï¼Œå¾—åˆ°æœ€ç»ˆçš„é¢„æµ‹ç»“æœã€‚

  Boostingæ–¹æ³•çš„ä¸»è¦ç‰¹ç‚¹å¦‚ä¸‹ï¼š

  1. é¡ºåºè®­ç»ƒï¼šBoostingæ–¹æ³•æ˜¯ä¸€ä¸ªè¿­ä»£è¿‡ç¨‹ï¼Œæ¯ä¸€è½®è®­ç»ƒéƒ½ä¾èµ–äºå‰ä¸€è½®çš„ç»“æœã€‚å› æ­¤ï¼ŒBoostingæ–¹æ³•æ— æ³•å¹¶è¡Œè®­ç»ƒå¼±å­¦ä¹ å™¨ã€‚
  2. è¯¯å·®ä¿®æ­£ï¼šBoostingæ–¹æ³•é€šè¿‡è°ƒæ•´æ ·æœ¬æƒé‡ï¼Œä½¿å¾—åç»­çš„å¼±å­¦ä¹ å™¨æ›´åŠ å…³æ³¨é‚£äº›è¢«å‰ä¸€è½®å¼±å­¦ä¹ å™¨é”™è¯¯åˆ†ç±»çš„æ ·æœ¬ã€‚è¿™æœ‰åŠ©äºæé«˜æ¨¡å‹åœ¨å¤æ‚æ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚
  3. é˜²æ­¢è¿‡æ‹Ÿåˆï¼šBoostingæ–¹æ³•é€šè¿‡ç»“åˆå¤šä¸ªå¼±å­¦ä¹ å™¨æ¥é™ä½æ¨¡å‹çš„æ–¹å·®ï¼Œä»è€Œå‡å°‘è¿‡æ‹Ÿåˆçš„é£é™©ã€‚ç„¶è€Œï¼Œå¦‚æœè¿­ä»£æ¬¡æ•°è¿‡å¤šï¼ŒBoostingæ–¹æ³•ä»ç„¶å¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆã€‚

  Boostingæ–¹æ³•çš„å¸¸è§ç®—æ³•åŒ…æ‹¬ï¼š

  1. AdaBoostï¼ˆAdaptive Boostingï¼‰ï¼šAdaBoostæ˜¯æœ€æ—©çš„Boostingç®—æ³•ä¹‹ä¸€ï¼Œå®ƒé€šè¿‡è°ƒæ•´æ ·æœ¬æƒé‡å’Œå¼±å­¦ä¹ å™¨æƒé‡æ¥å®ç°è¯¯å·®ä¿®æ­£ã€‚åœ¨æ¯ä¸€è½®è¿­ä»£ä¸­ï¼ŒAdaBoostä¼šå¢åŠ è¢«é”™è¯¯åˆ†ç±»æ ·æœ¬çš„æƒé‡ï¼ŒåŒæ—¶å‡å°‘æ­£ç¡®åˆ†ç±»æ ·æœ¬çš„æƒé‡ã€‚å¼±å­¦ä¹ å™¨çš„æƒé‡ä¸å…¶åœ¨è®­ç»ƒé›†ä¸Šçš„åˆ†ç±»å‡†ç¡®ç‡æœ‰å…³ã€‚
  2. Gradient Boostingï¼šGradient Boostingæ˜¯ä¸€ç§é€šç”¨çš„Boostingç®—æ³•ï¼Œå®ƒé€šè¿‡ä¼˜åŒ–æŸå¤±å‡½æ•°çš„æ¢¯åº¦æ¥å®ç°è¯¯å·®ä¿®æ­£ã€‚åœ¨æ¯ä¸€è½®è¿­ä»£ä¸­ï¼ŒGradient Boostingä¼šè®­ç»ƒä¸€ä¸ªæ–°çš„å¼±å­¦ä¹ å™¨æ¥æ‹Ÿåˆå‰ä¸€è½®é¢„æµ‹ç»“æœä¸çœŸå®æ ‡ç­¾ä¹‹é—´çš„æ®‹å·®ã€‚æœ€åï¼Œå°†æ‰€æœ‰å¼±å­¦ä¹ å™¨çš„é¢„æµ‹ç»“æœç´¯åŠ ï¼Œå¾—åˆ°æœ€ç»ˆçš„é¢„æµ‹ç»“æœã€‚
  3. XGBoostï¼ˆeXtreme Gradient Boostingï¼‰ï¼šXGBoostæ˜¯Gradient Boostingçš„ä¸€ç§ä¼˜åŒ–å®ç°ï¼Œå®ƒåœ¨åŸå§‹çš„Gradient Boostingç®—æ³•åŸºç¡€ä¸Šå¼•å…¥äº†æ­£åˆ™åŒ–é¡¹ï¼Œä»¥é˜²æ­¢è¿‡æ‹Ÿåˆã€‚æ­¤å¤–ï¼ŒXGBoostè¿˜é‡‡ç”¨äº†ä¸€äº›é«˜æ•ˆçš„è®¡ç®—æŠ€æœ¯ï¼Œå¦‚åˆ—å—å­˜å‚¨ã€ç¼“å­˜æ„ŸçŸ¥è®¿é—®æ¨¡å¼å’Œå¹¶è¡Œè®¡ç®—ï¼Œä»¥æé«˜è®­ç»ƒé€Ÿåº¦ã€‚

  æ€»ä¹‹ï¼ŒBoostingæ˜¯ä¸€ç§é›†æˆå­¦ä¹ æ–¹æ³•ï¼Œå®ƒé€šè¿‡ç»“åˆå¤šä¸ªå¼±å­¦ä¹ å™¨æ¥æ„å»ºä¸€ä¸ªå¼ºå­¦ä¹ å™¨ã€‚Boostingæ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨æ¯ä¸€è½®è¿­ä»£ä¸­ï¼Œæ ¹æ®å‰ä¸€è½®çš„é¢„æµ‹é”™è¯¯è°ƒæ•´æ ·æœ¬æƒé‡ï¼Œä½¿å¾—åç»­çš„å¼±å­¦ä¹ å™¨æ›´åŠ å…³æ³¨é‚£äº›è¢«å‰ä¸€è½®å¼±å­¦ä¹ å™¨é”™è¯¯åˆ†ç±»çš„æ ·æœ¬ã€‚Boostingæ–¹æ³•å¯ä»¥æé«˜æ¨¡å‹åœ¨å¤æ‚æ•°æ®é›†ä¸Šçš„æ€§èƒ½ï¼Œå¹¶é™ä½è¿‡æ‹Ÿåˆçš„é£é™©ã€‚å¸¸è§çš„Boostingç®—æ³•åŒ…æ‹¬AdaBoostã€Gradient Boostingå’ŒXGBoostã€‚

## Baggingä¸éšæœºæ£®æ—

- æ¬²å¾—åˆ°æ³›åŒ–æ€§èƒ½å¼ºçš„é›†æˆ,é›†æˆä¸­çš„<u>ä¸ªä½“å­¦ä¹ å™¨åº”å°½å¯èƒ½ç›¸äº’ç‹¬ç«‹</u>;
- è™½ç„¶â€œç‹¬ç«‹â€åœ¨ç°å®ä»»åŠ¡ä¸­æ— æ³•åšåˆ°,ä½†å¯ä»¥è®¾æ³•ä½¿åŸºå­¦ä¹ å™¨å°½å¯èƒ½å…·æœ‰è¾ƒå¤§çš„å·®å¼‚ç»™å®šä¸€ä¸ªè®­ç»ƒæ•°æ®é›†
  - ä¸€ç§å¯èƒ½çš„åšæ³•æ˜¯å¯¹è®­ç»ƒæ ·æœ¬è¿›è¡Œé‡‡æ ·,äº§ç”Ÿå‡ºè‹¥å¹²ä¸ªä¸åŒçš„å­é›†,å†ä»æ¯ä¸ªæ•°æ®å­é›†ä¸­è®­ç»ƒå‡ºä¸€ä¸ªåŸºå­¦ä¹ å™¨ï¼Œè¿™æ ·ï¼Œç”±äºè®­ç»ƒæ•°æ®ä¸åŒ,æˆ‘ä»¬è·å¾—çš„åŸºå­¦ä¹ å™¨å¯æœ›å…·æœ‰æ¯”è¾ƒå¤§çš„å·®å¼‚
  - åŒæ—¶æˆ‘ä»¬è¿˜å¸Œæœ›ä¸ªä½“å­¦ä¹ å™¨ä¸èƒ½å¤ªå·®
    - å¦‚æœé‡‡æ ·å‡ºçš„æ¯ä¸ªå­é›†éƒ½å®Œå…¨ä¸åŒ,åˆ™æ¯ä¸ªåŸºå­¦ä¹ å™¨åªç”¨åˆ°äº†ä¸€å°éƒ¨åˆ†è®­ç»ƒæ•°æ®ï¼Œç”šè‡³ä¸è¶³ä»¥è¿›è¡Œæœ‰æ•ˆå­¦ä¹ ,è¿™æ˜¾ç„¶æ— æ³•ç¡®ä¿äº§ç”Ÿå‡ºæ¯”è¾ƒå¥½çš„åŸºå­¦ä¹ å™¨.
    - ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜,æˆ‘ä»¬å¯è€ƒè™‘ä½¿ç”¨<u>ç›¸äº’æœ‰äº¤å çš„é‡‡æ ·å­é›†.</u>

### Bagging

- Bagging (<u>B</u>ootstrap <u>Agg</u>ereat<u>ing</u>çš„ç¼©å†™)[Breiman,1996a]æ˜¯**å¹¶è¡Œå¼**é›†æˆå­¦ä¹ æ–¹æ³•æœ€è‘—åçš„ä»£è¡¨.

- Bagging çš„åŸºæœ¬æµç¨‹:

  - å®ƒç›´æ¥åŸºäº**è‡ªåŠ©é‡‡æ ·æ³•**(bootstrap sampling).ç»™å®šåŒ…å«mä¸ªæ ·æœ¬çš„æ•°æ®é›†,æˆ‘ä»¬å…ˆéšæœºå–å‡ºä¸€ä¸ªæ ·æœ¬æ”¾å…¥é‡‡æ ·é›†ä¸­,å†æŠŠè¯¥æ ·æœ¬æ”¾å›åˆå§‹æ•°æ®é›†,ä½¿å¾—ä¸‹æ¬¡é‡‡æ ·æ—¶è¯¥æ ·æœ¬ä»æœ‰å¯èƒ½è¢«é€‰ä¸­,è¿™æ ·,ç»è¿‡mæ¬¡éšæœºé‡‡æ ·æ“ä½œï¼Œæˆ‘ä»¬å¾—åˆ°å«mä¸ªæ ·æœ¬çš„é‡‡æ ·é›†,åˆå§‹è®­ç»ƒé›†ä¸­æœ‰çš„æ ·æœ¬åœ¨é‡‡æ ·é›†é‡Œå¤šæ¬¡å‡ºç°,æœ‰çš„åˆ™ä»æœªå‡ºç°.(æ ¹æ®ç»Ÿè®¡è§„å¾‹æ±‚æé™(mè¶³å¤Ÿå¤§æ—¶)å¯çŸ¥åˆå§‹è®­ç»ƒé›†ä¸­çº¦æœ‰63.2%çš„æ ·æœ¬å‡ºç°åœ¨é‡‡æ ·é›†ä¸­.)

  - æˆ‘ä»¬å¯é‡‡æ ·å‡ºTä¸ªå«mä¸ªè®­ç»ƒæ ·æœ¬çš„é‡‡æ ·é›†,ç„¶ååŸºäº<u>æ¯ä¸ªé‡‡æ ·é›†è®­ç»ƒ</u>å‡ºä¸€ä¸ª**åŸºå­¦ä¹ å™¨**,å†å°†è¿™äº›åŸºå­¦ä¹ å™¨è¿›è¡Œç»“åˆ.

- å¯¹<u>é¢„æµ‹è¾“å‡º</u>è¿›è¡Œ**ç»“åˆ**,Bagging é€šå¸¸
  - å¯¹<u>åˆ†ç±»ä»»åŠ¡</u>ä½¿ç”¨**ç®€å•æŠ•ç¥¨æ³•**,
  - å¯¹å›å½’ä»»åŠ¡ä½¿ç”¨**ç®€å•å¹³å‡æ³•**
    - è‹¥åˆ†ç±»é¢„æµ‹æ—¶å‡ºç°ä¸¤ä¸ªç±»æ”¶åˆ°åŒæ ·ç¥¨æ•°çš„æƒ…å½¢ï¼Œåˆ™æœ€ç®€å•çš„åšæ³•æ˜¯éšæœºé€‰æ‹©ä¸€ä¸ª
    - ä¹Ÿå¯è¿›ä¸€æ­¥è€ƒå¯Ÿå­¦ä¹ å™¨æŠ•ç¥¨çš„ç½®ä¿¡åº¦æ¥ç¡®å®šæœ€ç»ˆèƒœè€…. 
  - Note:æ¯ä¸ªåŸºå­¦ä¹ å™¨ä½¿ç”¨ç›¸åŒçš„æƒé‡å’Œå¹³å‡

#### ä¼ªä»£ç 

- Baggingçš„ç®—æ³•æè¿°(ä¼ªä»£ç )

- input:

  - è®­ç»ƒé›†$D=\{(\boldsymbol{x}_1,y_1),(\boldsymbol{x}_2,y_2),\cdots,(\boldsymbol{x}_m,y_m)\}$;
  - åŠå­¦ä¹ ç®—æ³•$\mathfrak{L}$
  - è®­ç»ƒè½®æ•°$T$

- $$
  \begin{array}{l}
  \text{1:}& \text{for}\:t=1,2,\cdots,T\:\text{do}\\
  \text{2:}&\quad h_t=\mathfrak{L}(D,\mathcal{D_{bs}}) \\
  \text{3:}&\text{end for}
  \end{array}
  $$

- output:

  - $$
    H(\boldsymbol{x})=\underset{y\in{\gamma}}{\arg\max}
    \sum_{t=1}^{T}\mathbb{I}(h_t(\boldsymbol{x})=y)
    $$

### ç‰¹ç‚¹

#### ç®—æ³•æ•ˆç‡

- å‡å®šåŸºå­¦ä¹ å™¨çš„è®¡ç®—å¤æ‚åº¦ä¸º$O(m)$ï¼Œåˆ™ Bagging çš„å¤æ‚åº¦å¤§è‡´ä¸º$T(O(m)ï¼‹O(s))$ï¼Œè€ƒè™‘åˆ°é‡‡æ ·ä¸æŠ•ç¥¨/å¹³å‡è¿‡ç¨‹çš„å¤æ‚åº¦$O(s)$å¾ˆå°ï¼Œè€Œ$T$é€šå¸¸æ˜¯ä¸€ä¸ªä¸å¤ªå¤§çš„å¸¸æ•°,å› æ­¤,è®­ç»ƒä¸€ä¸ªBaggingé›†æˆä¸ç›´æ¥ä½¿ç”¨åŸºå­¦ä¹ ç®—æ³•è®­ç»ƒä¸€ä¸ªå­¦ä¹ å™¨çš„å¤æ‚åº¦**åŒé˜¶**,è¿™è¯´æ˜Baggingæ˜¯ä¸€ä¸ªå¾ˆé«˜æ•ˆçš„é›†æˆå­¦ä¹ ç®—æ³•.

#### ç›´æ¥åº”ç”¨äºå¤šåˆ†ç±»

- ä¸æ ‡å‡†AdaBoost åªé€‚ç”¨äºäºŒåˆ†ç±»ä»»åŠ¡ä¸åŒ,Bagging èƒ½ä¸ç»ä¿®æ”¹åœ°ç”¨äºå¤šåˆ†ç±»ã€å›å½’ç­‰ä»»åŠ¡.

### è‡ªåŠ©é‡‡æ ·å’ŒåŒ…å¤–ä¼°è®¡

- è‡ªåŠ©é‡‡æ ·è¿‡ç¨‹è¿˜ç»™Bagging å¸¦æ¥äº†å¦ä¸€ä¸ªä¼˜ç‚¹:ç”±äºæ¯ä¸ªåŸºå­¦ä¹ å™¨åªä½¿ç”¨äº†åˆå§‹è®­ç»ƒé›†ä¸­çº¦63.2%çš„æ ·æœ¬,å‰©ä¸‹çº¦36.8%çš„æ ·æœ¬å¯ç”¨ä½œ**éªŒè¯é›†**æ¥å¯¹**æ³›åŒ–æ€§èƒ½**è¿›è¡Œ**åŒ…å¤–ä¼°è®¡**(out-of-bag estimate,`oob`)[Breimanï¼Œ1996a;Wolpert and Macready,1999].

- ä¸ºæ­¤éœ€è®°å½•æ¯ä¸ªåŸºå­¦ä¹ å™¨æ‰€ä½¿ç”¨çš„è®­ç»ƒæ ·æœ¬.

  - ä¸å¦¨ä»¤$D_t$è¡¨ç¤º$h_t$ å®é™…ä½¿ç”¨çš„è®­ç»ƒæ ·æœ¬é›†

  - ä»¤$H^{oob}(\boldsymbol{x})$è¡¨ç¤ºå¯¹æ ·æœ¬$\boldsymbol{x}$ çš„åŒ…å¤–é¢„æµ‹,å³ä»…è€ƒè™‘é‚£äº›æœªä½¿ç”¨$\boldsymbol{x}$è®­ç»ƒçš„åŸºå­¦ä¹ å™¨åœ¨$\boldsymbol{x}$ä¸Šçš„é¢„æµ‹,æœ‰

    - $$
      H^{oob}(\boldsymbol{x})=\underset{y\in{\gamma}}{\arg\max}
      \sum_{t=1}^{T}\mathbb{I}(h_t(\boldsymbol{x})=y)\cdot
      \mathbb{I}(\boldsymbol{x}\notin{D_t})
      $$

  - Baggingçš„**æ³›åŒ–è¯¯å·®**çš„**åŒ…å¤–ä¼°è®¡**ä¸º:

    - $$
      \epsilon^{oob}=\frac{1}{|D|}
      \sum_{(\boldsymbol{x},y)\in{D}}
      \mathbb{I}(H^{oob}(\boldsymbol{x})\neq{y})
      $$

      

- äº‹å®ä¸Š,åŒ…å¤–æ ·æœ¬è¿˜æœ‰è®¸å¤šå…¶ä»–ç”¨é€”

  - ä¾‹å¦‚å½“åŸºå­¦ä¹ å™¨æ˜¯å†³ç­–æ ‘æ—¶,å¯ä½¿ç”¨åŒ…å¤–æ ·æœ¬æ¥<u>è¾…åŠ©å‰ªæ</u>,æˆ–ç”¨äºä¼°è®¡å†³ç­–æ ‘ä¸­å„ç»“ç‚¹çš„<u>åéªŒæ¦‚ç‡</u>ä»¥è¾…åŠ©å¯¹é›¶è®­ç»ƒæ ·æœ¬ç»“ç‚¹çš„å¤„ç†;
  - å½“åŸºå­¦ä¹ å™¨æ˜¯ç¥ç»ç½‘ç»œæ—¶,å¯ä½¿ç”¨åŒ…å¤–æ ·æœ¬æ¥è¾…åŠ©æ—©æœŸåœæ­¢ä»¥å‡å°è¿‡æ‹Ÿåˆé£é™©.

- ä»åå·®-æ–¹å·®åˆ†è§£çš„è§’åº¦çœ‹ï¼ŒBaggingä¸»è¦å…³æ³¨**é™ä½æ–¹å·®**,å› æ­¤å®ƒåœ¨ä¸å‰ªæå†³ç­–æ ‘ã€ç¥ç»ç½‘ç»œç­‰æ˜“å—æ ·æœ¬æ‰°åŠ¨çš„å­¦ä¹ å™¨ä¸Šæ•ˆç”¨æ›´ä¸ºæ˜æ˜¾.

### éšæœºæ£®æ—

- éšæœºæ£®æ—(Random Forestï¼Œç®€ç§°RF) [Breimanï¼Œ2001a]æ˜¯ Bagging çš„ä¸€ä¸ªæ‰©å±•å˜ä½“. 
- RFåœ¨ä»¥**å†³ç­–æ ‘**ä¸ºåŸºå­¦ä¹ å™¨æ„å»ºBagging é›†æˆçš„åŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥åœ¨å†³ç­–æ ‘çš„è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥äº†**éšæœºå±æ€§**é€‰æ‹©.
- ä¼ ç»Ÿå†³ç­–æ ‘åœ¨é€‰æ‹©åˆ’åˆ†å±æ€§æ—¶æ˜¯åœ¨å½“å‰ç»“ç‚¹çš„å±æ€§é›†åˆ(å‡å®šæœ‰dä¸ªå±æ€§)ä¸­é€‰æ‹©ä¸€ä¸ªæœ€ä¼˜å±æ€§;
- è€Œåœ¨RFä¸­,å¯¹åŸºå†³ç­–æ ‘çš„æ¯ä¸ª**ç»“ç‚¹**,å…ˆä»è¯¥ç»“ç‚¹çš„**å±æ€§é›†åˆ**ä¸­**éšæœºé€‰æ‹©**ä¸€ä¸ªåŒ…å«kä¸ªå±æ€§çš„å­é›†,ç„¶åå†ä»è¿™ä¸ªå­é›†ä¸­é€‰æ‹©ä¸€ä¸ªæœ€ä¼˜å±æ€§ç”¨äºåˆ’åˆ†.
- è¿™é‡Œçš„å‚æ•°kæ§åˆ¶äº†éšæœºæ€§çš„å¼•å…¥ç¨‹åº¦;
  - è‹¥ä»¤k = d,åˆ™åŸºå†³ç­–æ ‘çš„æ„å»ºä¸ä¼ ç»Ÿå†³ç­–æ ‘ç›¸åŒ;
  - è‹¥ä»¤k= 1ï¼Œåˆ™æ˜¯éšæœºé€‰æ‹©ä¸€ä¸ªå±æ€§ç”¨äºåˆ’åˆ†;
  - ä¸€èˆ¬æƒ…å†µä¸‹,æ¨èå€¼$k = \log_2d$[Breimanï¼Œ2001a].
  - éšæœºæ£®æ—ç®€å•ã€å®¹æ˜“å®ç°ã€è®¡ç®—å¼€é”€å°,å®ƒåœ¨å¾ˆå¤šç°å®ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ€§èƒ½,è¢«èª‰ä¸ºâ€œ<u>ä»£è¡¨é›†æˆå­¦ä¹ æŠ€æœ¯æ°´å¹³çš„æ–¹æ³•</u>â€.
- éšæœºæ£®æ—å¯¹Baggingåªåšäº†å°æ”¹åŠ¨,ä½†æ˜¯ä¸Bagging ä¸­åŸºå­¦ä¹ å™¨çš„â€œå¤šæ ·æ€§â€**ä»…é€šè¿‡æ ·æœ¬æ‰°åŠ¨**(é€šè¿‡å¯¹åˆå§‹è®­ç»ƒé›†é‡‡æ ·)è€Œæ¥ä¸åŒ,éšæœºæ£®æ—ä¸­åŸºå­¦ä¹ å™¨çš„å¤šæ ·æ€§<u>ä¸ä»…æ¥è‡ªæ ·æœ¬æ‰°åŠ¨,è¿˜æ¥è‡ª**å±æ€§æ‰°åŠ¨**</u>,è¿™å°±ä½¿å¾—æœ€ç»ˆé›†æˆçš„æ³›åŒ–æ€§èƒ½å¯é€šè¿‡ä¸ªä½“å­¦ä¹ å™¨ä¹‹é—´<u>å·®å¼‚åº¦çš„å¢åŠ </u>è€Œè¿›ä¸€æ­¥æå‡.
- éšæœºæ£®æ—çš„æ”¶æ•›æ€§ä¸ Bagging ç›¸ä¼¼
- éšæœºæ£®æ—çš„èµ·å§‹æ€§èƒ½å¾€å¾€ç›¸å¯¹è¾ƒå·®,ç‰¹åˆ«æ˜¯åœ¨é›†æˆä¸­åªåŒ…å«ä¸€ä¸ªåŸºå­¦ä¹ å™¨æ—¶.
  - è¿™å¾ˆå®¹æ˜“ç†è§£ï¼Œå› ä¸ºé€šè¿‡å¼•å…¥å±æ€§æ‰°åŠ¨,éšæœºæ£®æ—ä¸­ä¸ªä½“å­¦ä¹ å™¨çš„æ€§èƒ½å¾€å¾€æœ‰æ‰€é™ä½.ç„¶è€Œ,éšç€ä¸ªä½“å­¦ä¹ å™¨æ•°ç›®çš„å¢åŠ ï¼Œéšæœºæ£®æ—é€šå¸¸ä¼šæ”¶æ•›åˆ°æ›´ä½çš„æ³›åŒ–è¯¯å·®.
  - å€¼å¾—ä¸€æçš„æ˜¯,éšæœºæ£®æ—çš„è®­ç»ƒæ•ˆç‡å¸¸ä¼˜äºBagging,å› ä¸ºåœ¨ä¸ªä½“å†³ç­–æ ‘çš„æ„å»ºè¿‡ç¨‹ä¸­ï¼ŒBaggingä½¿ç”¨çš„æ˜¯â€œç¡®å®šå‹â€å†³ç­–æ ‘,åœ¨é€‰æ‹©åˆ’åˆ†å±æ€§æ—¶è¦å¯¹ç»“ç‚¹çš„æ‰€æœ‰å±æ€§è¿›è¡Œè€ƒå¯Ÿï¼Œè€Œéšæœºæ£®æ—ä½¿ç”¨çš„â€œéšæœºå‹â€å†³ç­–æ ‘åˆ™åªéœ€è€ƒå¯Ÿä¸€ä¸ªå±æ€§å­é›†.

## Stacking

- [sklearn.ensemble.StackingClassifier â€” scikit-learn  documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html)

- `<<Mastering Machine Learning with scikit-learn v2>>,`ç®€ç§°MMLWS

### sklearnä¸­çš„StackingğŸˆ

- Stackingé›†æˆæ–¹æ³•åœ¨0.22ç‰ˆæœ¬åŠ å…¥ï¼š[Version 0.22.2.post1 â€” scikit-learn documentation](https://scikit-learn.org/stable/whats_new/v0.22.html#version-0-22-0)

- Stacked generalization is a method for combining estimators to reduce their biases [[W1992\]](https://scikit-learn.org/stable/modules/ensemble.html#w1992) [[HTF\]](https://scikit-learn.org/stable/modules/ensemble.html#htf). More precisely, the predictions of each individual estimator are stacked together and used as input to a final estimator to compute the prediction. This final estimator is trained through cross-validation.

  The [`StackingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier) and [`StackingRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingRegressor.html#sklearn.ensemble.StackingRegressor) provide such strategies which can be applied to classification and regression problems.

#### æ„é€ åˆçº§å­¦ä¹ å™¨

- The `estimators` parameter corresponds to the list of the estimators which are stacked together in parallel on the input data. It should be given as a list of names and estimators:

  ```python
  from sklearn.linear_model import RidgeCV, LassoCV
  from sklearn.neighbors import KNeighborsRegressor
  estimators = [('ridge', RidgeCV()),
                ('lasso', LassoCV(random_state=42)),
                ('knr', KNeighborsRegressor(n_neighbors=20,
                                            metric='euclidean'))]
  ```

- å †å æ³›åŒ–ï¼ˆStacked Generalizationï¼‰æ˜¯ä¸€ç§å°†ä¼°ç®—å™¨ç»„åˆèµ·æ¥ä»¥å‡å°‘å…¶åå·®çš„æ–¹æ³•[W1992] [HTF]ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œå°†æ¯ä¸ªç‹¬ç«‹ä¼°ç®—å™¨çš„é¢„æµ‹ç»“æœå †å åœ¨ä¸€èµ·ï¼Œå¹¶å°†å…¶ç”¨ä½œæœ€ç»ˆä¼°ç®—å™¨çš„è¾“å…¥æ¥è®¡ç®—é¢„æµ‹ç»“æœã€‚è¿™ä¸ªæœ€ç»ˆä¼°ç®—å™¨æ˜¯é€šè¿‡äº¤å‰éªŒè¯è¿›è¡Œè®­ç»ƒçš„ã€‚

  StackingClassifierå’ŒStackingRegressoræä¾›äº†è¿™æ ·çš„ç­–ç•¥ï¼Œå¯ä»¥åº”ç”¨äºåˆ†ç±»å’Œå›å½’é—®é¢˜ã€‚

  estimatorså‚æ•°å¯¹åº”äºåœ¨è¾“å…¥æ•°æ®ä¸Šå¹¶è¡Œå †å åœ¨ä¸€èµ·çš„ä¼°ç®—å™¨åˆ—è¡¨ã€‚å®ƒåº”è¯¥ä½œä¸ºåç§°å’Œä¼°ç®—å™¨çš„åˆ—è¡¨ç»™å‡º

#### æ„é€ æ¬¡çº§å­¦ä¹ å™¨

- The `final_estimator` will use the predictions of the `estimators` as input. It needs to be a classifier or a regressor when using [`StackingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier) or [`StackingRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingRegressor.html#sklearn.ensemble.StackingRegressor), respectively:

  - ```python
    from sklearn.ensemble import GradientBoostingRegressor
    from sklearn.ensemble import StackingRegressor
    #å®šä¹‰æ¬¡çº§å­¦ä¹ å™¨
    final_estimator = GradientBoostingRegressor(
        n_estimators=25, subsample=0.5, min_samples_leaf=25, max_features=1,
        random_state=42)
    #å°†åˆçº§å­¦ä¹ å™¨å’Œæ¬¡çº§å­¦ä¹ å™¨å †å ,å¾—åˆ°ä¸€ä¸ªé›†æˆçš„å †å å­¦ä¹ å™¨
    reg = StackingRegressor(
        estimators=estimators,
        final_estimator=final_estimator)
    ```

- To train the `estimators` and `final_estimator`, the `fit` method needs to be called on the training data:

  - ```python
    # åŠ è½½ç³–å°¿ç—…æ•°æ®(åšå›å½’å›å½’ä»»åŠ¡)
    from sklearn.datasets import load_diabetes
    X, y = load_diabetes(return_X_y=True)
    #åˆ©ç”¨ttsæ–¹æ³•å°†æ•°æ®é›†ç®€å•åˆ†å‰²
    # åšåŸºæœ¬çš„ç•™å‡ºæ³•å®éªŒ
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=42)
    #åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒ
    reg.fit(X_train, y_train)
    ```

- During training, the `estimators` are fitted on the whole training data `X_train`. They will be used when calling `predict` or `predict_proba`. 

- To generalize and avoid over-fitting, the `final_estimator` is trained on **out-samples** using [`sklearn.model_selection.cross_val_predict`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html#sklearn.model_selection.cross_val_predict) internally.

- For [`StackingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier), note that the **output** of the `estimators` is controlled by the parameter `stack_method` and it is called by each estimator. This parameter is either a **string**, being estimator method names, or `'auto'` which will automatically identify an available method depending on the availability, tested in the **order** of preference: `predict_proba`, `decision_function` and `predict`.

- A [`StackingRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingRegressor.html#sklearn.ensemble.StackingRegressor) and [`StackingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier) can be used as any other regressor or classifier, exposing a `predict`, `predict_proba`, and `decision_function` methods, e.g.:

  - ```python
    y_pred = reg.predict(X_test)
    # è®¡ç®—r2åˆ†æ•°
    from sklearn.metrics import r2_score
    print('R2 score: {:.2f}'.format(r2_score(y_test, y_pred)))
    
    ```

- åœ¨è®­ç»ƒæœŸé—´ï¼Œä¼°ç®—å™¨å°†åœ¨æ•´ä¸ªè®­ç»ƒæ•°æ®X_trainä¸Šè¿›è¡Œæ‹Ÿåˆã€‚åœ¨è°ƒç”¨predictæˆ–predict_probaæ—¶ï¼Œå®ƒä»¬å°†è¢«ç”¨äºé¢„æµ‹ã€‚ä¸ºäº†æ³›åŒ–å’Œé¿å…è¿‡åº¦æ‹Ÿåˆï¼Œfinal_estimatorå°†ä½¿ç”¨sklearn.model_selection.cross_val_predictåœ¨out-of-sampleä¸Šè¿›è¡Œè®­ç»ƒã€‚

  å¯¹äºStackingClassifierï¼Œè¯·æ³¨æ„ä¼°ç®—å™¨çš„è¾“å‡ºç”±stack_methodå‚æ•°æ§åˆ¶ï¼Œå¹¶ç”±æ¯ä¸ªä¼°ç®—å™¨è°ƒç”¨ã€‚è¯¥å‚æ•°å¯ä»¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå³ä¼°ç®—å™¨æ–¹æ³•åç§°ï¼Œæˆ–è€…æ˜¯â€œautoâ€ï¼Œå®ƒå°†æ ¹æ®å¯ç”¨æ€§è‡ªåŠ¨è¯†åˆ«ä¸€ç§æ–¹æ³•ï¼Œé¦–é€‰é¡ºåºä¸ºï¼špredict_probaã€decision_functionå’Œpredictã€‚

  StackingRegressorå’ŒStackingClassifierå¯ä»¥åƒå…¶ä»–å›å½’å™¨æˆ–åˆ†ç±»å™¨ä¸€æ ·ä½¿ç”¨ï¼Œæš´éœ²å‡ºpredictã€predict_probaå’Œdecision_functionæ–¹æ³•

### å¤šå±‚å †å 

-  Multiple stacking layers can be achieved by assigning `final_estimator` to a [`StackingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier) or [`StackingRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingRegressor.html#sklearn.ensemble.StackingRegressor):

- ```python
  from sklearn.datasets import load_diabetes
  
  X, y = load_diabetes(return_X_y=True)
  from sklearn.model_selection import train_test_split
  
  X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
  
  from sklearn.ensemble import (
      RandomForestRegressor,
      GradientBoostingRegressor,
      StackingRegressor,
  )
  from sklearn.neighbors import KNeighborsRegressor
  from sklearn.linear_model import RidgeCV, LassoCV
  
  #ç¬¬ä¸€å±‚å †å å™¨
  # ä¸ªä½“å­¦ä¹ å™¨rfr,gbr(# (éƒ½æ˜¯é›†æˆå­¦ä¹ å™¨,æŒ‡å®šè¾ƒå¤šè¶…å‚æ•°))
  final_layer_rfr = RandomForestRegressor(
      n_estimators=10, max_features=1, max_leaf_nodes=5, random_state=42
  )
  final_layer_gbr = GradientBoostingRegressor(
      n_estimators=10, max_features=1, max_leaf_nodes=5, random_state=42
  )
  
  # ç¬¬ä¸€å±‚å…ƒå­¦ä¹ å™¨ä½¿ç”¨Ridge,å¹¶å°†ç¬¬ä¸€å±‚ä¸ªä½“å­¦ä¹ å™¨å…±åŒæ‰“åŒ…ä¸ºç¬¬ä¸€å±‚å †å å™¨
  final_layer = StackingRegressor(
      estimators=[("rf", final_layer_rfr), ("gbrt", final_layer_gbr)],
      final_estimator=RidgeCV(),
  )
  #æ„å»ºç¬¬äºŒå±‚å †å å™¨
  # ä¸ªä½“å­¦ä¹ å™¨(ä½¿ç”¨é»˜è®¤å‚æ•°,ä¹¦å†™æ›´å…·ç´§å‡‘å’Œç®€æ´),æ¬¡çº§å­¦ä¹ å™¨ä½¿ç”¨ç¬¬ä¸€å±‚å †å å™¨
  multi_layer_regressor = StackingRegressor(
      estimators=[
          ("ridge", RidgeCV()),
          ("lasso", LassoCV(random_state=42)),
          ("knr", KNeighborsRegressor(n_neighbors=20, metric="euclidean")),
      ],
      final_estimator=final_layer,
  )
  # è®­ç»ƒè¿™ä¸ªå¤šå±‚å †å å›å½’å™¨
  multi_layer_regressor.fit(X_train, y_train)
  # è¯„ä¼°åˆ†æ•°
  print("R2 score: {:.2f}".format(multi_layer_regressor.score(X_test, y_test)))
  ```
  
  

#### Stackingç‰¹ç‚¹

- In practice, a stacking predictor predicts <u>as good as the best predictor of the base layer</u> and even sometimes **outperforms** it by combining the different **strengths** of the these predictors. 
- However, training a stacking predictor is computationally expensive.

  å®é™…ä¸Šï¼Œå †å é¢„æµ‹å™¨çš„é¢„æµ‹ç»“æœå’ŒåŸºå±‚çš„æœ€ä½³é¢„æµ‹å™¨ä¸€æ ·å¥½ï¼Œç”šè‡³æœ‰æ—¶ä¼šé€šè¿‡ç»“åˆè¿™äº›é¢„æµ‹å™¨çš„ä¸åŒä¼˜ç‚¹è€Œè¡¨ç°æ›´å¥½ã€‚ç„¶è€Œï¼Œè®­ç»ƒå †å é¢„æµ‹å™¨çš„è®¡ç®—æˆæœ¬å¾ˆé«˜ã€‚

### StackingClassifier

- Stack of estimators with a final classifier.

  Stacked generalization consists in stacking the output of individual estimator and use a classifier to compute the final prediction. Stacking allows to use the strength of each individual estimator by using their output as input of a final estimator.

  Note that `estimators_` are fitted on the full `X` while `final_estimator_` is trained using cross-validated predictions of the base estimators using `cross_val_predict`.

  å…·æœ‰æœ€ç»ˆåˆ†ç±»å™¨çš„ä¼°è®¡å™¨å †æ ˆã€‚

  å †å æ³›åŒ–çš„æ–¹æ³•æ˜¯å°†æ¯ä¸ªä¼°è®¡å™¨çš„è¾“å‡ºå †å èµ·æ¥ï¼Œå¹¶ä½¿ç”¨ä¸€ä¸ªåˆ†ç±»å™¨æ¥è®¡ç®—æœ€ç»ˆé¢„æµ‹ç»“æœã€‚å°†æ¯ä¸ªä¼°è®¡å™¨çš„è¾“å‡ºä½œä¸ºæœ€ç»ˆä¼°è®¡å™¨çš„è¾“å…¥ï¼Œå †å èƒ½å¤Ÿåˆ©ç”¨æ¯ä¸ªä¼°è®¡å™¨çš„ä¼˜åŠ¿ã€‚

  è¯·æ³¨æ„ï¼Œ`estimators_`æ˜¯åœ¨å®Œæ•´çš„Xä¸Šæ‹Ÿåˆçš„ï¼Œè€Œ`final_estimator_`æ˜¯ä½¿ç”¨**åŸºä¼°è®¡å™¨**çš„äº¤å‰éªŒè¯é¢„æµ‹æ¥è®­ç»ƒçš„ï¼Œä½¿ç”¨äº†`cross_val_predict`æ–¹æ³•ã€‚

#### Note

- For [`StackingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier), when using `stack_method_='predict_proba'`, the first column is dropped when the problem is a binary classification problem. Indeed, both probability columns predicted by each estimator are perfectly collinear.

#### demo

1. å¯¼å…¥æ‰€éœ€çš„åº“å’Œæ¨¡å—ï¼š

```python
from sklearn.datasets import load_iris
 sklearn.ensemble import RandomForestClassifier
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.ensemble import StackingClassifier
```

1. åŠ è½½é¸¢å°¾èŠ±æ•°æ®é›†ï¼š

```python
X, y = load_iris(return_X_y=True)
```

`X`æ˜¯ä¸€ä¸ªåŒ…å«é¸¢å°¾èŠ±é›†çš„ç‰¹å¾çš„çŸ©é˜µï¼Œ`y`æ˜¯ä¸€ä¸ªåŒ…å«é¸¢å°¾èŠ±æ•°æ®é›†çš„ç›®æ ‡ç±»åˆ«çš„å‘é‡ã€‚

- å®šä¹‰åŸºæœ¬åˆ†ç±»å™¨ï¼ˆåŸºæ¨¡å‹ï¼‰ï¼š

  - ```python
    estimators = [
        ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),
        ('svr', make_pipeline(StandardScaler(),
                              LinearSVC(random_state=42)))
    ]
    ```

  - è¿™é‡Œå®šä¹‰äº†ä¸¤ä¸ªåŸºæœ¬åˆ†ç±»å™¨ï¼šä¸€ä¸ªæ˜¯éšæœºæ£®æ—åˆ†ç±»å™¨ï¼ˆ`RandomForestClassifier`ï¼‰ï¼Œå¦ä¸€ä¸ªæ˜¯æ€§æ”¯æŒå‘é‡æœºåˆ†ç±»å™¨ï¼ˆ`LinearSVC`ï¼‰ã€‚çº¿æ€§æ”¯æŒå‘é‡æœºåˆ†ç±»å™¨ä½¿ç”¨`make_pipeline`å‡½æ•°ä¸`StandardScaler`é¢„å¤„ç†å™¨ç»„åˆï¼Œä»¥ç¡®ä¿åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹æ•°æ®è¿›è¡Œå‡†åŒ–ã€‚

- å®šä¹‰å †å åˆ†ç±»å™¨ï¼š

  - ```python
    clf = StackingClassifier(
        estimators=estimators, final_estimator=LogisticRegression()
    )
    ```

  - å †å åˆ†ç±»å™¨ï¼ˆ`StackingClassifier`ï¼‰ä½¿ç”¨åŸºæœ¬åˆ†ç±»å™¨ï¼ˆ`estimators`ï¼‰å’Œä¸€ä¸ªæœ€ç»ˆçš„å…ƒåˆ†ç±»å™¨ï¼ˆ`final_estimator`ï¼‰ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­å…ƒåˆ†ç±»å™¨æ˜¯ä¸€ä¸ªé€»è¾‘å›å½’åˆ†ç±»å™¨ï¼ˆ`LogisticRegression`ï¼‰ã€‚

- å°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š

  - ```python
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, stratify=y, random_state=42
    )
    ```

  - `train_test_split`å‡½æ•°å°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå…¶ä¸­`stratify=y`ç¡®è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­çš„ç±»åˆ«åˆ†å¸ƒä¸åŸå§‹æ•°æ®é›†ç›¸åŒã€‚

- è®­ç»ƒå †å åˆ†ç±»å™¨å¹¶è¯„ä¼°å…¶æ€§èƒ½ï¼š

  - ```python
    clf.fit(X_train, y_train).score(X_test, y_test)
    ```

- `fit`æ–¹æ³•ç”¨äºè®­ç»ƒå †å åˆ†ç±»å™¨ï¼Œ`score`æ–¹æ³•ç”¨äºä¼°åˆ†ç±»å™¨åœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®æ€§ã€‚è¿™å°†è¿”å›ä¸€ä¸ªä»‹äº0å’Œ1ä¹‹é—´çš„åˆ†æ•°ï¼Œè¡¨ç¤ºåˆ†ç±»å™¨åœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®æ€§æ€»ä¹‹ï¼Œè¿™æ®µä»£ç å®ç°äº†ä¸€ä¸ªå †å åˆ†ç±»å™¨ï¼Œç”¨äºå¯¹é¸¢å°¾èŠ±æ•°æ®é›†è¿›è¡Œåˆ†ç±»ã€‚å †å åˆ†ç±»å™¨ç»“åˆäº†å¤šä¸ªåŸºæœ¬åˆ†ç±»å™¨çš„é¢„æµ‹ï¼Œå¹¶ä½¿ç”¨å…ƒåˆ†ç±»å™¨æ¥ç”Ÿæˆæœ€ç»ˆçš„é¢„æµ‹ç»“æœã€‚è¿™ç§æ–¹æ³•å¯ä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚



### Stackingä¸»è¦å†…å®¹

#### ç›¸å…³è®ºæ–‡å’Œæœ¯è¯­

- åœ¨Wolpertçš„è®ºæ–‡ "Stacked generalization"ï¼ˆå †å æ³›åŒ–ï¼‰ä¸­ï¼Œä»‹ç»äº†ä¸€ç§é›†æˆå­¦ä¹ æ–¹æ³•ï¼Œå³Stackingã€‚ä»¥ä¸‹æ˜¯è®ºæ–‡ä¸­çš„ä¸€äº›å…³é”®æœ¯è¯­åŠå…¶ä¸­è‹±æ–‡å¯¹ç…§è§£é‡Šï¼š

  1. Stacked Generalizationï¼ˆå †å æ³›åŒ–ï¼‰ï¼šä¸€ç§é›†æˆå­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆå¤šä¸ªåŸºå­¦ä¹ å™¨çš„é¢„æµ‹ç»“æœï¼Œç”Ÿæˆä¸€ä¸ªæ–°çš„ã€æ›´å¼ºå¤§çš„æ¨¡å‹ï¼ˆå…ƒå­¦ä¹ å™¨ï¼‰ã€‚
  2. Generalizerï¼ˆæ³›åŒ–å™¨ï¼‰ï¼šåœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæ³›åŒ–å™¨æ˜¯æŒ‡æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå®ƒå¯ä»¥æ˜¯åŸºå­¦ä¹ å™¨æˆ–å…ƒå­¦ä¹ å™¨ã€‚
  3. Level-0 Generalizerï¼ˆ0çº§æ³›åŒ–å™¨ï¼‰ï¼šåˆç§°ä¸ºåŸºå­¦ä¹ å™¨ï¼Œæ˜¯åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒçš„å¤šä¸ªä¸åŒç±»å‹çš„æ¨¡å‹ã€‚
  4. Level-1 Generalizerï¼ˆ1çº§æ³›åŒ–å™¨ï¼‰ï¼šåˆç§°ä¸ºå…ƒå­¦ä¹ å™¨ï¼Œæ˜¯ä¸€ä¸ªæ¨¡å‹ï¼Œä½¿ç”¨åŸºå­¦ä¹ å™¨ç”Ÿæˆçš„å…ƒç‰¹å¾è¿›è¡Œè®­ç»ƒã€‚
  5. Meta-featuresï¼ˆå…ƒç‰¹å¾ï¼‰ï¼šåŸºå­¦ä¹ å™¨å¯¹è®­ç»ƒé›†è¿›è¡Œé¢„æµ‹æ‰€ç”Ÿæˆçš„æ–°ç‰¹å¾ï¼Œç”¨äºè®­ç»ƒå…ƒå­¦ä¹ å™¨ã€‚
  6. Cross-validationï¼ˆäº¤å‰éªŒè¯ï¼‰ï¼šä¸€ç§è¯„ä¼°æ¨¡å‹æ€§èƒ½çš„æ–¹æ³•ï¼Œå°†æ•°æ®é›†åˆ’åˆ†ä¸ºKä¸ªå­é›†ï¼Œæ¯æ¬¡ä½¿ç”¨K-1ä¸ªå­é›†è¿›è¡Œè®­ç»ƒï¼Œå‰©ä¸‹çš„å­é›†è¿›è¡ŒéªŒè¯ã€‚è¿™ä¸ªè¿‡ç¨‹é‡å¤Kæ¬¡ï¼Œæ¯æ¬¡ä½¿ç”¨ä¸åŒçš„å­é›†è¿›è¡ŒéªŒè¯ã€‚
  7. Biasï¼ˆåå·®ï¼‰ï¼šæ¨¡å‹é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„å·®å¼‚ï¼Œè¡¡é‡äº†æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚åå·®è¾ƒå¤§çš„æ¨¡å‹å¯èƒ½æ— æ³•æ•æ‰åˆ°æ•°æ®ä¸­çš„åŸºæœ¬å…³ç³»ã€‚
  8. Varianceï¼ˆæ–¹å·®ï¼‰ï¼šæ¨¡å‹é¢„æµ‹å€¼åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„å˜åŒ–ç¨‹åº¦ï¼Œè¡¡é‡äº†æ¨¡å‹çš„ç¨³å®šæ€§ã€‚æ–¹å·®è¾ƒå¤§çš„æ¨¡å‹å¯èƒ½å¯¹è®­ç»ƒæ•°æ®è¿‡åº¦æ‹Ÿåˆã€‚
  9. Overfittingï¼ˆè¿‡æ‹Ÿåˆï¼‰ï¼šæ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æ–°æ•°æ®ä¸Šè¡¨ç°è¾ƒå·®ã€‚è¿™é€šå¸¸æ˜¯å› ä¸ºæ¨¡å‹è¿‡äºå¤æ‚ï¼Œæ•æ‰åˆ°äº†è®­ç»ƒæ•°æ®ä¸­çš„å™ªå£°ã€‚
  10. Underfittingï¼ˆæ¬ æ‹Ÿåˆï¼‰ï¼šæ¨¡å‹åœ¨è®­ç»ƒæ•°æ®å’Œæ–°æ•°æ®ä¸Šéƒ½è¡¨ç°ä¸ä½³ã€‚è¿™é€šå¸¸æ˜¯å› ä¸ºæ¨¡å‹è¿‡äºç®€å•ï¼Œæ— æ³•æ•æ‰åˆ°æ•°æ®ä¸­çš„åŸºæœ¬å…³ç³»ã€‚

  è¿™äº›æœ¯è¯­åœ¨è®ºæ–‡ä¸­è¢«ç”¨æ¥æè¿°å’Œè§£é‡ŠStackingæ–¹æ³•çš„åŸç†ã€ä¼˜ç‚¹å’Œå±€é™æ€§ã€‚é€šè¿‡ç†è§£è¿™äº›æœ¯è¯­ï¼Œå¯ä»¥æ›´å¥½åœ°ç†è§£Stackingæ–¹æ³•çš„å·¥ä½œåŸç†å’Œåº”ç”¨åœºæ™¯ã€‚

#### ç§°å‘¼è¯´æ˜

- è¿™é‡Œ**å…ƒä¼°è®¡å™¨**(å…ƒå­¦ä¹ å™¨)æŒ‡çš„æ˜¯åˆå¹¶åˆçº§ä¼°è®¡å™¨çš„æ¬¡çº§ä¼°è®¡å™¨,è€Œä¸æ˜¯åˆçº§ä¼°è®¡å™¨

#### ç›¸å…³åŸç†

-  å †å æ³• (Stacking)æ˜¯ä¸€ç§åˆ›å»ºé›†æˆçš„æ–¹æ³•ï¼Œå®ƒä½¿ç”¨ä¸€ä¸ª**å…ƒä¼°è®¡å™¨**(æ¬¡çº§ä¼°è®¡å™¨)å»åˆå¹¶**åŸºä¼°è®¡å™¨**(åˆçº§ä¼°è®¡å™¨)çš„é¢„æµ‹ç»“æœã€‚
- **å †å æ³•**æœ‰æ—¶ä¹Ÿè¢«ç§°ä¸º**æ··åˆæ³•**ï¼Œä¼šå¢åŠ ç¬¬äºŒä¸ªç›‘ç£å­¦ä¹ é—®é¢˜ï¼š**å…ƒä¼°è®¡å™¨**å¿…é¡»è¢«è®­ç»ƒå»<u>ä½¿ç”¨**åŸºç¡€ä¼°è®¡å™¨**çš„é¢„æµ‹ç»“æœ</u>æ¥é¢„æµ‹å“åº”å˜é‡çš„å€¼ã€‚
- ä¸ºäº†è®­ç»ƒä¸€ä¸ª**å †å é›†åˆ**ï¼Œé¦–å…ˆéœ€è¦ä½¿ç”¨<u>è®­ç»ƒé›†å»è®­ç»ƒåŸºç¡€ä¼°è®¡å™¨</u>ã€‚
- å’Œå¥—è¢‹æ³•ä»¥åŠæ¨è¿›æ³•ä¸åŒï¼Œå †å æ³•å¯ä»¥<u>ä½¿ç”¨ä¸åŒç§ç±»çš„åŸºç¡€ä¼°è®¡å™¨</u>ã€‚
  - ä¾‹å¦‚ï¼Œä¸€ä¸ªéšæœºæ£®æ—å¯ä»¥å’Œä¸€ä¸ªé€»è¾‘å›å½’åˆ†ç±»åˆå¹¶ã€‚
  - æ¥ä¸‹æ¥ï¼ŒåŸºç¡€ä¼°è®¡å™¨çš„é¢„æµ‹ç»“æœå’ŒçœŸå®æƒ…å†µä¼šä½œä¸ºå…ƒä¼°è®¡å™¨çš„è®­ç»ƒé›†ã€‚
- å…ƒä¼°è®¡å™¨å¯ä»¥åœ¨ç›¸æ¯”æŠ•ç¥¨å’Œå¹³å‡æ›´å¤æ‚çš„æƒ…å†µä¸‹å­¦ä¹ åˆå¹¶åŸºç¡€ä¼°è®¡å™¨çš„é¢„æµ‹ç»“æœã€‚

#### è‡ªå®šä¹‰ç®€å•çš„StackingClassifier

- scikit-learn ç°åœ¨å·²ç»æä¾›äº†`Stacking`ç³»åˆ—æ–¹æ³•
- æˆ‘ä»¬ä¹Ÿå¯ä»¥æ‰©å±• `base.BaseEstimator` ç±»å»åˆ›å»ºè‡ªå·±çš„`StackingClassifer`ã€‚
- åœ¨ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œä½¿ç”¨ä¸€ä¸ªå•ä¸€å†³ç­–æ ‘ä½œä¸ºå…ƒä¼°è®¡å™¨ï¼ŒåŸºç¡€ä¼°è®¡å™¨åŒ…æ‹¬ä¸€ä¸ªé€»è¾‘å›å½’åˆ†ç±»å™¨å’Œä¸€ä¸ª KNN åˆ†ç±»å™¨ã€‚
- æˆ‘ä»¬ä½¿ç”¨ç±»çš„**é¢„æµ‹æ¦‚ç‡**ä½œä¸ºç‰¹å¾ï¼Œè€Œä¸æ˜¯ä½¿ç”¨ç±»çš„**é¢„æµ‹æ ‡ç­¾**ã€‚
- å¦å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨ `make_classification`å‡½æ•°åˆ›å»ºä¸€ä¸ªäººå·¥åˆ†ç±»æ•°æ®é›†ã€‚æ¥ç€ï¼Œè®­ç»ƒå¹¶è¯„ä¼°æ¯ä¸€ä¸ªåŸºç¡€ä¼°è®¡å™¨ã€‚æœ€åï¼Œè®­ç»ƒå¹¶è¯„ä¼°é›†åˆï¼Œå®ƒå…·æœ‰æ›´å¥½çš„å‡†ç¡®ç‡ã€‚

- ```python
  import numpy as np 
  from sklearn.model_selection import train_test_split 
  from sklearn.neighbors import KNeighborsClassifier 
  from sklearn.tree import DecisionTreeClassifier 
  from sklearn.linear_model import LogisticRegression 
  from sklearn.datasets import make_classification 
  from sklearn.base import clone, BaseEstimator, TransformerMixin,ClassifierMixin 
   
   
  class StackingCustomClassifier(BaseEstimator, ClassifierMixin, TransformerMixin): 
   
      def __init__(self, classifiers): 
          self.classifiers = classifiers 
          self.meta_classifier = DecisionTreeClassifier() 
   
      def fit(self, X, y): 
          for clf in self.classifiers: 
              clf.fit(X, y) 
              self.meta_classifier.fit(self._get_meta_features(X), y) 
          return self 
   
      def _get_meta_features(self, X): 
          probas = np.asarray([clf.predict_proba(X) for clf in 
            self.classifiers]) 
          return np.concatenate(probas, axis=1) 
   
      def predict(self, X): 
          return self.meta_classifier.predict(self._get_meta_features(X)) 
   
      def predict_proba(self, X): 
          return self.meta_classifier.predict_proba(self._get_meta_features(X)) 
      
      
  X, y = make_classification( 
      n_samples=1000, n_features=50, n_informative=30, 
      n_clusters_per_class=3, 
      random_state=11) 
  X_train, X_test, y_train, y_test = train_test_split(X, y, 
  random_state=11) 
  
  lr = LogisticRegression() 
  lr.fit(X_train, y_train) 
  print('Logistic regression accuracy: %s' % lr.score(X_test, 
  y_test)) 
  
  knn_clf = KNeighborsClassifier() 
  knn_clf.fit(X_train, y_train) 
  print('KNN accuracy: %s' % knn_clf.score(X_test, y_test)) 
  
  # Out[1]: 
  # Logistic regression accuracy: 0.816 
  # KNN accuracy: 0.836 
  
  # In[2]: 
  base_classifiers = [lr, knn_clf] 
  stacking_clf = StackingCustomClassifier(base_classifiers) 
  stacking_clf.fit(X_train, y_train) 
  print('Stacking classifier accuracy: %s' % stacking_clf.score(X_test, 
  y_test)) 
  ```

#### å°ç»“

- Stackingï¼ˆå †å ï¼‰æ˜¯ä¸€ç§é›†æˆå­¦ä¹ æ–¹æ³•ï¼Œå®ƒç»“åˆäº†å¤šä¸ªåŸºæœ¬æ¨¡å‹ï¼ˆä¹Ÿç§°ä¸ºåŸºå­¦ä¹ å™¨ï¼‰çš„é¢„æµ‹ç»“æœï¼Œä»¥ç”Ÿæˆä¸€ä¸ªæ–°çš„ã€æ›´å¼ºå¤§çš„æ¨¡å‹ï¼ˆä¹Ÿç§°ä¸ºå…ƒå­¦ä¹ å™¨æˆ–æ¬¡çº§æ¨¡å‹ï¼‰ã€‚Stackingå…¶ä¸»è¦ç›®çš„æ˜¯é€šè¿‡ç»“åˆå¤šä¸ªåŸºç¡€æ¨¡å‹çš„é¢„æµ‹ç»“æœæ¥æé«˜æ€»ä½“é¢„æµ‹æ€§èƒ½ã€‚ä¸Baggingå’ŒBoostingç­‰å…¶ä»–é›†æˆæ–¹æ³•ä¸åŒï¼ŒStackingä½¿ç”¨ä¸€ä¸ªå…ƒå­¦ä¹ å™¨ï¼ˆä¹Ÿç§°ä¸ºæ¬¡çº§æ¨¡å‹æˆ–æ¨¡å‹å †å å™¨ï¼‰å°†åŸºç¡€æ¨¡å‹çš„é¢„æµ‹ç»“æœä½œä¸ºè¾“å…¥ï¼Œè®­ç»ƒå‡ºä¸€ä¸ªæœ€ç»ˆé¢„æµ‹æ¨¡å‹ã€‚Stackingçš„æ ¸å¿ƒæ€æƒ³æ˜¯å€ŸåŠ©å¤šä¸ªæ¨¡å‹çš„ä¸åŒä¼˜åŠ¿ï¼Œæ•æ‰åˆ°æ•°æ®ä¸­çš„æ›´å¤šä¿¡æ¯å’Œç‰¹å¾ï¼Œä»è€Œå®ç°æ›´é«˜çš„é¢„æµ‹æ­£ç¡®ç‡ã€‚
- ä»¥ä¸‹æ˜¯Stackingæ–¹æ³•çš„è¯¦ç»†ä»‹ç»å’Œæ€»ç»“ï¼š
  1. **åŸºç¡€æ¨¡å‹**ï¼šåœ¨Stackingæ–¹æ³•ä¸­ï¼Œé¦–å…ˆéœ€è¦è®­ç»ƒä¸€ç»„åŸºç¡€æ¨¡å‹ï¼ˆä¹Ÿç§°ä¸ºç¬¬ä¸€å±‚æ¨¡å‹ï¼‰ã€‚è¿™äº›æ¨¡å‹å¯ä»¥æ˜¯ç›¸åŒçš„ç®—æ³•ï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸åŒçš„ç®—æ³•ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨å†³ç­–æ ‘ã€æ”¯æŒå‘é‡æœºã€ç¥ç»ç½‘ç»œç­‰å¤šç§æœºå™¨å­¦ä¹ ç®—æ³•ä½œä¸ºåŸºç¡€æ¨¡å‹ã€‚
  2. **è®­ç»ƒé›†å’ŒéªŒè¯é›†**ï¼šä¸ºäº†è®­ç»ƒå…ƒå­¦ä¹ å™¨ï¼Œéœ€è¦å°†åŸå§‹è®­ç»ƒæ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ã€‚è®­ç»ƒé›†ç”¨äºè®­ç»ƒåŸºç¡€æ¨¡å‹ï¼Œè€ŒéªŒè¯é›†ç”¨äºæ„å»ºå…ƒå­¦ä¹ å™¨çš„è¾“å…¥ç‰¹å¾ã€‚
  3. **åŸºç¡€æ¨¡å‹é¢„æµ‹**ï¼šä½¿ç”¨è®­ç»ƒé›†è®­ç»ƒåŸºç¡€æ¨¡å‹åï¼Œå°†éªŒè¯é›†è¾“å…¥åˆ°è¿™äº›æ¨¡å‹ä¸­å¾—åˆ°é¢„æµ‹ç»“æœã€‚è¿™äº›é¢„æµ‹ç»“æœå°†ä½œä¸ºå…ƒå­¦ä¹ å™¨çš„è¾“å…¥ç‰¹å¾ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä¸ºäº†é¿å…è¿‡æ‹Ÿåˆï¼Œå¯ä»¥ä½¿ç”¨KæŠ˜äº¤å‰éªŒè¯çš„æ–¹æ³•åœ¨ä¸åŒçš„æ•°æ®å­é›†ä¸Šè®­ç»ƒåŸºç¡€æ¨¡å‹ï¼Œå¹¶å°†ä¸åŒå­é›†ä¸Šçš„é¢„æµ‹ç»“æœæ‹¼æ¥èµ·æ¥ï¼Œä½œä¸ºå…ƒå­¦ä¹ å™¨çš„è¾“å…¥ã€‚
  4. **å…ƒå­¦ä¹ å™¨è®­ç»ƒ**ï¼šå°†åŸºç¡€æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„é¢„æµ‹ç»“æœä½œä¸ºè¾“å…¥ç‰¹å¾ï¼Œå°†éªŒè¯é›†çš„çœŸå®æ ‡ç­¾ä½œä¸ºè¾“å‡ºæ ‡ç­¾ï¼Œè®­ç»ƒå…ƒå­¦ä¹ å™¨ã€‚å…ƒå­¦ä¹ å™¨å¯ä»¥æ˜¯ä»»ä½•é€‚ç”¨çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå¦‚çº¿æ€§å›å½’ã€é€»è¾‘å›å½’æˆ–æ¢¯åº¦æå‡æ ‘ç­‰ã€‚
  5. **é¢„æµ‹**ï¼šåœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹æ—¶ï¼Œé¦–å…ˆä½¿ç”¨åŸºç¡€æ¨¡å‹å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹ï¼Œç„¶åå°†è¿™äº›é¢„æµ‹ç»“æœä½œä¸ºç‰¹å¾è¾“å…¥åˆ°å…ƒå­¦ä¹ å™¨ä¸­ï¼Œå¾—åˆ°æœ€ç»ˆçš„é¢„æµ‹ç»“æœã€‚

#### ä¸»è¦æ­¥éª¤

Stacking ç®—æ³•çš„ä¸»è¦æ­¥éª¤å¦‚ä¸‹ï¼š

1. **å‡†å¤‡æ•°æ®**ï¼šå°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚
2. **è®­ç»ƒåŸºå­¦ä¹ å™¨**ï¼šé€‰æ‹©å¤šä¸ª**åŸºå­¦ä¹ å™¨**ï¼ˆä¾‹å¦‚ï¼Œå†³ç­–æ ‘ã€æ”¯æŒå‘é‡æœºã€ç¥ç»ç½‘ç»œç­‰ï¼‰ï¼Œå¹¶åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒå®ƒä»¬ã€‚
3. **ç”Ÿæˆå…ƒç‰¹å¾**ï¼šä½¿ç”¨è®­ç»ƒå¥½çš„åŸºå­¦ä¹ å™¨å¯¹è®­ç»ƒé›†è¿›è¡Œé¢„æµ‹ã€‚å°†è¿™äº›é¢„æµ‹ç»“æœä½œä¸ºæ–°çš„ç‰¹å¾ï¼ˆç§°ä¸º**å…ƒç‰¹å¾**ï¼‰ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªæ–°çš„è®­ç»ƒé›†ã€‚é€šå¸¸ï¼Œæˆ‘ä»¬ä½¿ç”¨ K æŠ˜äº¤å‰éªŒè¯çš„æ–¹æ³•ç”Ÿæˆå…ƒç‰¹å¾ï¼Œä»¥é¿å…è¿‡æ‹Ÿåˆã€‚
4. **è®­ç»ƒå…ƒå­¦ä¹ å™¨**ï¼šä½¿ç”¨ç”Ÿæˆçš„å…ƒç‰¹å¾å’Œè®­ç»ƒé›†çš„ç›®æ ‡å˜é‡è®­ç»ƒå…ƒå­¦ä¹ å™¨ã€‚å…ƒå­¦ä¹ å™¨å¯ä»¥æ˜¯ä»»ä½•æ¨¡å‹ï¼Œä¾‹å¦‚çº¿æ€§å›å½’ã€é€»è¾‘å›å½’æˆ–æ¢¯åº¦æå‡æ ‘ç­‰ã€‚
5. **é¢„æµ‹**ï¼šé¦–å…ˆï¼Œä½¿ç”¨åŸºå­¦ä¹ å™¨å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹ï¼Œç„¶åå°†è¿™äº›é¢„æµ‹ç»“æœä½œä¸ºå…ƒç‰¹å¾è¾“å…¥åˆ°å…ƒå­¦ä¹ å™¨ä¸­ï¼Œæœ€åç”±å…ƒå­¦ä¹ å™¨ç”Ÿæˆæœ€ç»ˆé¢„æµ‹ç»“æœã€‚

#### Stacking çš„ä¼˜ç‚¹:

- é€šè¿‡ç»“åˆå¤šä¸ªåŸºå­¦ä¹ å™¨çš„é¢„æµ‹èƒ½åŠ›ï¼ŒStacking å¯ä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½ã€‚
- Stackingæ–¹æ³•çš„ä¼˜åŠ¿åœ¨äºå®ƒå¯ä»¥æœ‰æ•ˆåœ°æ•´åˆå¤šä¸ªæ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›ï¼Œä½¿å¾—é›†æˆåçš„æ¨¡å‹å…·æœ‰æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒStackingå¯ä»¥å¾ˆå¥½åœ°å¤„ç†åŸºç¡€æ¨¡å‹ä¹‹é—´çš„å¼‚è´¨æ€§ï¼Œä½¿å¾—ä¸åŒç±»å‹çš„æ¨¡å‹å¯ä»¥å…±åŒä¸ºæé«˜é¢„æµ‹æ€§èƒ½åšå‡ºè´¡çŒ®ã€‚
- Stacking å¯ä»¥åˆ©ç”¨ä¸åŒç±»å‹çš„åŸºå­¦ä¹ å™¨ï¼Œä»è€Œå……åˆ†åˆ©ç”¨å®ƒä»¬çš„å¤šæ ·æ€§ã€‚

#### Stacking çš„ç¼ºç‚¹ï¼š

- Stacking æ¶‰åŠå¤šä¸ªæ¨¡å‹çš„è®­ç»ƒå’Œé¢„æµ‹ï¼Œå› æ­¤è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚
- Stacking æ¨¡å‹çš„å¯è§£é‡Šæ€§è¾ƒå·®ï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ä¸ªå¤šå±‚æ¬¡çš„é›†æˆæ¨¡å‹ã€‚
- **å±€é™æ€§**ï¼šStackingæ–¹æ³•çš„ä¸»è¦å±€é™æ€§åœ¨äºå…¶è®¡ç®—å¤æ‚åº¦è¾ƒé«˜ï¼Œå› ä¸ºéœ€è¦è®­ç»ƒå¤šä¸ªåŸºç¡€æ¨¡å‹å’Œä¸€ä¸ªå…ƒå­¦ä¹ å™¨ã€‚æ­¤å¤–ï¼Œå¦‚æœåŸºç¡€æ¨¡å‹çš„é¢„æµ‹ç»“æœç›¸å…³æ€§è¾ƒé«˜ï¼Œé‚£ä¹ˆStackingæ–¹æ³•çš„æ€§èƒ½å¯èƒ½å—åˆ°é™åˆ¶ï¼Œå› ä¸ºå…ƒå­¦ä¹ å™¨éš¾ä»¥ä»é«˜åº¦ç›¸å…³çš„åŸºç¡€æ¨¡å‹é¢„æµ‹ä¸­è·å–é¢å¤–çš„ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¯ä»¥å°è¯•ä½¿ç”¨å¤šæ ·æ€§è¾ƒé«˜çš„åŸºç¡€æ¨¡å‹ä»¥æé«˜Stackingçš„æ€§èƒ½ã€‚

åœ¨å®è·µä¸­ï¼Œä¸ºäº†è·å¾—æ›´å¥½çš„æ€§èƒ½ï¼Œå¯ä»¥å°è¯•ä½¿ç”¨ä¸åŒç±»å‹å’Œå‚æ•°è®¾ç½®çš„åŸºå­¦ä¹ å™¨ï¼Œä»¥åŠè°ƒæ•´å…ƒå­¦ä¹ å™¨çš„ç±»å‹å’Œå‚æ•°ã€‚æ­¤å¤–ï¼Œå¯ä»¥é€šè¿‡ç‰¹å¾å·¥ç¨‹ã€è°ƒæ•´è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŠ½æ ·ç­–ç•¥ç­‰æ–¹æ³•è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹ã€‚

- æ›´å¤šå†…å®¹å‚è€ƒ[å­¦ä¹ ç­–ç•¥:å­¦ä¹ æ³•|stacking](###å­¦ä¹ æ³•)



##  ç»“åˆç­–ç•¥

- å­¦ä¹ å™¨ç»“åˆå¯èƒ½ä¼šä»ä¸‰ä¸ªæ–¹é¢å¸¦æ¥å¥½å¤„[Dietterichï¼Œ2000]
  - ä»**ç»Ÿè®¡**çš„æ–¹é¢æ¥çœ‹,ç”±äºå­¦ä¹ ä»»åŠ¡çš„**å‡è®¾ç©ºé—´**å¾€å¾€å¾ˆå¤§,å¯èƒ½æœ‰å¤šä¸ªå‡è®¾åœ¨è®­ç»ƒé›†ä¸Šè¾¾åˆ°**åŒç­‰æ€§èƒ½**,æ­¤æ—¶è‹¥ä½¿ç”¨å•å­¦ä¹ å™¨å¯èƒ½å› è¯¯é€‰è€Œå¯¼è‡´æ³›åŒ–æ€§èƒ½ä¸ä½³,ç»“åˆå¤šä¸ªå­¦ä¹ å™¨åˆ™ä¼šå‡å°è¿™ä¸€é£é™©;
  - ä»**è®¡ç®—**çš„æ–¹é¢æ¥çœ‹,å­¦ä¹ ç®—æ³•å¾€å¾€ä¼šé™·å…¥**å±€éƒ¨æå°**,æœ‰çš„å±€éƒ¨æå°ç‚¹æ‰€å¯¹åº”çš„æ³›åŒ–æ€§èƒ½å¯èƒ½å¾ˆç³Ÿç³•,è€Œé€šè¿‡å¤šæ¬¡è¿è¡Œä¹‹åè¿›è¡Œç»“åˆ,å¯é™ä½é™·å…¥ç³Ÿç³•å±€éƒ¨æå°ç‚¹çš„é£é™©;
  - ä»**è¡¨ç¤º**çš„æ–¹é¢æ¥çœ‹,æŸäº›å­¦ä¹ ä»»åŠ¡çš„çœŸå®å‡è®¾å¯èƒ½ä¸åœ¨å½“å‰å­¦ä¹ ç®—æ³•æ‰€è€ƒè™‘çš„å‡è®¾ç©ºé—´ä¸­,æ­¤æ—¶è‹¥ä½¿ç”¨å•å­¦ä¹ å™¨åˆ™è‚¯å®šæ— æ•ˆ,è€Œé€šè¿‡ç»“åˆå¤šä¸ªå­¦ä¹ å™¨,ç”±äºç›¸åº”çš„<u>å‡è®¾ç©ºé—´æœ‰æ‰€æ‰©å¤§</u>,æœ‰å¯èƒ½å­¦å¾—æ›´å¥½çš„è¿‘ä¼¼.
- å‡å®šæŸé›†æˆåŒ…å«Tä¸ªå­¦ä¹ å™¨${h_1,h_2,\cdots,h_T}$,å…¶ä¸­$h_i$åœ¨ç¤ºä¾‹$\boldsymbol{x}$ä¸Šçš„è¾“å‡ºä¸º$h_i(\boldsymbol{x})$

### å¹³å‡æ³•

- å¯¹æ•°å€¼å‹è¾“å‡º$h_i(\boldsymbol{x})\in{\mathbb{R}}$,æœ€å¸¸è§çš„ç»“åˆç­–ç•¥æ˜¯ä½¿ç”¨å¹³å‡æ³•(averaging)

#### ç®€å•å¹³å‡æ³•

- $$
  H(\boldsymbol{x})=\frac{1}{T}\sum_{i=1}^{T}h_i(\boldsymbol{x})
  $$

  

#### åŠ æƒå¹³å‡æ³•

- åŠ æƒå¹³å‡æ³•(weighted averaging,WA)

  - $$
    H(\boldsymbol{x})=\sum_{i=1}^{T}w_ih_i(\boldsymbol{x})
    $$

  - $w_i$æ˜¯ä¸ªä½“å­¦ä¹ å™¨$h_i$çš„æƒé‡,é€šå¸¸è¦æ±‚$w_i\geqslant{0}$,$\sum_{i=1}^{T}w_i=1$

    - ç›¸å…³ç ”ç©¶è¡¨æ˜,å¿…é¡»ä½¿ç”¨éè´Ÿæƒé‡æ‰èƒ½ç¡®ä¿é›†æˆæ€§èƒ½ä¸€å®šç”±äºå•ä¸€æœ€ä½³ä¸ªä½“å­¦ä¹ å™¨
    - å› æ­¤é›†æˆå­¦ä¹ ä¸­,ä¸€èˆ¬å¯¹å­¦ä¹ å™¨çš„æƒé‡æ–½åŠ éè´Ÿçº¦æŸ

  - ç®€å•å¹³å‡æ³•æ˜¯åŠ æƒå¹³å‡æ³•çš„ä¸€ä¸ªç‰¹ä¾‹($w_i=\frac{1}{T}$)

  - é›†æˆå­¦ä¹ ä¸­çš„å„ç§**ç»“åˆæ–¹æ³•**éƒ½å¯ä»¥è§†ä¸ºå…¶ç‰¹ä¾‹æˆ–å˜ä½“

  - å¯¹äºç»™å®šçš„åŸºå­¦ä¹ å™¨,ä¸åŒçš„é›†æˆå­¦ä¹ æ–¹æ³•å¯ä»¥è§†ä¸ºé€šè¿‡<u>ä¸åŒçš„æ–¹å¼</u>æ¥<u>ç¡®å®š</u>**åŠ æƒå¹³å‡æ³•**ä¸­çš„åŸºå­¦ä¹ å™¨<u>æƒé‡</u>

    - ä¾‹å¦‚ä¼°è®¡å‡ºä¸ªä½“å­¦ä¹ å™¨çš„è¯¯å·®,ç„¶åä»¤æƒé‡å¤§å°å’Œè¯¯å·®å¤§å°æˆåæ¯”

  - åŠ æƒå¹³å‡æ³•çš„æƒé‡ä¸€èˆ¬æ˜¯ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ å¾—åˆ°,ç”±äºç°å®ä»»åŠ¡ä¸­çš„æ ·æœ¬è®­ç»ƒé€šå¸¸ä¸å……åˆ†(æˆ–å­˜åœ¨å™ªå£°),ä½¿å¾—å­¦ä¹ å¾—åˆ°çš„æƒé‡ä¸å®Œå…¨å¯é .å¯¹äºè§„æ¨¡è¾ƒå¤§çš„é›†æˆæ¥è¯´,å­¦ä¹ çš„æƒé‡æ¯”è¾ƒå¤š,å®¹æ˜“å¯¼è‡´è¿‡æ‹Ÿåˆ.

  - åŸºäºä¸Šè¿°æƒ…å†µ,å®éªŒå’Œåº”ç”¨ä¸­,åŠ æƒå¹³å‡æ³•æœªå¿…ç”±äºç®€å•å¹³å‡æ³•

- ä¸€èˆ¬åœ°,ä¸ªä½“å­¦ä¹ å™¨æ€§èƒ½ç›¸å·®è¾ƒå¤§æ—¶,é€‚åˆé‡‡ç”¨åŠ æƒå¹³å‡æ³•,ä¸ªä½“å­¦ä¹ å™¨æ€§èƒ½æ¥è¿‘æ—¶,é€‚åˆé‡‡ç”¨ç®€å•å¹³å‡æ³•

### æŠ•ç¥¨æ³•

- å¯¹äº**åˆ†ç±»ä»»åŠ¡**æ¥è¯´,å­¦ä¹ å™¨$h_i$å°†ä»åˆ†ç±»åˆ«æ ‡è®°é›†åˆ$\{c_1,c_2,\cdots,c_N\}$ä¸­é¢„æµ‹å‡ºä¸€ä¸ªæ ‡è®°
- æœ€å¸¸è§çš„å­¦ä¹ å™¨ç»“åˆç­–ç•¥æ—¶ä½¿ç”¨æŠ•ç¥¨æ³•(voting)
- ä¸å¦¨å°†$h_i$åœ¨æ ·æœ¬$\boldsymbol{x}$ä¸Šçš„é¢„æµ‹è¾“å‡ºè¡¨ç¤ºä¸ºä¸€ä¸ªNç»´å‘é‡$(h_i^{1}(\boldsymbol{x});h_i^2(\boldsymbol{x});\cdots;h_i^{N}(\boldsymbol{x}))$
  - å…¶ä¸­$h_{i}^{j}(\boldsymbol{x})$è¡¨ç¤º$h_i$åœ¨ç±»åˆ«$c_j$ä¸Šçš„è¾“å‡º

#### ç»å¯¹å¤šæ•°æŠ•ç¥¨æ³•MV

- ç»å¯¹å¤šæ•°æŠ•ç¥¨æ³•(majority voting,æœ‰æ—¶ç®€å†™ä¸ºvoting)

  - $$
    H(\boldsymbol{x})=
    \begin{cases}
    c_j,&{\text{if }\sum_{i=1}^{T}h_{i}^{j}(\boldsymbol{x})
    >0.5\sum_{k=1}^{N}\sum_{i=1}^{T}h_{i}^{k}(\boldsymbol{x})}\\
    \text{reject},&\text{otherwise}
    \end{cases}
    $$

    - æ€»ç»“ä¸º:è‹¥æŸæ ‡è®°$c_k$å¾—ç¥¨è¶…è¿‡åŠæ•°,åˆ™é¢„æµ‹ä¸º$c_k$;å¦åˆ™æ‹’ç»é¢„æµ‹

#### ç›¸å¯¹å¤šæ•°æŠ•ç¥¨æ³•PV

- ç›¸å¯¹å¤šæ•°æŠ•ç¥¨æ³•(plurality voting)

- $$
  H(\boldsymbol{x})=\huge{c}_{
  \normalsize{\underset{j}{\arg\max}}
  {\sum_{i=1}^{T}h_i^{j}(\boldsymbol{x})}
  }
  $$

  - Hé¢„æµ‹ä¸ºå¾—ç¥¨æœ€å¤šçš„æ ‡è®°
  - å¦‚æœåŒæ—¶åˆå¤šä¸ªæ ‡è®°è·å¾—æœ€é«˜ç¥¨,åˆ™ä»ä¸­éšæœºé€‰å–ä¸€ä¸ª

#### åŠ æƒæŠ•ç¥¨æ³•WV

- weighted voting

- $$
  H(\boldsymbol{x})=\huge{c}_{
  \normalsize{\underset{j}{\arg\max}}
  {\sum_{i=1}^{T}
  w_ih_i^{j}(\boldsymbol{x})}
  }
  $$

  - ä¸åŠ æƒå¹³å‡æ³•ç±»ä¼¼,$w_i$æ˜¯$h_i$çš„æƒé‡
  - é€šå¸¸$w_i\geqslant{0}$,$\sum_{i=1}^{T}w_i=1$

#### å°ç»“

- æ ‡å‡†çš„ç»å¯¹å¤šæ•°æŠ•ç¥¨æ³•æä¾›äº†**æ‹’ç»é¢„æµ‹**çš„é€‰é¡¹(å¯èƒ½æƒ…å†µ),åœ¨è¦æ±‚é«˜å¯é æ€§çš„å­¦ä¹ ä»»åŠ¡ä¸­æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„æœºåˆ¶
- å¦‚æœå­¦ä¹ ä»»åŠ¡è¦æ±‚<u>å¿…é¡»æä¾›é¢„æµ‹ç»“æœ</u>,åˆ™ç»å¯¹æŠ•ç¥¨æ³•é€€åŒ–ä¸ºç›¸å¯¹å¤šæ•°æŠ•ç¥¨æ³•
  - å› æ­¤,è¿™ç±»ä»»åŠ¡ä¸­,ç»å¯¹å¤šæ•°,ç›¸å¯¹å¤šæ•°æŠ•ç¥¨æ³•ç»Ÿç§°ä¸º**å¤šæ•°æŠ•ç¥¨æ³•**

#### å…¶ä»–æŠ•ç¥¨æ³•æŠ€å·§

- åœ¨ç°å®ä»»åŠ¡ä¸­,ä¸åŒç±»å‹ä¸ªä½“å­¦ä¹ å™¨å¯èƒ½å·®ç”Ÿä¸åŒç±»å‹çš„$h_i^{j}(\boldsymbol{x})$å€¼:
  - ç±»æ ‡è®°:$h_i^j(\boldsymbol{x})\in\{0,1\}$
    - è‹¥$h_i$å°†ç±»æ ·æœ¬é¢„æµ‹ä¸º$c_j$ç±»åˆ«,åˆ™$h_i^j(\boldsymbol{x})$å–1,å¦åˆ™ä¸º0
    - è¿™ç±»æŠ•ç¥¨ç§°ä¸º**ç¡¬æŠ•ç¥¨**(hard voting)
  - ç±»æ¦‚ç‡:$h_i^{j}(\boldsymbol{x})\in[0,1]$
    - ç›¸å½“äºå¯¹åéªŒæ¦‚ç‡$P(c_j|\boldsymbol{x})$çš„ä¸€ä¸ªä¼°è®¡
    - è¿™ç±»æŠ•ç¥¨æ–¹æ³•ç§°ä¸º**è½¯æŠ•ç¥¨**(soft voting)
- ä¸åŒç±»å‹çš„$h_i^j(\boldsymbol{x})$å€¼ä¸èƒ½æ··ç”¨,å¯¹ä¸€äº›èƒ½åœ¨é¢„æµ‹å‡ºç±»åˆ«æ ‡è®°çš„åŒæ—¶äº§ç”Ÿ**åˆ†ç±»ç½®ä¿¡åº¦**çš„å­¦ä¹ å™¨,å…¶<u>åˆ†ç±»ç½®ä¿¡åº¦å¯è½¬åŒ–ä¸ºç±»æ¦‚ç‡ä½¿ç”¨ï¼</u>
  - è‹¥æ­¤ç±»å€¼æœªè¿›è¡Œè§„èŒƒåŒ–,ä¾‹å¦‚æ”¯æŒå‘é‡æœºçš„åˆ†ç±»é—´éš”å€¼,åˆ™å¿…é¡»ä½¿ç”¨ä¸€äº›æŠ€æœ¯å¦‚
    - Platt ç¼©æ”¾(Platt scaling)[Plattï¼Œ2000]
    - ç­‰åˆ†å›å½’(isotonic regression)[Zadrozny andElkanï¼Œ2001]
    - ç­‰æ–¹æ³•è¿›è¡Œâ€œæ ¡å‡†â€(calibration)åæ‰èƒ½ä½œä¸º**ç±»æ¦‚ç‡**ä½¿ç”¨ï¼
  - è™½ç„¶åˆ†ç±»å™¨ä¼°è®¡å‡ºçš„ç±»æ¦‚ç‡å€¼ä¸€èˆ¬éƒ½ä¸å¤ªå‡†ç¡®,ä½†åŸºäºç±»æ¦‚ç‡è¿›è¡Œç»“åˆå´å¾€å¾€æ¯”ç›´æ¥åŸºäºç±»æ ‡è®°è¿›è¡Œç»“åˆæ€§èƒ½æ›´å¥½
  - è‹¥<u>åŸºå­¦ä¹ å™¨çš„ç±»å‹ä¸åŒ</u>,åˆ™å…¶ç±»æ¦‚ç‡å€¼**ä¸èƒ½ç›´æ¥è¿›è¡Œæ¯”è¾ƒ**;
    - åœ¨æ­¤ç§æƒ…å½¢ä¸‹,é€šå¸¸å¯å°†ç±»æ¦‚ç‡è¾“å‡º<u>è½¬åŒ–ä¸ºç±»æ ‡è®°</u>è¾“å‡º
    - ä¾‹å¦‚å°†ç±»æ¦‚ç‡è¾“å‡ºæœ€å¤§çš„$h_i^j(\boldsymbol{x})$è®¾ä¸º1,å…¶ä»–è®¾ä¸º0,ç„¶åå†æŠ•ç¥¨.

### å­¦ä¹ æ³•

- å½“è®­ç»ƒæ•°æ®å¾ˆå¤šæ—¶ï¼Œä¸€ç§æ›´ä¸ºå¼ºå¤§çš„ç»“åˆç­–ç•¥æ˜¯ä½¿ç”¨â€œå­¦ä¹ æ³•â€,å³é€šè¿‡å¦ä¸€ä¸ªå­¦ä¹ å™¨æ¥è¿›è¡Œç»“åˆ.
- stackingæ–¹æ³•æ˜¯ä¸€ç§å…¸å‹çš„æ–¹æ³•,ä¸èƒ½è¯´stakcingå®Œå…¨ç­‰äºåŒäºå­¦ä¹ æ³•.

#### Stacking

- Stacking [Wolpert,1992; Breiman,1996b]æ˜¯å­¦ä¹ æ³•çš„**å…¸å‹ä»£è¡¨**.(stackingæœ¬èº«ä¹Ÿæ˜¯ä¸€ç§é›†æˆå­¦ä¹ æ–¹æ³•)
- è¿™é‡Œæˆ‘ä»¬æŠŠ**ä¸ªä½“å­¦ä¹ å™¨**(åŸºç¡€å­¦ä¹ å™¨)ç§°ä¸º**åˆçº§å­¦ä¹ å™¨**.
- ç”¨äºç»“åˆçš„å­¦ä¹ å™¨ç§°ä¸º**æ¬¡çº§å­¦ä¹ å™¨**æˆ–**å…ƒå­¦ä¹ å™¨**(meta-learner).
- Stacking å…ˆä»**åˆå§‹æ•°æ®é›†**è®­ç»ƒå‡º**åˆçº§å­¦ä¹ å™¨**,ç„¶åâ€œç”Ÿæˆâ€ä¸€ä¸ª**æ–°æ•°æ®é›†**
  - åœ¨è¿™ä¸ªæ–°æ•°æ®é›†ä¸­,<u>åˆçº§å­¦ä¹ å™¨</u>çš„**è¾“å‡º**è¢«å½“ä½œ<u>æ ·ä¾‹è¾“å…¥ç‰¹å¾</u>,è€Œ**åˆå§‹æ ·æœ¬**çš„æ ‡è®°ä»è¢«å½“ä½œæ ·ä¾‹æ ‡è®°.
  - æ–°æ•°æ®é›†ç”¨äºè®­ç»ƒ**æ¬¡çº§å­¦ä¹ å™¨**.

#### ä¼ªä»£ç 

- å‡å®šåˆçº§å­¦ä¹ å™¨ä½¿ç”¨ä¸åŒå­¦ä¹ ç®—æ³•äº§ç”Ÿ,å³åˆçº§é›†æˆæ˜¯å¼‚è´¨çš„.

- input:

  - è®­ç»ƒé›†$D=\{(\boldsymbol{x}_1,y_1),(\boldsymbol{x}_2,y_2),\cdots,(\boldsymbol{x}_m,y_m)\}$
  - åˆçº§å­¦ä¹ ç®—æ³•$\mathfrak{L}_1,\mathfrak{L}_2,\cdots,\mathfrak{L}_T;$
  - æ¬¡çº§å­¦ä¹ ç®—æ³•$\mathfrak{L}$

- $$
  \begin{array}{l}
  &01:\textbf{for }t=1,2,\ldots,T\textbf{do} \\
  &02:\quad h_{t}={\mathfrak{L}}_{t}(D); \\
  &03:\textbf{end for} \\
  &04:D'=\varnothing; \\
  &05:\textbf{for }i=1,2,\ldots,m\textbf{ do} \\
  &06: \quad \textbf{for }t=1,2,\ldots,T \textbf{ do} \\
  &07: \quad\quad z_{it}=h_t(\boldsymbol{x}_i); \\
  &08:\quad\textbf{end for} \\
  &09:\quad D'=D'\cup((z_{i1},z_{i2},\ldots,z_{iT}),y_i); \\
  &10:\textbf{end for} \\
  &11:h^{\prime}={\mathfrak{L}}(D^{\prime}); \\
  \end{array}
  $$

- $$
  H(\boldsymbol{x})=h'(h_1(\boldsymbol{x}),h_2(\boldsymbol{x}),\dots,h_T(\boldsymbol{x}))
  $$

- comments:
  - 1-3:ä½¿ç”¨åˆçº§å­¦ä¹ ç®—æ³•$\mathfrak{L}_t$äº§ç”Ÿåˆçº§å­¦ä¹ å™¨$h_t$
  - 4-10:ç”Ÿæˆ**æ¬¡çº§è®­ç»ƒé›†**
  - 11:åœ¨$\mathcal{D'}$ä¸Šä½¿ç”¨æ¬¡çº§å­¦ä¹ ç®—æ³•$\mathfrak{L}$äº§ç”Ÿ**æ¬¡çº§å­¦ä¹ å™¨**$h'$
  
- åœ¨è®­ç»ƒé˜¶æ®µ,æ¬¡çº§è®­ç»ƒé›†æ˜¯åˆ©ç”¨åˆçº§å­¦ä¹ å™¨äº§ç”Ÿçš„,è‹¥ç›´æ¥ç”¨åˆçº§å­¦ä¹ å™¨çš„è®­ç»ƒé›†æ¥äº§ç”Ÿæ¬¡çº§è®­ç»ƒé›†,åˆ™è¿‡æ‹Ÿåˆé£é™©ä¼šæ¯”è¾ƒå¤§;

- å› æ­¤,ä¸€èˆ¬æ˜¯é€šè¿‡ä½¿ç”¨äº¤å‰éªŒè¯æˆ–ç•™ä¸€æ³•è¿™æ ·çš„æ–¹å¼,<u>ç”¨è®­ç»ƒåˆçº§å­¦ä¹ å™¨æœªä½¿ç”¨çš„æ ·æœ¬æ¥äº§ç”Ÿæ¬¡çº§å­¦ä¹ å™¨çš„è®­ç»ƒæ ·æœ¬ï¼</u>

#### æ¬¡çº§è®­ç»ƒé›†çš„ç”ŸæˆğŸˆ

- ä»¥kæŠ˜äº¤å‰éªŒè¯ä¸ºä¾‹
  - åˆå§‹è®­ç»ƒé›†$D$è¢«éšæœºåˆ’åˆ†ä¸ºkä¸ªå¤§å°ç›¸ä¼¼çš„é›†åˆ$D_1,D_2,\cdots,D_k$

  - ä»¤$D_j$å’Œ$\overline{D_j}=D\backslash{D_{j}}$åˆ†åˆ«è¡¨ç¤ºç¬¬$j$æŠ˜çš„æµ‹è¯•é›†å’Œè®­ç»ƒé›†.

  - ç»™å®š$T$ä¸ªåˆçº§å­¦ä¹ ç®—æ³•,åˆçº§å­¦ä¹ å™¨$h_{t}^{(j)}$é€šè¿‡åœ¨$\overline{D_{j}}$ä¸Šä½¿ç”¨ç¬¬$t$ä¸ªå­¦ä¹ ç®—æ³•è€Œå¾—.

  - å¯¹$D_j$(æµ‹è¯•é›†)ä¸­æ¯ä¸ªæ ·æœ¬$\boldsymbol{x}_i$,ä»¤ $z_{it}=h_t^{(j)}(\boldsymbol{x}_i)$ï¼Œ($i$è¡¨ç¤º$D_j$çš„ç¬¬$i$ä¸ªæ ·æœ¬,è€Œtè¡¨ç¤ºç¬¬tä¸ªå­¦ä¹ ç®—æ³•,è®¾$D_j$ä¸­å«æœ‰$p\approx{m/k}$ä¸ªæ ·æœ¬,ç”±äºäº¤å‰éªŒè¯å®Œæˆåæ‰€æœ‰æ ·æœ¬éƒ½ç­‰å®Œæˆæ˜ å°„ï¼Œ$p$å€¼ä»…åšå‚è€ƒ)

  - åˆ™ç”±$\boldsymbol{x}_i$æ‰€äº§ç”Ÿçš„**æ¬¡çº§è®­ç»ƒæ ·ä¾‹**çš„<u>ç¤ºä¾‹éƒ¨åˆ†</u>ä¸º$\boldsymbol{z}_i=(z_{i1};z_{i2};\cdots;z_{iT})$ï¼Œ<u>æ ‡è®°éƒ¨åˆ†</u>ä¸º$y_i$(æ³¨æ„åˆ°,æ­¤æ—¶ç¤ºä¾‹çš„ç»´æ•°æ­¤æ—¶æ˜¯$T$,å’Œåˆçº§å­¦ä¹ å™¨çš„ä¸ªæ•°ä¸€è‡´)ï¼Œç¤ºä¾‹ç»´æ•°å˜æ¢å…³ç³»ï¼š$\boldsymbol{x}_i\in{\mathbb{R}^{U}}
    \to{\boldsymbol{z}_i}\in{\mathbb{R}^{T}}$ï¼Œå…¶ä¸­$U$è¡¨ç¤ºåˆçº§è®­ç»ƒé›†ç¤ºä¾‹çš„ç»´æ•°ã€‚

    - $$
      \boldsymbol{x}_i\in{\mathbb{R}^{U}}
      \to{\boldsymbol{z}_i}\in{\mathbb{R}^{T}}
      $$

      

  - åœ¨æ•´ä¸ªäº¤å‰éªŒè¯è¿‡ç¨‹ç»“æŸå,ä»è¿™Tä¸ªåˆçº§å­¦ä¹ å™¨äº§ç”Ÿçš„**æ¬¡çº§è®­ç»ƒé›†**æ˜¯$D'=\{(\boldsymbol{z}_i,y_i)\}_{i=1}^{m}$,ç„¶å$D'$å°†ç”¨äºè®­ç»ƒæ¬¡çº§å­¦ä¹ å™¨.

  - ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/be97e3f69f544f7d935f61ef03c18cdc.png)

- æ¬¡çº§å­¦ä¹ å™¨çš„<u>è¾“å…¥å±æ€§è¡¨ç¤º</u>å’Œæ¬¡çº§å­¦ä¹ ç®—æ³•å¯¹Stackingé›†æˆçš„æ³›åŒ–æ€§èƒ½æœ‰å¾ˆå¤§å½±å“.

  - æœ‰ç ”ç©¶è¡¨æ˜,å°†åˆçº§å­¦ä¹ å™¨çš„è¾“å‡ºç±»æ¦‚ç‡ä½œä¸ºæ¬¡çº§å­¦ä¹ å™¨çš„è¾“å…¥å±æ€§,ç”¨**å¤šå“åº”çº¿æ€§å›å½’**(Multi-response Linear Regressionï¼Œç®€ç§°MLR)ä½œä¸º**æ¬¡çº§å­¦ä¹ ç®—æ³•**æ•ˆæœè¾ƒå¥½[Ting and Wittenï¼Œ1999]
  - MLRæ˜¯åŸºäº**çº¿æ€§å›å½’**çš„åˆ†ç±»å™¨ï¼Œå®ƒå¯¹**æ¯ä¸ªç±»**åˆ†åˆ«è¿›è¡Œ**çº¿æ€§å›å½’**ï¼Œå±äºè¯¥ç±»çš„è®­ç»ƒæ ·ä¾‹æ‰€å¯¹åº”çš„è¾“å‡ºè¢«ç½®ä¸º1ï¼Œå…¶ä»–ç±»ç½®ä¸º0;æµ‹è¯•ç¤ºä¾‹å°†è¢«åˆ†ç»™è¾“å‡ºå€¼æœ€å¤§çš„ç±».
    WEKAä¸­çš„StackingCç®—æ³•å°±æ˜¯è¿™æ ·å®ç°çš„.
  - åœ¨ MLRä¸­ä½¿ç”¨**ä¸åŒçš„å±æ€§é›†**æ›´ä½³[Seewald, 2002].

## æ›´å¤šé›†æˆå­¦ä¹ ç›¸å…³å‚è€ƒ

- `<<äººå·¥æ™ºèƒ½ï¼šç°ä»£æ–¹æ³•(æ–¯å›¾å°”ç‰¹Â·ç½—ç´ ) >>`
- `<<pythonæœºå™¨å­¦ä¹ åŸºç¡€æ•™ç¨‹>>`
  - åœ¨å†³ç­–æ ‘é›†æˆä¸€èŠ‚ä¸­ä»‹ç»äº†randomForestå’ŒGradientBoosting

### GradientBoosting

- æ¢¯åº¦æå‡å›å½’æ ‘(GradientBoostingRegressionTree,GBRT),å¯¹äºå› å­åŒ–è¡¨æ ¼æ•°æ®çš„å›å½’å’Œåˆ†ç±»é—®é¢˜ï¼Œæ¢¯åº¦æå‡(gradient boosting)ï¼Œæœ‰æ—¶ç§°ä¸ºæ¢¯åº¦æå‡æœºï¼ˆGBMï¼‰ï¼Œæˆ–æ¢¯åº¦æå‡å›å½’æ ‘ï¼ˆGBRTï¼‰å·²æˆä¸ºä¸€ç§éå¸¸çƒ­é—¨çš„æ–¹æ³•ã€‚
- æ¢¯åº¦æå‡å›å½’æ ‘æ˜¯ä¸€ç§é›†æˆæ–¹æ³•ï¼Œé€šè¿‡åˆå¹¶å¤šä¸ªå†³ç­–æ ‘æ¥æ„å»ºä¸€ä¸ªæ›´ä¸ºå¼ºå¤§çš„æ¨¡å‹ã€‚è™½ç„¶åå­—ä¸­å«æœ‰â€œå›å½’â€ï¼Œä½†è¿™ä¸ªæ¨¡å‹æ—¢å¯ä»¥ç”¨äºå›å½’ä¹Ÿå¯ä»¥ç”¨äºåˆ†ç±»ã€‚é¡¾åæ€ä¹‰ï¼Œæ¢¯åº¦æå‡æ³•æ˜¯ä¸€ç§ä½¿ç”¨äº†**æ¢¯åº¦ä¸‹é™**çš„è‡ªé€‚åº”æå‡æ³•ã€‚
- ä¸éšæœºæ£®æ—æ–¹æ³•ä¸åŒï¼Œæ¢¯åº¦æå‡é‡‡ç”¨è¿ç»­çš„æ–¹å¼æ„é€ æ ‘ï¼Œæ¯æ£µæ ‘éƒ½è¯•å›¾çº æ­£å‰ä¸€æ£µæ ‘çš„é”™è¯¯ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œæ¢¯åº¦æå‡å›å½’æ ‘ä¸­æ²¡æœ‰éšæœºåŒ–ï¼Œè€Œæ˜¯ç”¨åˆ°äº†**å¼ºé¢„å‰ªæ**ã€‚æ¢¯åº¦æå‡æ ‘é€šå¸¸ä½¿ç”¨**æ·±åº¦å¾ˆå°**çš„æ ‘ï¼ˆ1 åˆ° 5 å±‚ä¹‹é—´ï¼‰ï¼Œè¿™æ ·æ¨¡å‹å ç”¨çš„å†…å­˜æ›´å°‘ï¼Œé¢„æµ‹é€Ÿåº¦ä¹Ÿæ›´å¿«ã€‚
- æ¢¯åº¦æå‡èƒŒåçš„**ä¸»è¦æ€æƒ³**æ˜¯åˆå¹¶è®¸å¤šç®€å•çš„æ¨¡å‹ï¼ˆå¼±å­¦ä¹ å™¨ ï¼‰ï¼Œæ¯”å¦‚æ·±åº¦è¾ƒå°çš„æ ‘ã€‚æ¯æ£µæ ‘åªèƒ½å¯¹éƒ¨åˆ†æ•°æ®åšå‡ºå¥½çš„é¢„æµ‹ï¼Œå› æ­¤ï¼Œæ·»åŠ çš„æ ‘è¶Šæ¥è¶Šå¤šï¼Œå¯ä»¥ä¸æ–­è¿­ä»£æé«˜æ€§èƒ½ã€‚
- æ¢¯åº¦æå‡æ ‘ç»å¸¸æ˜¯æœºå™¨å­¦ä¹ ç«èµ›çš„ä¼˜èƒœè€…ï¼Œå¹¶ä¸”å¹¿æ³›åº”ç”¨äºä¸šç•Œã€‚ä¸éšæœºæ£®æ—ç›¸æ¯”ï¼Œå®ƒé€šå¸¸å¯¹å‚æ•°è®¾ç½®æ›´ä¸ºæ•æ„Ÿï¼Œä½†å¦‚æœå‚æ•°è®¾ç½®æ­£ç¡®çš„è¯ï¼Œæ¨¡å‹ç²¾åº¦æ›´é«˜ã€‚
- é™¤äº†é¢„å‰ªæä¸é›†æˆä¸­æ ‘çš„æ•°é‡ä¹‹å¤–ï¼Œæ¢¯åº¦æå‡çš„å¦ä¸€ä¸ªé‡è¦å‚æ•°æ˜¯ learning_rate ï¼ˆå­¦ä¹ ç‡ï¼‰ï¼Œç”¨äºæ§åˆ¶æ¯æ£µæ ‘çº æ­£å‰ä¸€æ£µæ ‘çš„é”™è¯¯çš„å¼ºåº¦ã€‚
- è¾ƒé«˜çš„å­¦ä¹ ç‡æ„å‘³ç€æ¯æ£µæ ‘éƒ½å¯ä»¥åšå‡ºè¾ƒå¼ºçš„ä¿®æ­£ï¼Œè¿™æ ·æ¨¡å‹æ›´ä¸ºå¤æ‚ã€‚é€šè¿‡å¢å¤§ `n_estimators` æ¥å‘é›†æˆä¸­æ·»åŠ æ›´å¤šæ ‘ï¼Œä¹Ÿå¯ä»¥å¢åŠ æ¨¡å‹å¤æ‚åº¦ï¼Œå› ä¸ºæ¨¡å‹æœ‰æ›´å¤šæœºä¼šçº æ­£è®­ç»ƒé›†ä¸Šçš„é”™è¯¯ã€‚

#### Sklearnä¸­çš„GradientBoosting

- ä¸‹é¢æ˜¯åœ¨ä¹³è…ºç™Œæ•°æ®é›†ä¸Šåº”ç”¨ GradientBoostingClassifier çš„ç¤ºä¾‹ã€‚

  - é»˜è®¤ä½¿ç”¨ 100 æ£µæ ‘ï¼Œ
  - æœ€å¤§æ·±åº¦æ˜¯ 3ï¼Œ
  - å­¦ä¹ ç‡ä¸º 0.1ï¼š

- 

  ```python
  from sklearn.ensemble import GradientBoostingClassifier
  from sklearn.model_selection import train_test_split 
  from sklearn.datasets import load_breast_cancer
  
  cancer = load_breast_cancer()
  
  X_train, X_test, y_train, y_test = train_test_split( 
      cancer.data, cancer.target, random_state=0) 
   
  gbrt = GradientBoostingClassifier(random_state=0) 
  gbrt.fit(X_train, y_train) 
   
  print("Accuracy on training set: {:.3f}".format(gbrt.score(X_train, y_train))) 
  print("Accuracy on test set: {:.3f}".format(gbrt.score(X_test, y_test))) 
   
  ```

  - ```bash
    Accuracy on training set: 1.000
    Accuracy on test set: 0.965
    ```

  - è®­ç»ƒé›†å¾—åˆ†ä¸º1,å¾ˆå¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆ,ä¸ºäº†é™ä½è¿‡æ‹Ÿåˆï¼Œæˆ‘ä»¬å¯ä»¥

    - é™åˆ¶æœ€å¤§æ·±åº¦æ¥åŠ å¼ºé¢„å‰ªæ
    - é™ä½å­¦ä¹ ç‡

- ```python
  gbrt = GradientBoostingClassifier(random_state=0, max_depth=1) 
  gbrt.fit(X_train, y_train) 
   
  print("Accuracy on training set: {:.3f}".format(gbrt.score(X_train, y_train)))
  print("Accuracy on test set: {:.3f}".format(gbrt.score(X_test, y_test))) 
  ```

  - ```bash
    Accuracy on training set: 0.991
    Accuracy on test set: 0.972
    ```

- ```python
  gbrt = GradientBoostingClassifier(random_state=0, learning_rate=0.01) 
  gbrt.fit(X_train, y_train) 
   
  print("Accuracy on training set: {:.3f}".format(gbrt.score(X_train, y_train)))
  print("Accuracy on test set: {:.3f}".format(gbrt.score(X_test, y_test))) 
   
   
  ```

  - ```bash
    Accuracy on training set: 0.988
    Accuracy on test set: 0.958
    ```

- é™ä½æ¨¡å‹å¤æ‚åº¦çš„ä¸¤ç§æ–¹æ³•éƒ½é™ä½äº†è®­ç»ƒé›†ç²¾åº¦ï¼Œè¿™å’Œé¢„æœŸç›¸åŒã€‚

- åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œå‡å°æ ‘çš„æœ€å¤§æ·±åº¦æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œè€Œé™ä½å­¦ä¹ ç‡ä»…ç¨ç¨æé«˜äº†æ³›åŒ–æ€§èƒ½ã€‚

- å¯¹äºå…¶ä»–åŸºäºå†³ç­–æ ‘çš„æ¨¡å‹ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å°†ç‰¹å¾é‡è¦æ€§**å¯è§†åŒ–**ï¼Œä»¥ä¾¿æ›´å¥½åœ°ç†è§£æ¨¡å‹ã€‚

  - ```python
    import matplotlib.pyplot as plt
    import numpy as np
    #é«˜æ¸…å›¾(svg)æ”¾å¤§ä¸å¤±çœŸ!
    from matplotlib_inline import backend_inline
    backend_inline.set_matplotlib_formats('svg')
    
    
    def plot_feature_importances_cancer(model): 
        n_features = cancer.data.shape[1] 
        plt.barh(range(n_features), model.feature_importances_, align='center') 
        plt.yticks(np.arange(n_features), cancer.feature_names) 
        plt.xlabel("Feature importance") 
        plt.ylabel("Feature") 
    
    gbrt = GradientBoostingClassifier(random_state=0, max_depth=1) 
    gbrt.fit(X_train, y_train) 
     
    plot_feature_importances_cancer(gbrt) 
     
     
    ```

    



- åœ¨ADABOOSTä¸­ï¼Œæˆ‘ä»¬ä»ä¸€ä¸ªå‡è®¾$h_1$å‡ºå‘ï¼Œå¹¶ç”¨ä¸€ç³»åˆ—å‡è®¾å¯¹å…¶è¿›è¡Œ**è‡ªé€‚åº”æå‡**ï¼Œè¿™äº›å‡è®¾æ›´åŠ <u>æ³¨é‡ä¹‹å‰å‡è®¾åˆ†ç±»é”™è¯¯çš„æ ·ä¾‹ã€‚</u>
- åœ¨**æ¢¯åº¦æå‡æ³•**ä¸­ï¼Œè¿˜å¼•å…¥äº†æ–°çš„è‡ªé€‚åº”æå‡å‡è®¾ï¼Œè¿™äº›å‡è®¾å¹¶<u>ä¸å…³æ³¨ç‰¹å®šçš„æ ·ä¾‹</u>ï¼Œè€Œæ³¨é‡<u>æ­£ç¡®ç­”æ¡ˆä¸å…ˆå‰å‡è®¾æ‰€ç»™å‡ºçš„ç­”æ¡ˆä¹‹é—´çš„æ¢¯åº¦</u>ã€‚
- åƒå…¶ä»–ä½¿ç”¨äº†æ¢¯åº¦ä¸‹é™çš„ç®—æ³•ä¸€æ ·ï¼Œä»å¯å¾®çš„æŸå¤±å‡½æ•°å…¥æ‰‹,æˆ‘ä»¬å¯ä»¥å°†å¹³æ–¹è¯¯å·®æŸå¤±ç”¨äºå›å½’ï¼Œå°†å¯¹æ•°æŸå¤±ç”¨äºåˆ†ç±»ã€‚
- ä¸ADABOOSTä¸­ä¸€æ ·ï¼Œæœ‰äº†åŸºæ¨¡å‹åï¼Œæ„é€ å†³ç­–æ ‘ã€‚ä¾‹å¦‚ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ¥è·å¾—æœ€å°åŒ–æŸå¤±çš„æ¨¡å‹å‚æ•°â€”â€”è®¡ç®—æŸå¤±å‡½æ•°ï¼Œå¹¶æœæŸå¤±å‡½æ•°é™ä½æœ€å¿«çš„æ–¹å‘æ›´æ–°å‚æ•°ã€‚
- ä½¿ç”¨æ¢¯åº¦æå‡æ³•æ—¶ï¼Œæˆ‘ä»¬ä¸ä¼šæ›´æ–°ç°æœ‰æ¨¡å‹çš„å‚æ•°ï¼Œæˆ‘ä»¬<u>æ›´æ–°çš„æ˜¯ä¸‹ä¸€ä¸ªå†³ç­–æ ‘çš„å‚æ•°</u>ï¼Œä½†æ˜¯å¿…é¡»é€šè¿‡æ²¿ç€æ¢¯åº¦çš„æ–¹å‘ç§»åŠ¨æ¥å‡å°æŸå¤±ã€‚
- æ­£åˆ™åŒ–æœ‰åŠ©äºé˜²æ­¢è¿‡æ‹Ÿåˆ,å…¶å…·ä½“å½¢å¼å¯ä»¥æ˜¯:
  - é™åˆ¶å†³ç­–æ ‘çš„æ•°é‡æˆ–å¤§å°ï¼ˆå°±å…¶æ·±åº¦æˆ–èŠ‚ç‚¹æ•°è€Œè¨€ï¼‰ã€‚
  - æ­£åˆ™åŒ–å¯ä»¥æ¥è‡ªå­¦ä¹ ç‡ ï¼Œå®ƒè¡¨ç¤ºæ²¿æ¢¯åº¦æ–¹å‘ç§»åŠ¨çš„è·ç¦»ï¼›å­¦ä¹ ç‡è¶Šå°æ„å‘³ç€æˆ‘ä»¬åœ¨é›†æˆæ—¶éœ€è¦çš„å†³ç­–æ ‘è¶Šå¤šã€‚

#### å°ç»“

- Gradient Boostingï¼ˆæ¢¯åº¦æå‡ï¼‰ç®—æ³•æ˜¯ä¸€ç§é›†æˆå­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆå¤šä¸ªå¼±å­¦ä¹ å™¨æ¥æ„å»ºä¸€ä¸ªå¼ºå­¦ä¹ å™¨ã€‚
- ä¸AdaBoostç±»ä¼¼ï¼ŒGradient Boostingä¹Ÿæ˜¯ä¸€ä¸ªè¿­ä»£è¿‡ç¨‹ï¼Œä½†å®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨æŸå¤±å‡½æ•°çš„æ¢¯åº¦æ¥ä¼˜åŒ–æ¨¡å‹ã€‚åœ¨æ¯ä¸€è½®è¿­ä»£ä¸­ï¼ŒGradient Boostingä¼šè®­ç»ƒä¸€ä¸ªæ–°çš„å¼±å­¦ä¹ å™¨æ¥æ‹Ÿåˆå‰ä¸€è½®æ¨¡å‹çš„è´Ÿæ¢¯åº¦ï¼ˆæ®‹å·®ï¼‰ï¼Œä»è€Œé€æ­¥å‡å°æŸå¤±å‡½æ•°çš„å€¼ã€‚
- Gradient Boostingç®—æ³•çš„ä¸»è¦æ­¥éª¤å¦‚ä¸‹ï¼š
  1. åˆå§‹åŒ–æ¨¡å‹ï¼šä½¿ç”¨ä¸€ä¸ªå¸¸æ•°å€¼ï¼ˆå¦‚è®­ç»ƒé›†ç›®æ ‡å€¼çš„å‡å€¼ï¼‰ä½œä¸ºåˆå§‹æ¨¡å‹ã€‚
  2. å¯¹äºæ¯ä¸€è½®è¿­ä»£ï¼š a. è®¡ç®—è®­ç»ƒé›†ä¸Šæ¯ä¸ªæ ·æœ¬çš„è´Ÿæ¢¯åº¦ï¼ˆæ®‹å·®ï¼‰ã€‚ b. ä½¿ç”¨å¸¦æƒé‡çš„è®­ç»ƒé›†ï¼ˆæƒé‡ä¸ºè´Ÿæ¢¯åº¦ï¼‰è®­ç»ƒä¸€ä¸ªå¼±å­¦ä¹ å™¨ã€‚ c. è®¡ç®—å¼±å­¦ä¹ å™¨åœ¨è®­ç»ƒé›†ä¸Šçš„é¢„æµ‹ç»“æœã€‚ d. ä½¿ç”¨çº¿æ€§æœç´¢æ–¹æ³•æ‰¾åˆ°ä¸€ä¸ªæœ€ä½³çš„å­¦ä¹ ç‡ï¼Œä½¿å¾—æŸå¤±å‡½æ•°å€¼æœ€å°ã€‚ e. å°†å¼±å­¦ä¹ å™¨çš„é¢„æµ‹ç»“æœä¹˜ä»¥å­¦ä¹ ç‡ï¼Œç„¶ååŠ åˆ°å½“å‰æ¨¡å‹ä¸Šï¼Œå¾—åˆ°æ–°çš„æ¨¡å‹ã€‚
  3. å°†æ‰€æœ‰å¼±å­¦ä¹ å™¨çš„é¢„æµ‹ç»“æœåŠ æƒç»“åˆï¼Œå¾—åˆ°æœ€ç»ˆçš„é¢„æµ‹ç»“æœã€‚
- Gradient Boostingç®—æ³•çš„ä¼˜ç‚¹ï¼š
  1. çµæ´»æ€§ï¼šGradient Boostingç®—æ³•å¯ä»¥ç”¨äºå¤šç§æŸå¤±å‡½æ•°ï¼ŒåŒ…æ‹¬å¹³æ–¹æŸå¤±ã€å¯¹æ•°æŸå¤±ç­‰ï¼Œå› æ­¤é€‚ç”¨äºå›å½’å’Œåˆ†ç±»é—®é¢˜ã€‚
  2. é«˜æ•ˆæ€§ï¼šGradient Boostingç®—æ³•é€šè¿‡ä¼˜åŒ–æŸå¤±å‡½æ•°çš„æ¢¯åº¦æ¥å®ç°è¯¯å·®ä¿®æ­£ï¼Œè¿™ä½¿å¾—ç®—æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæ›´å¿«åœ°æ”¶æ•›ã€‚
  3. é˜²æ­¢è¿‡æ‹Ÿåˆï¼šé€šè¿‡ç»“åˆå¤šä¸ªå¼±å­¦ä¹ å™¨ï¼ŒGradient Boostingç®—æ³•å¯ä»¥é™ä½æ¨¡å‹çš„æ–¹å·®ï¼Œä»è€Œå‡å°‘è¿‡æ‹Ÿåˆçš„é£é™©ã€‚æ­¤å¤–ï¼Œå¯ä»¥é€šè¿‡è°ƒæ•´å­¦ä¹ ç‡å’Œæ ‘çš„æ·±åº¦ç­‰è¶…å‚æ•°æ¥è¿›ä¸€æ­¥æ§åˆ¶è¿‡æ‹Ÿåˆã€‚
- Gradient Boostingç®—æ³•çš„ç¼ºç‚¹ï¼š
  1. è®­ç»ƒé€Ÿåº¦ï¼šç”±äºGradient Boostingç®—æ³•æ˜¯ä¸€ä¸ªè¿­ä»£è¿‡ç¨‹ï¼Œæ¯ä¸€è½®è®­ç»ƒéƒ½ä¾èµ–äºå‰ä¸€è½®çš„ç»“æœï¼Œå› æ­¤æ— æ³•å¹¶è¡Œè®­ç»ƒå¼±å­¦ä¹ å™¨ï¼Œè®­ç»ƒé€Ÿåº¦å¯èƒ½è¾ƒæ…¢ã€‚ç„¶è€Œï¼Œå¯ä»¥ä½¿ç”¨ä¼˜åŒ–è¿‡çš„å®ç°ï¼ˆå¦‚XGBoostå’ŒLightGBMï¼‰æ¥åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚
  2. è°ƒå‚ï¼šGradient Boostingç®—æ³•æœ‰å¤šä¸ªè¶…å‚æ•°éœ€è¦è°ƒæ•´ï¼Œå¦‚å­¦ä¹ ç‡ã€æ ‘çš„æ·±åº¦ã€å¼±å­¦ä¹ å™¨çš„æ•°é‡ç­‰ã€‚è¿™å¯èƒ½å¯¼è‡´è°ƒå‚è¿‡ç¨‹è¾ƒä¸ºå¤æ‚ã€‚

## å¯¹æ¯”AdaBoosting@GradientBoosting

- ä¸¤ç§æ–¹æ³•éƒ½æ˜¯ç”¨äºæé«˜æ¨¡å‹æ€§èƒ½çš„æŠ€æœ¯ï¼Œå®ƒä»¬é€šè¿‡ç»„åˆå¤šä¸ªå¼±å­¦ä¹ å™¨æ¥æ„å»ºä¸€ä¸ªå¼ºå¤§çš„é¢„æµ‹æ¨¡å‹.
- Adaboostä¸»è¦å…³æ³¨çº æ­£åˆ†ç±»é”™è¯¯ï¼Œè€ŒGradient Boostå…³æ³¨ä¼˜åŒ–æŸå¤±å‡½æ•°ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å®ç°å’ŒåŸç†ä¸Šæœ‰ä¸€äº›å…³é”®åŒºåˆ«ã€‚

- ç®—æ³•åŸç†ï¼š Adaboostï¼ˆè‡ªé€‚åº”å¢å¼ºï¼‰æ˜¯ä¸€ç§åŸºäºåŠ æƒæŠ•ç¥¨çš„é›†æˆæ–¹æ³•ã€‚å®ƒé€šè¿‡è¿­ä»£åœ°è®­ç»ƒä¸€ç³»åˆ—å¼±å­¦ä¹ å™¨ï¼Œæ¯ä¸ªå­¦ä¹ å™¨éƒ½è¯•å›¾çº æ­£å‰ä¸€ä¸ªå­¦ä¹ å™¨çš„é”™è¯¯ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼ŒAdaboostä¼šå¢åŠ è¯¯åˆ†ç±»æ ·æœ¬çš„æƒé‡ï¼Œä½¿å¾—åç»­çš„å­¦ä¹ å™¨æ›´å…³æ³¨è¿™äº›éš¾ä»¥åˆ†ç±»çš„æ ·æœ¬ã€‚æœ€åï¼Œæ‰€æœ‰å­¦ä¹ å™¨çš„é¢„æµ‹ç»“æœä¼šæ ¹æ®å…¶åœ¨è®­ç»ƒé›†ä¸Šçš„å‡†ç¡®æ€§è¿›è¡ŒåŠ æƒæŠ•ç¥¨ï¼Œä»¥å¾—åˆ°æœ€ç»ˆçš„é¢„æµ‹ã€‚

- Gradient Boostï¼ˆæ¢¯åº¦æå‡ï¼‰æ˜¯ä¸€ç§åŸºäºæ¢¯åº¦ä¸‹é™çš„é›†æˆæ–¹æ³•ã€‚ä¸Adaboostä¸åŒï¼ŒGradient Boostå…³æ³¨çš„æ˜¯ä¼˜åŒ–æŸå¤±å‡½æ•°ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œå®ƒä¼šè®­ç»ƒä¸€ä¸ªæ–°çš„å¼±å­¦ä¹ å™¨æ¥æ‹Ÿåˆå‰ä¸€ä¸ªå­¦ä¹ å™¨é¢„æµ‹æ®‹å·®çš„è´Ÿæ¢¯åº¦ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒGradient Boosté€æ­¥ä¼˜åŒ–æŸå¤±å‡½æ•°ï¼Œä»è€Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚

### æŸå¤±å‡½æ•°

- Adaboostä¸»è¦å…³æ³¨åˆ†ç±»é”™è¯¯ï¼Œä½¿ç”¨æŒ‡æ•°æŸå¤±å‡½æ•°ã€‚è¿™ä½¿å¾—Adaboostå¯¹å¼‚å¸¸å€¼éå¸¸æ•æ„Ÿï¼Œå› ä¸ºå®ƒä¼šè¯•å›¾çº æ­£æ‰€æœ‰çš„é”™è¯¯ï¼Œå³ä½¿æ˜¯é‚£äº›å¯èƒ½æ˜¯å™ªå£°çš„æ ·æœ¬ã€‚

- Gradient Boostå¯ä»¥ä½¿ç”¨å¤šç§æŸå¤±å‡½æ•°ï¼Œè¿™ä½¿å¾—å®ƒæ›´åŠ çµæ´»ï¼Œå¯ä»¥åº”ç”¨äºå›å½’å’Œåˆ†ç±»é—®é¢˜ã€‚æ­¤å¤–ï¼Œç”±äºå®ƒå…³æ³¨çš„æ˜¯ä¼˜åŒ–æŸå¤±å‡½æ•°ï¼Œè€Œä¸ä»…ä»…æ˜¯çº æ­£é”™è¯¯ï¼Œå› æ­¤å®ƒå¯¹å¼‚å¸¸å€¼çš„æ•æ„Ÿæ€§è¾ƒä½ã€‚

### å­¦ä¹ é€Ÿç‡

-  Gradient Boosté€šå¸¸åŒ…å«ä¸€ä¸ªå­¦ä¹ é€Ÿç‡å‚æ•°ï¼Œç”¨äºæ§åˆ¶æ¯ä¸ªå¼±å­¦ä¹ å™¨å¯¹æœ€ç»ˆé¢„æµ‹çš„è´¡çŒ®ã€‚è¾ƒä½çš„å­¦ä¹ é€Ÿç‡å¯èƒ½éœ€è¦æ›´å¤šçš„è¿­ä»£æ¬¡æ•°ï¼Œä½†å¯ä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
- Adaboostæ²¡æœ‰ç±»ä¼¼çš„å‚æ•°ï¼Œä½†å¯ä»¥é€šè¿‡è°ƒæ•´å¼±å­¦ä¹ å™¨çš„æ•°é‡æ¥æ§åˆ¶æ¨¡å‹å¤æ‚æ€§ã€‚

### æ­£åˆ™åŒ–

- Gradient Boostå¯ä»¥é€šè¿‡å¼•å…¥æ­£åˆ™åŒ–é¡¹æ¥é˜²æ­¢è¿‡æ‹Ÿåˆã€‚ä¾‹å¦‚ï¼Œå®ƒå¯ä»¥ä½¿ç”¨L1æˆ–L2æ­£åˆ™åŒ–æ¥çº¦æŸæ¨¡å‹çš„å¤æ‚æ€§ã€‚
- Adaboostæ²¡æœ‰å†…ç½®çš„æ­£åˆ™åŒ–æœºåˆ¶ï¼Œä½†å¯ä»¥é€šè¿‡<u>è°ƒæ•´å¼±å­¦ä¹ å™¨çš„æ•°é‡</u>æˆ–ä½¿ç”¨<u>æ­£åˆ™åŒ–çš„åŸºå­¦ä¹ å™¨</u>æ¥å®ç°ç±»ä¼¼çš„æ•ˆæœã€‚

### å°ç»“

- ä¸€ä¸ªé›†æˆæ–¹æ³•æ˜¯æ¨¡å‹çš„ç»“åˆï¼Œå…¶æ€§èƒ½è¦ä¼˜äºä»»æ„ä¸€ä¸ªå…¶ä¸­çš„ç»„ä»¶ã€‚
- æˆ‘ä»¬ä¸»è¦è®¨è®ºäº† 3 ç§è®­ç»ƒé›†æˆçš„æ–¹æ³•ã€‚
  - è‡ªå‘èšé›†æˆ–è€…å¥—è¢‹æ³•ï¼Œå¯ä»¥å‡å°ä¸€ä¸ªä¼°è®¡å™¨çš„æ–¹å·®ã€‚
    - å¥—è¢‹æ³•ä½¿ç”¨è‡ªå‘é‡é‡‡æ ·æ¥åˆ›å»ºå¤šä¸ªè®­ç»ƒé›†å˜ä½“ã€‚åœ¨è¿™äº›å˜ä½“ä¸Šè®­ç»ƒçš„æ¨¡å‹çš„é¢„æµ‹å€¼å°†ä¼šè¢«å¹³å‡ã€‚
    - å¥—è¢‹å†³ç­–æ ‘è¢«ç§°ä¸ºéšæœºæ£®æ—ã€‚
  - æ¨è¿›æ³•æ˜¯ä¸€ç§èƒ½å‡å°‘åŸºç¡€ä¼°è®¡å™¨åå·®çš„é›†æˆå…ƒä¼°è®¡å™¨ã€‚
    - AdaBoost ç®—æ³•æ˜¯ä¸€ç§æµè¡Œçš„æ¨è¿›ç®—æ³•ï¼Œå®ƒè¿­ä»£åœ°åœ¨è®­ç»ƒæ•°æ®ä¸Šè®­ç»ƒä¼°è®¡å™¨ï¼Œè®­ç»ƒæ•°æ®çš„æƒé‡å°†ä¼šæ ¹æ®å‰ä¸€ä¸ªä¼°è®¡å™¨çš„è¯¯å·®è¿›è¡Œè°ƒæ•´ã€‚
  - å †å æ³•ä¸­ï¼Œä¸€ä¸ªå…ƒä¼°è®¡å™¨å¯ä»¥å­¦ä¹ å»åˆå¹¶å¼‚ç±»åŸºç¡€ä¼°è®¡å™¨çš„é¢„æµ‹ç»“æœã€‚ 

# Skearnä¸­çš„é›†æˆå­¦ä¹ 

## **ab**stract

- The goal of **ensemble methods** is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator.

- Two families of ensemble methods are usually distinguished:

  - In **averaging methods**, the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced.

    **Examples:** [Bagging methods](https://scikit-learn.org/stable/modules/ensemble.html#bagging), [Forests of randomized trees](https://scikit-learn.org/stable/modules/ensemble.html#forest), â€¦

  - By contrast, in **boosting methods**, base estimators are built sequentially and one tries to reduce the **bias** of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble.

    **Examples:** [AdaBoost](https://scikit-learn.org/stable/modules/ensemble.html#adaboost), [Gradient Tree Boosting](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting), â€¦

### ç¿»è¯‘1

- é›†æˆå­¦ä¹ çš„ç›®æ ‡æ˜¯å°†ä½¿ç”¨ç»™å®šå­¦ä¹ ç®—æ³•æ„å»ºçš„å¤šä¸ªåŸºä¼°è®¡å™¨çš„é¢„æµ‹ç»“æœç»“åˆèµ·æ¥ï¼Œä»¥æé«˜å¯¹å•ä¸ªä¼°è®¡å™¨çš„æ³›åŒ–æ€§å’Œé²æ£’æ€§ã€‚


- é€šå¸¸åŒºåˆ†ä¸¤ç±»é›†æˆæ–¹æ³•ï¼š
  - åœ¨å¹³å‡æ–¹æ³•ä¸­ï¼Œä¸»è¦åŸç†æ˜¯ç‹¬ç«‹æ„å»ºå¤šä¸ªä¼°è®¡å™¨ï¼Œç„¶åå¯¹å®ƒä»¬çš„é¢„æµ‹ç»“æœè¿›è¡Œå¹³å‡ã€‚å¹³å‡è€Œè¨€ï¼Œç»„åˆä¼°è®¡å™¨é€šå¸¸ä¼˜äºä»»ä½•å•ä¸ªåŸºä¼°è®¡å™¨ï¼Œå› ä¸ºå…¶æ–¹å·®é™ä½äº†ã€‚
    - ä¾‹å¦‚ï¼šBagging æ–¹æ³•ã€éšæœºæ£®æ—ç­‰ã€‚
  - ç›¸æ¯”ä¹‹ä¸‹ï¼Œåœ¨æå‡æ–¹æ³•ä¸­ï¼ŒåŸºä¼°è®¡å™¨æ˜¯é¡ºåºæ„å»ºçš„ï¼Œä»¥å°è¯•é™ä½ç»„åˆä¼°è®¡å™¨çš„åå·®ã€‚åŠ¨æœºæ˜¯å°†å¤šä¸ªå¼±æ¨¡å‹ç»„åˆæˆä¸€ä¸ªå¼ºå¤§çš„é›†æˆæ¨¡å‹ã€‚
    - ä¾‹å¦‚ï¼šAdaBoostã€æ¢¯åº¦æå‡æ ‘ç­‰ã€‚

### ç¿»è¯‘2

**é›†æˆæ–¹æ³•** çš„ç›®æ ‡æ˜¯æŠŠå¤šä¸ªä½¿ç”¨ç»™å®šå­¦ä¹ ç®—æ³•æ„å»ºçš„åŸºä¼°è®¡å™¨çš„é¢„æµ‹ç»“æœç»“åˆèµ·æ¥ï¼Œä»è€Œè·å¾—æ¯”å•ä¸ªä¼°è®¡å™¨æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›/é²æ£’æ€§ã€‚

é›†æˆæ–¹æ³•é€šå¸¸åˆ†ä¸ºä¸¤ç§:

- **å¹³å‡æ–¹æ³•**ï¼Œè¯¥æ–¹æ³•çš„åŸç†æ˜¯æ„å»ºå¤šä¸ªç‹¬ç«‹çš„ä¼°è®¡å™¨ï¼Œç„¶åå–å®ƒä»¬çš„é¢„æµ‹ç»“æœçš„å¹³å‡ã€‚ä¸€èˆ¬æ¥è¯´ç»„åˆä¹‹åçš„ä¼°è®¡å™¨æ˜¯ä¼šæ¯”å•ä¸ªä¼°è®¡å™¨è¦å¥½çš„ï¼Œå› ä¸ºå®ƒçš„æ–¹å·®å‡å°äº†ã€‚

  **ç¤ºä¾‹:** [Bagging æ–¹æ³•](https://scikitlearn.com.cn/0.21.3/12/#1111-bagging-meta-estimatorï¼ˆbagging-å…ƒä¼°è®¡å™¨ï¼‰) , [éšæœºæ£®æ—](https://scikitlearn.com.cn/0.21.3/12/#11121-éšæœºæ£®æ—) , â€¦

- ç›¸æ¯”ä¹‹ä¸‹ï¼Œåœ¨ **boosting æ–¹æ³•** ä¸­ï¼ŒåŸºä¼°è®¡å™¨æ˜¯ä¾æ¬¡æ„å»ºçš„ï¼Œå¹¶ä¸”æ¯ä¸€ä¸ªåŸºä¼°è®¡å™¨éƒ½å°è¯•å»å‡å°‘ç»„åˆä¼°è®¡å™¨çš„åå·®ã€‚è¿™ç§æ–¹æ³•ä¸»è¦ç›®çš„æ˜¯ä¸ºäº†ç»“åˆå¤šä¸ªå¼±æ¨¡å‹ï¼Œä½¿é›†æˆçš„æ¨¡å‹æ›´åŠ å¼ºå¤§ã€‚

  **ç¤ºä¾‹:** [AdaBoost](https://scikitlearn.com.cn/0.21.3/12/#1113-adaboost) , [æ¢¯åº¦æå‡æ ‘](https://scikitlearn.com.cn/0.21.3/12/#1114-gradient-tree-boostingï¼ˆæ¢¯åº¦æ ‘æå‡ï¼‰) , â€¦











