[toc]

# ML@sklearn@MLæµç¨‹Part3@AutomaticParameterSearches

## Automatic parameter searches

- Automatic parameter searchæ˜¯æŒ‡ä½¿ç”¨ç®—æ³•æ¥è‡ªåŠ¨æœç´¢æ¨¡å‹çš„æœ€ä½³è¶…å‚æ•°ï¼ˆhyperparametersï¼‰çš„è¿‡ç¨‹ã€‚è¶…å‚æ•°æ˜¯æ¨¡å‹çš„é…ç½®å‚æ•°ï¼Œå®ƒä»¬ä¸æ˜¯ä»æ•°æ®ä¸­å­¦ä¹ çš„ï¼Œè€Œæ˜¯ç”±äººå·¥è®¾å®šçš„ï¼Œä¾‹å¦‚å­¦ä¹ ç‡ã€æ­£åˆ™åŒ–å¼ºåº¦ã€æœ€å¤§æ·±åº¦ç­‰ã€‚è¶…å‚æ•°çš„é€‰æ‹©å¯¹æ¨¡å‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›æœ‰å¾ˆå¤§çš„å½±å“ï¼Œå› æ­¤é€‰æ‹©æœ€ä½³çš„è¶…å‚æ•°æ˜¯æœºå™¨å­¦ä¹ ä¸­ä¸€ä¸ªéå¸¸é‡è¦çš„ä»»åŠ¡ã€‚
- è‡ªåŠ¨å‚æ•°æœç´¢é€šå¸¸ä½¿ç”¨çš„ç®—æ³•æ˜¯ç½‘æ ¼æœç´¢ï¼ˆGrid Searchï¼‰ã€éšæœºæœç´¢ï¼ˆRandom Searchï¼‰å’Œè´å¶æ–¯ä¼˜åŒ–ï¼ˆBayesian Optimizationï¼‰ç­‰ã€‚è¿™äº›ç®—æ³•å…·æœ‰ä¸åŒçš„ä¼˜ç¼ºç‚¹ï¼Œå¯ä»¥æ ¹æ®é—®é¢˜çš„æ€§è´¨å’Œæ•°æ®é›†çš„å¤§å°é€‰æ‹©åˆé€‚çš„ç®—æ³•ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œç½‘æ ¼æœç´¢æ˜¯æœ€ç®€å•ã€æœ€ç›´è§‚çš„æ–¹æ³•ï¼Œä½†éœ€è¦æœç´¢çš„ç©ºé—´è¾ƒå¤§æ—¶ä¼šéå¸¸è€—æ—¶ï¼›éšæœºæœç´¢å¯ä»¥åœ¨æ›´çŸ­çš„æ—¶é—´å†…æ‰¾åˆ°åˆé€‚çš„è¶…å‚æ•°ï¼Œä½†å¯èƒ½æ— æ³•åœ¨æœç´¢ç©ºé—´ä¸­å…¨é¢è¦†ç›–ï¼›è´å¶æ–¯ä¼˜åŒ–åˆ™æ˜¯ä¸€ç§æ›´é«˜çº§çš„æ–¹æ³•ï¼Œå®ƒå¯ä»¥æ ¹æ®å…ˆå‰çš„æœç´¢ç»“æœæ¥è°ƒæ•´æœç´¢ç©ºé—´ï¼Œä»è€Œæ›´å¿«åœ°æ‰¾åˆ°æœ€ä¼˜è§£ã€‚
- è‡ªåŠ¨å‚æ•°æœç´¢çš„ä¼˜ç‚¹æ˜¯å¯ä»¥é¿å…æ‰‹åŠ¨è°ƒå‚çš„ç¹çè¿‡ç¨‹ï¼ŒåŒæ—¶å¯ä»¥æ›´å…¨é¢åœ°æœç´¢è¶…å‚æ•°çš„ç©ºé—´ï¼Œä»è€Œå¾—åˆ°æ›´å¥½çš„æ¨¡å‹æ€§èƒ½ã€‚ç¼ºç‚¹åˆ™æ˜¯éœ€è¦æ¶ˆè€—å¤§é‡çš„è®¡ç®—èµ„æºå’Œæ—¶é—´ï¼Œå°¤å…¶æ˜¯åœ¨æœç´¢ç©ºé—´è¾ƒå¤§çš„æƒ…å†µä¸‹ã€‚å› æ­¤ï¼Œè‡ªåŠ¨å‚æ•°æœç´¢é€šå¸¸ç”¨äºå¯¹æ¨¡å‹è¿›è¡Œç²¾ç»†è°ƒæ•´æ—¶ï¼Œè€Œä¸æ˜¯åœ¨æ¨¡å‹é€‰æ‹©å’ŒåŸå‹å¼€å‘é˜¶æ®µä½¿ç”¨ã€‚

- All <u>estimators</u> have **parameters** (often called **hyper-parameters** in the literature) that can be tuned. 
- The generalization power of an estimator often critically depends on a few parameters. 
  - For example a [`RandomForestRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor) has a `n_estimators` parameter that determines the number of trees in the forest, and a `max_depth` parameter that determines the maximum depth of each tree. 

- Quite often, it is not clear what the exact values of these parameters should be since they depend on the data at hand.

- `Scikit-learn` provides tools to automatically **find the best parameter combinations** (via cross-validation). 
- In the following example, we randomly search over the parameter space of a random forest with a [`RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV) object. 
- When the search is over, the [`RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV) behaves as a [`RandomForestRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor) that has been **fitted with the best set of parameters**. Read more in the [User Guide](https://scikit-learn.org/stable/modules/grid_search.html#grid-search):

- [3.2. Tuning the hyper-parameters of an estimator â€” scikit-learn documentation](https://scikit-learn.org/stable/modules/grid_search.html)

### demo

```python

from sklearn.datasets import fetch_california_housing
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV,GridSearchCV
from sklearn.model_selection import train_test_split
from scipy.stats import randint

X, y = fetch_california_housing(return_X_y=True)
iris=load_iris()
X,y=iris.data,iris.target # type: ignore
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

##
# define the parameter space that will be searched over
param_distributions = {'n_estimators': randint(1, 5),
                       'max_depth': randint(3,6)}

# now create a searchCV object and fit it to the data
#æ ¹æ®è¶…å‚æ•°ç©ºé—´,æ„é€ CVå¯¹è±¡
RFsearch = RandomizedSearchCV(estimator=RandomForestRegressor(random_state=0),
                            n_iter=5,
                            param_distributions=param_distributions,
                            verbose=3,
                            random_state=0)
# å¼€å§‹æœç´¢(è°ƒç”¨SearchCVå¯¹è±¡çš„fitæ–¹æ³•)
RFsearch.fit(X_train, y_train)
#ä»æœç´¢ç»“æœä¸­è·å–æœ€ä¼˜å‚æ•°
print(RFsearch.best_params_,"@{search.best_params_}")
#è®¡ç®—å¾—åˆ†
RFsearch.score(X_test, y_test)

```

- ```bash
  Fitting 3 folds for each of 12 candidates, totalling 36 fits
  [CV 1/3] END .......max_depth=3, n_estimators=1;, score=0.892 total time=   0.0s
  [CV 2/3] END .......max_depth=3, n_estimators=1;, score=0.967 total time=   0.0s
  [CV 3/3] END .......max_depth=3, n_estimators=1;, score=0.917 total time=   0.0s
  [CV 1/3] END .......max_depth=3, n_estimators=2;, score=0.908 total time=   0.0s
  [CV 2/3] END .......max_depth=3, n_estimators=2;, score=0.988 total time=   0.0s
  [CV 3/3] END .......max_depth=3, n_estimators=2;, score=0.917 total time=   0.0s
  [CV 1/3] END .......max_depth=3, n_estimators=3;, score=0.920 total time=   0.0s
  [CV 2/3] END .......max_depth=3, n_estimators=3;, score=0.992 total time=   0.0s
  [CV 3/3] END .......max_depth=3, n_estimators=3;, score=0.912 total time=   0.0s
  [CV 1/3] END .......max_depth=4, n_estimators=1;, score=0.913 total time=   0.0s
  [CV 2/3] END .......max_depth=4, n_estimators=1;, score=0.967 total time=   0.0s
  [CV 3/3] END .......max_depth=4, n_estimators=1;, score=0.917 total time=   0.0s
  [CV 1/3] END .......max_depth=4, n_estimators=2;, score=0.913 total time=   0.0s
  [CV 2/3] END .......max_depth=4, n_estimators=2;, score=0.992 total time=   0.0s
  [CV 3/3] END .......max_depth=4, n_estimators=2;, score=0.917 total time=   0.0s
  [CV 1/3] END .......max_depth=4, n_estimators=3;, score=0.923 total time=   0.0s
  [CV 2/3] END .......max_depth=4, n_estimators=3;, score=0.996 total time=   0.0s
  [CV 3/3] END .......max_depth=4, n_estimators=3;, score=0.912 total time=   0.0s
  [CV 1/3] END .......max_depth=5, n_estimators=1;, score=0.913 total time=   0.0s
  [CV 2/3] END .......max_depth=5, n_estimators=1;, score=0.967 total time=   0.0s
  [CV 3/3] END .......max_depth=5, n_estimators=1;, score=0.917 total time=   0.0s
  [CV 1/3] END .......max_depth=5, n_estimators=2;, score=0.913 total time=   0.0s
  [CV 2/3] END .......max_depth=5, n_estimators=2;, score=0.992 total time=   0.0s
  [CV 3/3] END .......max_depth=5, n_estimators=2;, score=0.917 total time=   0.0s
  ...
  [CV 1/3] END .......max_depth=6, n_estimators=3;, score=0.923 total time=   0.0s
  [CV 2/3] END .......max_depth=6, n_estimators=3;, score=0.996 total time=   0.0s
  [CV 3/3] END .......max_depth=6, n_estimators=3;, score=0.912 total time=   0.0s
  f{'max_depth': 4, 'n_estimators': 3}
  0.9794037940379404
  ```

- è¿™æ®µä»£ç ä½¿ç”¨äº†Scikit-learnåº“ä¸­çš„éšæœºæœç´¢ï¼ˆRandomized Searchï¼‰æ–¹æ³•æ¥å¯»æ‰¾æœ€ä½³çš„éšæœºæ£®æ—æ¨¡å‹è¶…å‚æ•°ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥ä»£ç æ‰§è¡Œäº†ä»¥ä¸‹æ­¥éª¤ï¼š

  1. ä»Scikit-learnåº“ä¸­å¯¼å…¥äº†California Housingæ•°æ®é›†ï¼Œéšæœºæ£®æ—å›å½’å™¨ï¼ˆRandomForestRegressorï¼‰ï¼Œéšæœºæœç´¢äº¤å‰éªŒè¯ï¼ˆRandomizedSearchCVï¼‰å’Œéšæœºæ•´æ•°ç”Ÿæˆå‡½æ•°ï¼ˆrandintï¼‰ç­‰å¿…è¦çš„æ¨¡å—ã€‚
  2. ä½¿ç”¨`fetch_california_housing()`å‡½æ•°ä»California Housingæ•°æ®é›†ä¸­è·å–æ•°æ®Xå’Œç›®æ ‡yï¼Œå¹¶å°†å®ƒä»¬åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚(ä½†æ˜¯è€ƒè™‘è¯¥æ•°æ®é›†è¾ƒå¤§,è¿è¡Œéœ€è¦æ‰§è¡Œè¾ƒé•¿æ—¶é—´,å› æ­¤æ”¹ç”¨å°æ•°æ®é›†`iris`)
  3. å®šä¹‰äº†ä¸€ä¸ªè¶…å‚æ•°ç©ºé—´`param_distributions`ï¼ŒåŒ…å«äº†`n_estimators`å’Œ`max_depth`ä¸¤ä¸ªè¶…å‚æ•°ã€‚å…¶ä¸­ï¼Œ`n_estimators`è¡¨ç¤ºéšæœºæ£®æ—æ¨¡å‹ä¸­æ ‘çš„æ•°é‡ï¼Œå–å€¼èŒƒå›´ä¸º1åˆ°5ä¹‹é—´çš„éšæœºæ•´æ•°ï¼›`max_depth`è¡¨ç¤ºæ¯æ£µæ ‘çš„æœ€å¤§æ·±åº¦ï¼Œå–å€¼èŒƒå›´ä¸º5åˆ°10ä¹‹é—´çš„éšæœºæ•´æ•°ã€‚
  4. åˆ›å»ºäº†ä¸€ä¸ªéšæœºæœç´¢äº¤å‰éªŒè¯å¯¹è±¡`search`ï¼Œå¹¶æŒ‡å®šäº†å®ƒçš„å‚æ•°ï¼ŒåŒ…æ‹¬è¦æœç´¢çš„è¶…å‚æ•°ç©ºé—´ã€è¦æœç´¢çš„è¿­ä»£æ¬¡æ•°ï¼ˆn_iterï¼‰ç­‰ã€‚
  5. è°ƒç”¨`fit()`å‡½æ•°å¯¹`search`å¯¹è±¡è¿›è¡Œæ‹Ÿåˆï¼Œä½¿å…¶ä»è®­ç»ƒé›†ä¸­å¯»æ‰¾æœ€ä½³çš„è¶…å‚æ•°ç»„åˆã€‚
  6. è¾“å‡º`search`å¯¹è±¡çš„æœ€ä½³è¶…å‚æ•°ç»„åˆ`search.best_params_`ã€‚
  7. æœ€åï¼Œä½¿ç”¨`search`å¯¹è±¡çš„`score()`å‡½æ•°åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶è¾“å‡ºæ¨¡å‹çš„å¾—åˆ†ã€‚

  éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œéšæœºæœç´¢æ–¹æ³•åœ¨ç»™å®šçš„è¶…å‚æ•°ç©ºé—´ä¸­éšæœºæŠ½æ ·ä¸€ç»„è¶…å‚æ•°è¿›è¡Œè®­ç»ƒï¼Œå¹¶è®¡ç®—å…¶æ€§èƒ½ã€‚å› æ­¤ï¼Œæ¯æ¬¡è¿è¡Œè¯¥ä»£ç å¯èƒ½ä¼šå¾—åˆ°ä¸åŒçš„æœ€ä½³è¶…å‚æ•°ç»„åˆå’Œæ¨¡å‹å¾—åˆ†ã€‚

- ```python
  param_grid = {
      'n_estimators': [1,2,3],
      'max_depth': [3, 4, 5, 6]
  }
  
  # Create a GridSearchCV object and fit it to the data
  Gsearch = GridSearchCV(estimator=RandomForestRegressor(random_state=0),
                          param_grid=param_grid,
                          cv=3,
                          verbose=3)
  Gsearch.fit(X_train, y_train)
  
  print(f"f{Gsearch.best_params_}")
  Gsearch.score(X_test, y_test)
  ```

  - ```bash
    Fitting 3 folds for each of 12 candidates, totalling 36 fits
    [CV 1/3] END .......max_depth=3, n_estimators=1;, score=0.892 total time=   0.0s
    [CV 2/3] END .......max_depth=3, n_estimators=1;, score=0.967 total time=   0.0s
    [CV 3/3] END .......max_depth=3, n_estimators=1;, score=0.917 total time=   0.0s
    [CV 1/3] END .......max_depth=3, n_estimators=2;, score=0.908 total time=   0.0s
    [CV 2/3] END .......max_depth=3, n_estimators=2;, score=0.988 total time=   0.0s
    [CV 3/3] END .......max_depth=3, n_estimators=2;, score=0.917 total time=   0.0s
    [CV 1/3] END .......max_depth=3, n_estimators=3;, score=0.920 total time=   0.0s
    [CV 2/3] END .......max_depth=3, n_estimators=3;, score=0.992 total time=   0.0s
    [CV 3/3] END .......max_depth=3, n_estimators=3;, score=0.912 total time=   0.0s
    [CV 1/3] END .......max_depth=4, n_estimators=1;, score=0.913 total time=   0.0s
    ...
    [CV 1/3] END .......max_depth=6, n_estimators=3;, score=0.923 total time=   0.0s
    [CV 2/3] END .......max_depth=6, n_estimators=3;, score=0.996 total time=   0.0s
    [CV 3/3] END .......max_depth=6, n_estimators=3;, score=0.912 total time=   0.0s
    f{'max_depth': 4, 'n_estimators': 3}
    0.9794037940379404
    ```

    

## model_selection::Hyper-parameter optimizers

- [model_selection document](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection)

- | Methods                                                      | Descriptions                                                 |
  | ------------------------------------------------------------ | ------------------------------------------------------------ |
  | [`model_selection.GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)(estimator, ...) | Exhaustive search over specified parameter values for an estimator. |
  | [`model_selection.HalvingGridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingGridSearchCV.html#sklearn.model_selection.HalvingGridSearchCV)(...[, ...]) | Search over specified parameter values with successive halving. |
  | [`model_selection.ParameterGrid`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ParameterGrid.html#sklearn.model_selection.ParameterGrid)(param_grid) | Grid of parameters with a discrete number of values for each. |
  | [`model_selection.ParameterSampler`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ParameterSampler.html#sklearn.model_selection.ParameterSampler)(...[, ...]) | Generator on parameters sampled from given distributions.    |
  | [`model_selection.RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV)(...[, ...]) | Randomized search on hyper parameters.                       |
  | [`model_selection.HalvingRandomSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingRandomSearchCV.html#sklearn.model_selection.HalvingRandomSearchCV)(...[, ...]) | Randomized search on hyper parameters.                       |

- å…¶ä¸­åˆ—å‡ºäº†`GridSearchCV`ã€`RandomizedSearchCV`ã€`HalvingGridSearchCV`ç­‰ç±»ï¼Œä»¥åŠå®ƒä»¬çš„å‚æ•°å’Œç”¨æ³•ã€‚è¿™äº›ç±»å¯ä»¥ç”¨äºå¯»æ‰¾æœ€ä½³çš„è¶…å‚æ•°ç»„åˆï¼Œå¸®åŠ©ç”¨æˆ·ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚
  - `GridSearchCV`é€šè¿‡ç©·ä¸¾æœç´¢è¶…å‚æ•°ç©ºé—´ä¸­æ‰€æœ‰çš„å¯èƒ½ç»„åˆï¼Œæ¥å¯»æ‰¾æœ€ä½³çš„è¶…å‚æ•°ç»„åˆã€‚
  - `RandomizedSearchCV`é€šè¿‡éšæœºé‡‡æ ·è¶…å‚æ•°ç©ºé—´ä¸­çš„ä¸€äº›ç‚¹ï¼Œæ¥å¯»æ‰¾æœ€ä½³çš„è¶…å‚æ•°ç»„åˆã€‚
  - `HalvingGridSearchCV`é€šè¿‡è¿­ä»£åœ°å‰Šå‡æœç´¢ç©ºé—´æ¥åŠ é€Ÿç½‘æ ¼æœç´¢çš„è¿‡ç¨‹ï¼Œä»è€Œåœ¨æ›´çŸ­çš„æ—¶é—´å†…æ‰¾åˆ°æœ€ä½³çš„è¶…å‚æ•°ç»„åˆã€‚

### GridSearchCV

`GridSearchCV`æ˜¯`scikit-learn`ä¸­ç”¨äºè¿›è¡Œç½‘æ ¼æœç´¢çš„ç±»ã€‚å®ƒå¯ä»¥åœ¨ç»™å®šçš„å‚æ•°ç©ºé—´å†…è¿›è¡Œå…¨é¢æœç´¢ï¼Œæ‰¾åˆ°æœ€ä½³çš„è¶…å‚æ•°ç»„åˆï¼Œä»è€Œä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚

`GridSearchCV`çš„ä½¿ç”¨æ–¹æ³•æ¯”è¾ƒç®€å•ï¼Œåªéœ€è¦å®šä¹‰ä¸€ä¸ªè¶…å‚æ•°ç©ºé—´ï¼Œå¹¶åœ¨å…¶ä¸­æŒ‡å®šè¦æœç´¢çš„è¶…å‚æ•°åŠå…¶å–å€¼èŒƒå›´ã€‚ç„¶åï¼Œ`GridSearchCV`ä¼šåœ¨æ‰€æœ‰çš„è¶…å‚æ•°ç»„åˆä¸­è¿›è¡Œæœç´¢ï¼Œå¹¶è¿”å›æœ€ä½³çš„è¶…å‚æ•°ç»„åˆåŠå…¶å¯¹åº”çš„æ¨¡å‹æ€§èƒ½æŒ‡æ ‡ã€‚

#### eg

- ä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„`GridSearchCV`çš„ä¾‹å­ï¼š

- ```python
  from sklearn import svm, datasets
  from sklearn.model_selection import GridSearchCV
  
  # åŠ è½½æ•°æ®é›†
  iris = datasets.load_iris()
  X = iris.data
  y = iris.target
  
  # è®¾ç½®è¦ä¼˜åŒ–çš„è¶…å‚æ•°èŒƒå›´
  parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
  
  # åˆ›å»ºSVMåˆ†ç±»å™¨å¯¹è±¡
  svc = svm.SVC()
  
  # åˆ›å»ºGridSearchCVå¯¹è±¡ï¼Œå¹¶è®¾ç½®å‚æ•°
  clf = GridSearchCV(svc, parameters)
  
  # åœ¨è®­ç»ƒé›†ä¸Šæ‹ŸåˆGridSearchCVå¯¹è±¡
  clf.fit(X, y)
  
  # è¾“å‡ºæœ€ä½³çš„è¶…å‚æ•°ç»„åˆå’Œå¯¹åº”çš„è¯„åˆ†
  print("Best parameters set found on development set:")
  print(clf.best_params_)
  print("Best score:")
  print(clf.best_score_)
  ```

  - ```bash
    Best parameters set found on development set:
    {'C': 1, 'kernel': 'linear'}
    Best score:
    0.9800000000000001
    ```

    

- è¿™ä¸ªç¤ºä¾‹ä½¿ç”¨äº†GridSearchCVæ¥æœç´¢SVCæ¨¡å‹çš„æœ€ä½³è¶…å‚æ•°ç»„åˆã€‚é¦–å…ˆï¼ŒåŠ è½½irisæ•°æ®é›†ï¼Œå¹¶å°†ç‰¹å¾çŸ©é˜µå’Œæ ‡ç­¾å‘é‡åˆ†åˆ«å­˜å‚¨åœ¨Xå’Œyä¸­ã€‚ç„¶åï¼Œè®¾ç½®è¦ä¼˜åŒ–çš„è¶…å‚æ•°èŒƒå›´ï¼ŒåŒ…æ‹¬kernelå’ŒCä¸¤ä¸ªå‚æ•°ã€‚æ¥ç€ï¼Œåˆ›å»ºsvm.SVC()åˆ†ç±»å™¨å¯¹è±¡ï¼Œå¹¶å°†å…¶ä½œä¸ºå‚æ•°ä¼ é€’ç»™GridSearchCV()å‡½æ•°ï¼ŒåŒæ—¶å°†è¶…å‚æ•°èŒƒå›´parametersä¹Ÿä¼ é€’ç»™è¯¥å‡½æ•°ã€‚ç„¶åï¼Œè°ƒç”¨fit()æ–¹æ³•åœ¨è®­ç»ƒé›†ä¸Šæ‹ŸåˆGridSearchCVå¯¹è±¡ã€‚æœ€åï¼Œè¾“å‡ºæœ€ä½³çš„è¶…å‚æ•°ç»„åˆå’Œå¯¹åº”çš„è¯„åˆ†ã€‚

- éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œç½‘æ ¼æœç´¢æ˜¯ä¸€ç§å…¨é¢æœç´¢æ–¹æ³•ï¼Œå¯ä»¥ä¿è¯æ‰¾åˆ°å…¨å±€æœ€ä¼˜è§£ï¼Œä½†è®¡ç®—ä»£ä»·æ¯”è¾ƒé«˜ï¼Œå°¤å…¶æ˜¯åœ¨è¶…å‚æ•°ç©ºé—´è¾ƒå¤§çš„æƒ…å†µä¸‹ã€‚

### RandomizedSearchCV

- `RandomizedSearchCV`æ˜¯`scikit-learn`ä¸­ç”¨äºè¿›è¡Œéšæœºæœç´¢çš„ç±»ã€‚å®ƒå¯ä»¥åœ¨ç»™å®šçš„å‚æ•°ç©ºé—´å†…è¿›è¡Œéšæœºæœç´¢ï¼Œæ‰¾åˆ°æœ€ä½³çš„è¶…å‚æ•°ç»„åˆï¼Œä»è€Œä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚
- `RandomizedSearchCV`çš„ä½¿ç”¨æ–¹æ³•ä¸`GridSearchCV`ç±»ä¼¼ï¼Œåªæ˜¯å®ƒä¸æ˜¯åœ¨æ‰€æœ‰çš„è¶…å‚æ•°ç»„åˆä¸­è¿›è¡Œæœç´¢ï¼Œè€Œæ˜¯åœ¨æŒ‡å®šçš„è¶…å‚æ•°åˆ†å¸ƒä¸­è¿›è¡ŒæŠ½æ ·ã€‚è¿™æ ·å¯ä»¥å‡å°‘æœç´¢ç©ºé—´ï¼Œä»è€Œåœ¨æ›´çŸ­çš„æ—¶é—´å†…æ‰¾åˆ°æœ€ä½³çš„è¶…å‚æ•°ç»„åˆã€‚

#### eg

ä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„`RandomizedSearchCV`çš„ä¾‹å­ï¼š

- ```python
  
  from sklearn.datasets import load_iris
  from sklearn.linear_model import LogisticRegression
  from sklearn.model_selection import RandomizedSearchCV
  from scipy.stats import uniform
  iris = load_iris()
  logistic = LogisticRegression(solver='saga', tol=1e-2, max_iter=200,
                                random_state=0)
  distributions = dict(C=uniform(loc=0, scale=4),
                       penalty=['l2', 'l1'])
  clf = RandomizedSearchCV(logistic, distributions, random_state=0)
  search = clf.fit(iris.data, iris.target)
  print(search.best_params_)
  ```

  - ```bash
    {'C': 2.195254015709299, 'penalty': 'l1'}
    ```

- è¿™ä¸ªç¤ºä¾‹ä½¿ç”¨äº†`RandomizedSearchCV`æ¥æœç´¢`LogisticRegression`æ¨¡å‹çš„æœ€ä½³è¶…å‚æ•°ç»„åˆã€‚é¦–å…ˆï¼Œä½¿ç”¨`load_iris()`å‡½æ•°åŠ è½½`iris`æ•°æ®é›†ã€‚

- ç„¶åï¼Œåˆ›å»ºä¸€ä¸ª`LogisticRegression`åˆ†ç±»å™¨å¯¹è±¡`logistic`ï¼Œå¹¶è®¾ç½®å…¶è¶…å‚æ•°ï¼ŒåŒ…æ‹¬`solver`ã€`tol`å’Œ`max_iter`ç­‰ã€‚

- æ¥ç€ï¼Œè®¾ç½®è¦æœç´¢çš„è¶…å‚æ•°ç©ºé—´ï¼ŒåŒ…æ‹¬`C`å’Œ`penalty`ä¸¤ä¸ªå‚æ•°ï¼Œå…¶ä¸­`C`çš„åˆ†å¸ƒæ˜¯`uniform(loc=0, scale=4)`ï¼Œè¡¨ç¤ºåœ¨0åˆ°4ä¹‹é—´å‡åŒ€åˆ†å¸ƒï¼Œ`penalty`çš„å€¼æ˜¯`['l2', 'l1']`ä¸­çš„ä¸€ä¸ªã€‚

- ç„¶åï¼Œåˆ›å»º`RandomizedSearchCV`å¯¹è±¡`clf`ï¼Œå¹¶ä¼ å…¥`logistic`åˆ†ç±»å™¨å¯¹è±¡å’Œè¶…å‚æ•°ç©ºé—´`distributions`ã€‚è°ƒç”¨`fit()`æ–¹æ³•ï¼Œåœ¨`iris`æ•°æ®é›†ä¸Šæ‹Ÿåˆ`RandomizedSearchCV`å¯¹è±¡ï¼Œå¹¶è¿”å›ä¸€ä¸ª`search`å¯¹è±¡ã€‚æœ€åï¼Œè¾“å‡ºæœ€ä½³çš„è¶…å‚æ•°ç»„åˆã€‚

  è¿™ä¸ªä¾‹å­å±•ç¤ºäº†ä½¿ç”¨`RandomizedSearchCV`è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–çš„åŸºæœ¬ç”¨æ³•ï¼ŒåŒ…æ‹¬å¦‚ä½•è®¾ç½®è¶…å‚æ•°èŒƒå›´ã€åˆ›å»ºåˆ†ç±»å™¨å¯¹è±¡ã€æ‹Ÿåˆ`RandomizedSearchCV`å¯¹è±¡å’Œè¾“å‡ºæœ€ä½³çš„è¶…å‚æ•°ç»„åˆã€‚ä¸`GridSearchCV`ä¸åŒçš„æ˜¯ï¼Œ`RandomizedSearchCV`ä¸ä¼šå¯¹æ‰€æœ‰å¯èƒ½çš„è¶…å‚æ•°ç»„åˆè¿›è¡Œæœç´¢ï¼Œè€Œæ˜¯åœ¨è¶…å‚æ•°ç©ºé—´ä¸­éšæœºé‡‡æ ·ä¸€äº›ç‚¹è¿›è¡Œæœç´¢ï¼Œå¯ä»¥åœ¨å¤§æ•°æ®é›†ä¸Šæ›´åŠ é«˜æ•ˆã€‚

## Note

- In practice, you almost always want to [search over a pipeline](https://scikit-learn.org/stable/modules/grid_search.html#composite-grid-search), instead of a single estimator. One of the main reasons is that if you apply a pre-processing step to the whole dataset without using a pipeline, and then perform any kind of cross-validation, you would be breaking the fundamental assumption of independence between training and testing data.

-  Indeed, since you pre-processed the data using the whole dataset, some information about the test sets are available to the train sets. This will lead to over-estimating the generalization power of the estimator (you can read more in this [Kaggle post](https://www.kaggle.com/alexisbcook/data-leakage)).

- Using a pipeline for cross-validation and searching will largely keep you from this common pitfall.

- å®é™…ä¸Šï¼Œä½ å‡ ä¹æ€»æ˜¯æƒ³è¦åœ¨ä¸€ä¸ªpipelineä¸Šè¿›è¡Œæœç´¢ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå•ä¸€çš„ä¼°è®¡å™¨ã€‚å…¶ä¸­ä¸€ä¸ªä¸»è¦åŸå› æ˜¯ï¼Œå¦‚æœä½ åœ¨ä¸ä½¿ç”¨pipelineçš„æƒ…å†µä¸‹å¯¹æ•´ä¸ªæ•°æ®é›†åº”ç”¨é¢„å¤„ç†æ­¥éª¤ï¼Œç„¶åæ‰§è¡Œä»»ä½•å½¢å¼çš„äº¤å‰éªŒè¯ï¼Œä½ å°†è¿åè®­ç»ƒå’Œæµ‹è¯•æ•°æ®ä¹‹é—´ç‹¬ç«‹æ€§çš„åŸºæœ¬å‡è®¾ã€‚

  å®é™…ä¸Šï¼Œç”±äºä½ ä½¿ç”¨æ•´ä¸ªæ•°æ®é›†å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œå› æ­¤ä¸€äº›å…³äºæµ‹è¯•é›†çš„ä¿¡æ¯å¯ç”¨äºè®­ç»ƒé›†ã€‚è¿™å°†å¯¼è‡´é«˜ä¼°ä¼°è®¡å™¨çš„æ³›åŒ–èƒ½åŠ›ã€‚

- ä½¿ç”¨pipelineè¿›è¡Œäº¤å‰éªŒè¯å’Œæœç´¢å°†å¤§å¤§é¿å…è¿™ç§å¸¸è§çš„é—®é¢˜ã€‚ğŸˆ

### RandomForestRegressor

- `RandomForestRegressor`æ˜¯ä¸€ç§åŸºäºéšæœºæ£®æ—çš„å›å½’æ¨¡å‹ã€‚

- å®ƒæ˜¯ä¸€ç§é›†æˆå­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡ç»„åˆå¤šä¸ªå†³ç­–æ ‘æ¥æé«˜é¢„æµ‹çš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚æ¯ä¸ªå†³ç­–æ ‘éƒ½æ˜¯ç‹¬ç«‹çš„ï¼Œå¹¶ä¸”é‡‡ç”¨éšæœºé€‰æ‹©çš„æ ·æœ¬å’Œç‰¹å¾æ¥è¿›è¡Œè®­ç»ƒã€‚

- åœ¨é¢„æµ‹æ—¶ï¼Œéšæœºæ£®æ—å°†æ‰€æœ‰å†³ç­–æ ‘çš„é¢„æµ‹ç»“æœè¿›è¡Œå¹³å‡æˆ–æŠ•ç¥¨ï¼Œä»¥å¾—åˆ°æœ€ç»ˆçš„é¢„æµ‹ç»“æœã€‚

- è¯¥æ¨¡å‹å¯ä»¥ç”¨äºè§£å†³å›å½’é—®é¢˜ï¼Œä¾‹å¦‚é¢„æµ‹æˆ¿ä»·ã€è‚¡ç¥¨ä»·æ ¼ç­‰è¿ç»­å˜é‡çš„å€¼ã€‚

- `RandomForestRegressor`ç±»æ˜¯Scikit-learnä¸­å®ç°éšæœºæ£®æ—å›å½’æ¨¡å‹çš„ç±»ã€‚å®ƒä½¿ç”¨å¤šä¸ªå†³ç­–æ ‘æ¥æ‹Ÿåˆæ•°æ®ï¼Œå¹¶ä½¿ç”¨Baggingæ–¹æ³•ï¼ˆè‡ªåŠ©é‡‡æ ·ï¼‰æ¥å‡å°‘è¿‡æ‹Ÿåˆã€‚åœ¨è®­ç»ƒæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‡å®šå†³ç­–æ ‘çš„æ•°é‡ã€æ¯ä¸ªå†³ç­–æ ‘çš„æœ€å¤§æ·±åº¦ã€æ¯ä¸ªå†³ç­–æ ‘ä½¿ç”¨çš„ç‰¹å¾æ•°é‡ç­‰å‚æ•°ã€‚

- ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨`RandomForestRegressor`ç±»æ¥è®­ç»ƒå’Œé¢„æµ‹çš„ç®€å•ä¾‹å­ï¼š

- ```python
  from sklearn.datasets import load_diabetes
  from sklearn.ensemble import RandomForestRegressor
  from sklearn.model_selection import train_test_split
  from sklearn.metrics import mean_squared_error
  
  # load data
  diabetes = load_diabetes()
  X, y = diabetes.data, diabetes.target
  
  # split data into train and test sets
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
  
  # train the model
  model = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)
  model.fit(X_train, y_train)
  
  # make predictions on test set
  y_pred = model.predict(X_test)
  
  # evaluate the model
  mse = mean_squared_error(y_test, y_pred)
  print("Mean Squared Error:", mse)
  ```

  - åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆåŠ è½½ç³–å°¿ç—…æ•°æ®é›†ï¼Œå¹¶å°†æ•°æ®é›†æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚æ¥ç€ï¼Œæˆ‘ä»¬ä½¿ç”¨`RandomForestRegressor`ç±»æ¥è®­ç»ƒæ¨¡å‹ã€‚åœ¨è®­ç»ƒæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬æŒ‡å®šäº†100ä¸ªå†³ç­–æ ‘ã€æ¯ä¸ªå†³ç­–æ ‘çš„æœ€å¤§æ·±åº¦ä¸º5ã€ä½¿ç”¨éšæœºç§å­ä¸º42ã€‚
  - æœ€åï¼Œæˆ‘ä»¬åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹ï¼Œå¹¶ä½¿ç”¨å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰æ¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚


### MSE

- å‡æ–¹è¯¯å·®ï¼ˆMean Squared Errorï¼ŒMSEï¼‰æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å›å½’æ¨¡å‹æ€§èƒ½çš„æŒ‡æ ‡ã€‚å®ƒåº¦é‡äº†æ¨¡å‹é¢„æµ‹å€¼å’ŒçœŸå®å€¼ä¹‹é—´çš„å¹³å‡å·®çš„å¹³æ–¹ã€‚

- å…·ä½“åœ°ï¼Œè®¾é¢„æµ‹å€¼ä¸º $\hat{y}_i$ï¼ŒçœŸå®å€¼ä¸º $y_i$ï¼Œæ ·æœ¬æ•°é‡ä¸º $n$ï¼Œåˆ™ MSE çš„è®¡ç®—å…¬å¼ä¸ºï¼š

  $$MSE = \frac{1}{n}\sum_{i=1}^{n}(\hat{y}_i - y_i)^2$$

  MSEè¶Šå°ï¼Œè¡¨ç¤ºé¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„å·®å¼‚è¶Šå°ï¼Œæ¨¡å‹çš„æ€§èƒ½è¶Šå¥½ã€‚(åœ¨æ·±åº¦å­¦ä¹ ä½œä¸ºæŸå¤±å‡½æ•°çš„ä¸€ç§)

  MSE çš„å€¼å—åˆ°ç¦»ç¾¤å€¼çš„å½±å“è¾ƒå¤§ï¼Œå› ä¸ºå®ƒè®¡ç®—çš„æ˜¯å·®å¼‚çš„å¹³æ–¹ï¼Œè€Œç¦»ç¾¤å€¼çš„å·®å¼‚å¾€å¾€æ¯”è¾ƒå¤§ã€‚

  MSE ä¸å‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰æœ‰å¯†åˆ‡å…³ç³»ï¼ŒRMSE æ˜¯ MSE çš„å¹³æ–¹æ ¹ï¼Œç”¨äºé‡åŒ–é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„å¹³å‡å·®å¼‚ã€‚RMSE çš„å•ä½ä¸é¢„æµ‹å€¼å’ŒçœŸå®å€¼çš„å•ä½ç›¸åŒï¼Œå› æ­¤æ›´åŠ ç›´è§‚ã€‚

## pipelineäº¤å‰éªŒè¯ğŸˆ

- [3.1. Cross-validation: evaluating estimator performance â€” scikit-learn 1.2.2 documentation](https://scikit-learn.org/stable/modules/cross_validation.html)

  - Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called overfitting. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a test set X_test, y_test. Note that the word â€œexperimentâ€ is not intended to denote academic use only, because even in commercial settings machine learning usually starts out experimentally. Here is a flowchart of typical cross validation workflow in model training. The best parameters can be determined by grid search techniques.

    åœ¨å­¦ä¹ é¢„æµ‹å‡½æ•°çš„å‚æ•°å¹¶åœ¨åŒä¸€æ•°æ®ä¸Šè¿›è¡Œæµ‹è¯•æ˜¯ä¸€ç§æ–¹æ³•è®ºä¸Šçš„é”™è¯¯ï¼šä¸€ä¸ªåªä¼šé‡å¤å…¶åˆšåˆšçœ‹åˆ°çš„æ ·æœ¬æ ‡ç­¾çš„æ¨¡å‹ä¼šå¾—åˆ°å®Œç¾çš„åˆ†æ•°ï¼Œä½†åœ¨å°šæœªè§è¿‡çš„æ•°æ®ä¸Šå°†æ— æ³•é¢„æµ‹ä»»ä½•æœ‰ç”¨çš„ä¿¡æ¯ï¼Œè¿™ç§æƒ…å†µç§°ä¸ºè¿‡åº¦æ‹Ÿåˆã€‚

    ä¸ºäº†é¿å…è¿™ç§æƒ…å†µï¼Œåœ¨è¿›è¡Œ(ç›‘ç£)æœºå™¨å­¦ä¹ å®éªŒæ—¶ï¼Œé€šå¸¸å°†å¯ç”¨æ•°æ®çš„ä¸€éƒ¨åˆ†ä¿ç•™ä¸ºæµ‹è¯•é›†X_testï¼Œy_testã€‚è¯·æ³¨æ„ï¼Œâ€œå®éªŒâ€è¿™ä¸ªè¯ä¸ä»…ç”¨äºå­¦æœ¯ç”¨é€”ï¼Œå› ä¸ºå³ä½¿åœ¨å•†ä¸šç¯å¢ƒä¸­ï¼Œæœºå™¨å­¦ä¹ é€šå¸¸ä¹Ÿæ˜¯ä»å®éªŒå¼€å§‹çš„ã€‚ä¸‹é¢æ˜¯æ¨¡å‹è®­ç»ƒä¸­å…¸å‹äº¤å‰éªŒè¯å·¥ä½œæµç¨‹çš„æµç¨‹å›¾ã€‚æœ€ä½³å‚æ•°å¯ä»¥é€šè¿‡ç½‘æ ¼æœç´¢æŠ€æœ¯ç¡®å®šã€‚

    - ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/47bca69a22454e4795b6867754e516c9.png)

- å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«äº†æˆ¿å±‹çš„é¢ç§¯ã€æˆ¿é—´æ•°é‡å’Œä»·æ ¼ç­‰ç‰¹å¾ã€‚æˆ‘ä»¬æƒ³è¦ä½¿ç”¨æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰æ¥é¢„æµ‹æˆ¿å±‹ä»·æ ¼ã€‚åœ¨ä½¿ç”¨SVMä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œä¾‹å¦‚å°†ç‰¹å¾ç¼©æ”¾åˆ°ç›¸åŒçš„å°ºåº¦ä¸Šã€‚

- å¦‚æœæˆ‘ä»¬ä¸ä½¿ç”¨pipelineï¼Œè€Œæ˜¯å…ˆ**å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œç¼©æ”¾**ï¼Œç„¶åå°†ç¼©æ”¾åçš„æ•°æ®é›†æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå¹¶å¯¹å®ƒä»¬è¿›è¡Œäº¤å‰éªŒè¯å’Œæœç´¢ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°†ä¼šå­˜åœ¨æ•°æ®æ³„æ¼ï¼ˆdata leakageï¼‰çš„é—®é¢˜ã€‚
  - è¿™æ˜¯å› ä¸ºï¼Œæˆ‘ä»¬åœ¨å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œç¼©æ”¾æ—¶ï¼Œå·²ç»ä½¿ç”¨äº†æµ‹è¯•é›†ä¸­çš„ä¿¡æ¯ï¼Œä»è€Œä½¿å¾—è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¹‹é—´ä¸å†æ˜¯ç‹¬ç«‹çš„ã€‚

  - å…·ä½“æ¥è¯´ï¼Œå¦‚æœæˆ‘ä»¬åœ¨å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œç¼©æ”¾åå†å¯¹å…¶è¿›è¡Œæ‹†åˆ†ï¼Œé‚£ä¹ˆè®­ç»ƒé›†ä¸­çš„æŸäº›æ•°æ®å¯èƒ½å·²ç»â€œçŸ¥é“â€äº†æµ‹è¯•é›†ä¸­çš„ä¸€äº›ä¿¡æ¯ï¼Œè¿™å°†å¯¼è‡´æˆ‘ä»¬é«˜ä¼°SVMçš„æ€§èƒ½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨pipelineæ¥ç¡®ä¿åœ¨äº¤å‰éªŒè¯å’Œæœç´¢è¿‡ç¨‹ä¸­ï¼Œé¢„å¤„ç†æ­¥éª¤ä»…ä½¿ç”¨è®­ç»ƒé›†ä¸­çš„ä¿¡æ¯ï¼Œè€Œä¸æ¶‰åŠæµ‹è¯•é›†ã€‚


### eg

ä½¿ç”¨pipelineçš„ä¾‹å­å¦‚ä¸‹ï¼š

```python

from sklearn.datasets import make_classification 
from sklearn.model_selection import train_test_split
# ç”ŸæˆäºŒåˆ†ç±»æ•°æ®é›† 
X, y = make_classification(n_samples=500, n_features=5,n_informative=3, n_classes=3, random_state=42) 
X_train,X_test,y_train,y_test=train_test_split(X,y)
# æŸ¥çœ‹æ•°æ®é›†çš„å½¢çŠ¶å’Œæ ‡ç­¾åˆ†å¸ƒ 
print(X.shape) 
print(y[:10])
##

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVR
from sklearn.model_selection import GridSearchCV

# create a pipeline with scaling and SVM
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('svm', SVR())
])

# define the parameter space for grid search
param_grid = {
    # 'svm__kernel':['linear','rbf'],
    'svm__C': [0.1, 1, 10],
    'svm__gamma': [0.01, 0.1,1],
}

# create a grid search object and fit to the data
grid_search = GridSearchCV(pipeline, param_grid, cv=5,verbose=3)
grid_search.fit(X_train, y_train)

# evaluate the best model on the test set
grid_search.score(X_test, y_test)
```

- ```bash
  ....
  [CV 2/5] END ...........svm__C=10, svm__gamma=1;, score=0.577 total time=   0.0s
  [CV 3/5] END ...........svm__C=10, svm__gamma=1;, score=0.445 total time=   0.0s
  [CV 4/5] END ...........svm__C=10, svm__gamma=1;, score=0.624 total time=   0.0s
  [CV 5/5] END ...........svm__C=10, svm__gamma=1;, score=0.670 total time=   0.0s
  
  0.7792794616124854
  ```

  

- åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªpipelineå¯¹è±¡ï¼Œå…¶ä¸­åŒ…å«äº†ä¸€ä¸ªStandardScalerå¯¹è±¡å’Œä¸€ä¸ªSVRå¯¹è±¡ã€‚StandardScalerå¯¹è±¡ç”¨äºå¯¹æ•°æ®è¿›è¡Œç¼©æ”¾ï¼Œè€ŒSVRå¯¹è±¡ç”¨äºè¿›è¡Œæ”¯æŒå‘é‡æœºå›å½’ã€‚

- ç„¶åï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªå‚æ•°ç©ºé—´param_gridï¼Œå…¶ä¸­åŒ…å«äº†SVMçš„ä¸¤ä¸ªè¶…å‚æ•°Cå’Œgammaçš„å–å€¼èŒƒå›´ã€‚æ¥ç€ï¼Œæˆ‘ä»¬ä½¿ç”¨GridSearchCVè¿›è¡Œäº¤å‰éªŒè¯å’Œæœç´¢ï¼Œå°†pipelineä½œä¸ºä¼°è®¡å™¨å¯¹è±¡ï¼Œå¹¶ä¼ å…¥å‚æ•°ç©ºé—´param_gridã€‚

- æœ€åï¼Œæˆ‘ä»¬åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°äº†æœ€ä½³æ¨¡å‹çš„æ€§èƒ½ã€‚

- éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†Pipelineå¯¹è±¡æ¥å°†å¤šä¸ªæ­¥éª¤ç»„åˆåœ¨ä¸€èµ·ï¼Œå¹¶ç¡®ä¿åœ¨äº¤å‰éªŒè¯å’Œæœç´¢è¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ªæ­¥éª¤ä»…ä½¿ç”¨è®­ç»ƒé›†ä¸­çš„ä¿¡æ¯ï¼Œè€Œä¸æ¶‰åŠæµ‹è¯•é›†ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å¯ä»¥é¿å…æ•°æ®æ³„æ¼é—®é¢˜ï¼Œå¹¶è·å¾—æ›´å‡†ç¡®çš„æ¨¡å‹è¯„ä¼°ç»“æœã€‚

## L1@L2æ­£åˆ™

- L1æ­£åˆ™å’ŒL2æ­£åˆ™æ˜¯æ­£åˆ™åŒ–æ–¹æ³•ä¸­æœ€å¸¸ç”¨çš„ä¸¤ç§æ–¹æ³•ã€‚

- L1æ­£åˆ™åŒ–ï¼ˆä¹Ÿç§°ä¸º[Lasso|**least absolute shrinkage and selection operator**; also **Lasso** or **LASSO**](https://en.wikipedia.org/wiki/Lasso_(statistics))ï¼‰æ˜¯ä¸€ç§çº¿æ€§å›å½’çš„æ­£åˆ™åŒ–æ–¹æ³•ï¼Œåœ¨æŸå¤±å‡½æ•°ä¸­åŠ å…¥L1èŒƒæ•°æƒ©ç½šé¡¹ï¼Œä½¿å¾—æ¨¡å‹ä¸­çš„ä¸€äº›ç³»æ•°å˜ä¸º0ï¼Œä»è€Œå®ç°ç‰¹å¾é€‰æ‹©çš„ä½œç”¨ã€‚L1æ­£åˆ™åŒ–å¯ä»¥ç”¨äºç‰¹å¾é€‰æ‹©ï¼Œå› ä¸ºå®ƒå¯ä»¥å°†ä¸€äº›ä¸é‡è¦æˆ–å†—ä½™çš„ç‰¹å¾ç³»æ•°ç¼©å°ç”šè‡³ç½®0ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

  L2æ­£åˆ™åŒ–ï¼ˆä¹Ÿç§°ä¸º[Ridge](https://en.wikipedia.org/wiki/Ridge_regression)ï¼‰æ˜¯ä¸€ç§çº¿æ€§å›å½’çš„æ­£åˆ™åŒ–æ–¹æ³•ï¼Œåœ¨æŸå¤±å‡½æ•°ä¸­åŠ å…¥L2èŒƒæ•°æƒ©ç½šé¡¹ï¼Œä½¿å¾—æ¨¡å‹ä¸­çš„ç³»æ•°å˜å¾—æ›´å°ï¼Œä»è€Œé˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆã€‚L2æ­£åˆ™åŒ–å¯ä»¥ç”¨äºå¤„ç†é«˜å…±çº¿æ€§æ•°æ®ï¼Œå› ä¸ºå®ƒå¯ä»¥é€šè¿‡å‡å°ç³»æ•°çš„å¤§å°æ¥ç¼©å°å…±çº¿æ€§çš„å½±å“ã€‚

- L1æ­£åˆ™åŒ–å’ŒL2æ­£åˆ™åŒ–çš„åŒºåˆ«åœ¨äºæƒ©ç½šé¡¹çš„å½¢å¼ä¸åŒã€‚L1æ­£åˆ™åŒ–çš„æƒ©ç½šé¡¹æ˜¯ç³»æ•°å‘é‡ä¸­å„ä¸ªç³»æ•°çš„ç»å¯¹å€¼ä¹‹å’Œï¼Œè€ŒL2æ­£åˆ™åŒ–çš„æƒ©ç½šé¡¹æ˜¯ç³»æ•°å‘é‡ä¸­å„ä¸ªç³»æ•°çš„å¹³æ–¹å’Œã€‚

- å› æ­¤ï¼ŒL1æ­£åˆ™åŒ–æ›´å€¾å‘äºäº§ç”Ÿç¨€ç–è§£ï¼Œå³ä¸€äº›ç³»æ•°ä¸º0ï¼Œè€ŒL2æ­£åˆ™åŒ–æ›´å€¾å‘äºäº§ç”Ÿç³»æ•°è¾ƒå°çš„è§£ï¼Œå¯ä»¥é¿å…è¿‡æ‹Ÿåˆã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œé€šå¸¸éœ€è¦æ ¹æ®å…·ä½“çš„é—®é¢˜å’Œæ•°æ®é›†é€‰æ‹©åˆé€‚çš„æ­£åˆ™åŒ–æ–¹æ³•ã€‚

- å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªåˆ†ç±»é—®é¢˜ï¼Œéœ€è¦å»ºç«‹ä¸€ä¸ªæ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰æ¨¡å‹æ¥åˆ†ç±»ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨L1æ­£åˆ™åŒ–æˆ–L2æ­£åˆ™åŒ–æ¥è®­ç»ƒæ¨¡å‹ï¼Œå¹¶æ¯”è¾ƒå®ƒä»¬çš„æ•ˆæœã€‚

  é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨L1æ­£åˆ™åŒ–è®­ç»ƒæ¨¡å‹ã€‚åœ¨æŸå¤±å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬åŠ å…¥L1èŒƒæ•°æƒ©ç½šé¡¹ï¼Œä½¿å¾—æ¨¡å‹ä¸­çš„ä¸€äº›ç‰¹å¾ç³»æ•°å˜ä¸º0ï¼Œä»è€Œå®ç°ç‰¹å¾é€‰æ‹©çš„ä½œç”¨ã€‚è¿™æ„å‘³ç€ï¼Œä¸€äº›ç‰¹å¾å°†è¢«å®Œå…¨å¿½ç•¥ï¼Œå¹¶ä»æ¨¡å‹ä¸­å‰”é™¤ã€‚è¿™å¯ä»¥é¿å…ç‰¹å¾ä¹‹é—´çš„å…±çº¿æ€§ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

  ```python
  from sklearn.datasets import load_iris
  from sklearn.svm import LinearSVC
  
  # åŠ è½½æ•°æ®é›†
  iris = load_iris()
  
  # åˆ›å»ºL1æ­£åˆ™åŒ–SVMæ¨¡å‹å¯¹è±¡
  l1_svm = LinearSVC(penalty='l1', dual=False,max_iter=3000)
  
  # åœ¨æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹
  l1_svm.fit(iris.data, iris.target)
  
  # è¾“å‡ºæ¨¡å‹ç³»æ•°
  print(l1_svm.coef_)
  ```

  åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨`LinearSVC`æ¨¡å‹å¯¹è±¡æ¥è®­ç»ƒæ¨¡å‹ï¼Œå¹¶å°†`penalty`å‚æ•°è®¾ç½®ä¸º'l1'ï¼Œè¿™æ˜¯L1æ­£åˆ™åŒ–çš„è¶…å‚æ•°ã€‚`fit()`æ–¹æ³•å°†æ¨¡å‹æ‹Ÿåˆåˆ°æ•°æ®é›†ä¸Šï¼Œå¹¶è¿”å›æ¨¡å‹ç³»æ•°ã€‚è¾“å‡ºçš„ç³»æ•°å‘é‡ä¸­ï¼Œä¸€äº›ç³»æ•°ä¸º0ï¼Œè¿™æ„å‘³ç€å®ƒä»¬å¯¹æ¨¡å‹çš„è´¡çŒ®å¾ˆå°ï¼Œè¢«å®Œå…¨å¿½ç•¥ã€‚

  æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä½¿ç”¨L2æ­£åˆ™åŒ–è®­ç»ƒæ¨¡å‹ã€‚åœ¨æŸå¤±å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬åŠ å…¥L2èŒƒæ•°æƒ©ç½šé¡¹ï¼Œä½¿å¾—æ¨¡å‹ä¸­çš„ç³»æ•°å˜å¾—æ›´å°ï¼Œä»è€Œé˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆã€‚è¿™æ„å‘³ç€ï¼Œæ‰€æœ‰ç‰¹å¾éƒ½è¢«ä¿ç•™ï¼Œä½†å®ƒä»¬çš„ç³»æ•°è¢«ç¼©å°ï¼Œä»¥é¿å…è¿‡åº¦æ‹Ÿåˆã€‚

  ```python
  # åˆ›å»ºL2æ­£åˆ™åŒ–SVMæ¨¡å‹å¯¹è±¡
  l2_svm = LinearSVC(penalty='l2', dual=False)
  
  # åœ¨æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹
  l2_svm.fit(iris.data, iris.target)
  
  # è¾“å‡ºæ¨¡å‹ç³»æ•°
  print(l2_svm.coef_)
  ```

  åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨`LinearSVC`æ¨¡å‹å¯¹è±¡æ¥è®­ç»ƒæ¨¡å‹ï¼Œå¹¶å°†`penalty`å‚æ•°è®¾ç½®ä¸º'l2'ï¼Œè¿™æ˜¯L2æ­£åˆ™åŒ–çš„è¶…å‚æ•°ã€‚`fit()`æ–¹æ³•å°†æ¨¡å‹æ‹Ÿåˆåˆ°æ•°æ®é›†ä¸Šï¼Œå¹¶è¿”å›æ¨¡å‹ç³»æ•°ã€‚è¾“å‡ºçš„ç³»æ•°å‘é‡ä¸­ï¼Œæ‰€æœ‰ç³»æ•°éƒ½è¢«ä¿ç•™ï¼Œä½†å®ƒä»¬çš„å¤§å°è¢«ç¼©å°ã€‚

  å¯ä»¥çœ‹åˆ°ï¼Œåœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼ŒL1æ­£åˆ™åŒ–å’ŒL2æ­£åˆ™åŒ–çš„æ•ˆæœæ˜¯ä¸åŒçš„ã€‚L1æ­£åˆ™åŒ–é€šè¿‡ç‰¹å¾é€‰æ‹©å»é™¤äº†ä¸€äº›ç‰¹å¾ï¼Œè€ŒL2æ­£åˆ™åŒ–åˆ™ä¿ç•™äº†æ‰€æœ‰ç‰¹å¾ï¼Œä½†ç¼©å°äº†å®ƒä»¬çš„ç³»æ•°ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œéœ€è¦æ ¹æ®å…·ä½“çš„é—®é¢˜å’Œæ•°æ®é›†é€‰æ‹©åˆé€‚çš„æ­£åˆ™åŒ–æ–¹æ³•ã€‚

## Next steps

- We have briefly covered estimator fitting and predicting, pre-processing steps, pipelines, cross-validation tools and automatic hyper-parameter searches. This guide should give you an overview of some of the main features of the library, but there is much more to `scikit-learn`!

- Please refer to our [User Guide](https://scikit-learn.org/stable/user_guide.html#user-guide) for details on all the tools that we provide. You can also find an exhaustive list of the public API in the [API Reference](https://scikit-learn.org/stable/modules/classes.html#api-ref).

- You can also look at our numerous [examples](https://scikit-learn.org/stable/auto_examples/index.html#general-examples) that illustrate the use of `scikit-learn` in many different contexts.

- The [tutorials](https://scikit-learn.org/stable/tutorial/index.html#tutorial-menu) also contain additional learning resources.

### User Guide vs Tutorial

- `scikit-learn`æä¾›äº†ç”¨æˆ·æŒ‡å—ï¼ˆUser Guideï¼‰å’Œæ•™ç¨‹ï¼ˆTutorialï¼‰ä¸¤ç§ä¸åŒç±»å‹çš„æ–‡æ¡£ï¼Œå®ƒä»¬çš„ç›®çš„å’Œç”¨é€”ç•¥æœ‰ä¸åŒã€‚

  - [User guide: contents â€” scikit-learn documentation](https://scikit-learn.org/stable/user_guide.html)

  ç”¨æˆ·æŒ‡å—ï¼ˆUser Guideï¼‰æ˜¯ä¸€ä»½è¯¦ç»†çš„æ–‡æ¡£ï¼Œå®ƒä»¥ä»»åŠ¡ä¸ºå¯¼å‘ï¼Œä»‹ç»äº†`scikit-learn`ä¸­æä¾›çš„å„ç§æœºå™¨å­¦ä¹ ç®—æ³•å’Œå·¥å…·çš„ç”¨æ³•ã€åŸç†ã€å‚æ•°è®¾ç½®ã€ä¼˜åŒ–æŠ€å·§ç­‰æ–¹é¢çš„å†…å®¹ã€‚ç”¨æˆ·æŒ‡å—çš„ä¸»è¦å—ä¼—æ˜¯é‚£äº›å·²ç»å¯¹æœºå™¨å­¦ä¹ æœ‰ä¸€å®šäº†è§£ï¼Œå¹¶ä¸”æƒ³è¦ä½¿ç”¨`scikit-learn`æ„å»ºå¤æ‚çš„æœºå™¨å­¦ä¹ ç³»ç»Ÿçš„ç”¨æˆ·ã€‚ç”¨æˆ·æŒ‡å—çš„ç›®çš„æ˜¯å¸®åŠ©ç”¨æˆ·æ·±å…¥ç†è§£`scikit-learn`çš„åŠŸèƒ½å’Œè®¾è®¡ï¼ŒæŒæ¡ä½¿ç”¨`scikit-learn`æ„å»ºæœºå™¨å­¦ä¹ ç³»ç»Ÿçš„æ–¹æ³•å’ŒæŠ€å·§ã€‚

  æ•™ç¨‹ï¼ˆTutorialï¼‰åˆ™æ˜¯ä¸€ä»½æ›´ä¸ºç®€å•å’Œæ˜“äºå…¥é—¨çš„æ–‡æ¡£ï¼Œå®ƒä¸»è¦æ˜¯ä¸ºé‚£äº›æ–°æ‰‹ç”¨æˆ·å‡†å¤‡çš„ã€‚æ•™ç¨‹ä»åŸºç¡€å¼€å§‹ï¼Œä»‹ç»äº†`scikit-learn`çš„æ ¸å¿ƒæ¦‚å¿µã€æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹é€‰æ‹©ã€æ¨¡å‹è¯„ä¼°ç­‰æ–¹é¢çš„å†…å®¹ï¼Œå¹¶æä¾›äº†ä¸€äº›ç¤ºä¾‹ä»£ç å’Œç»ƒä¹ é¢˜ï¼Œå¸®åŠ©ç”¨æˆ·é€æ­¥æŒæ¡`scikit-learn`çš„ä½¿ç”¨æ–¹æ³•ã€‚æ•™ç¨‹çš„ç›®çš„æ˜¯å¸®åŠ©ç”¨æˆ·å¿«é€Ÿå…¥é—¨`scikit-learn`ï¼Œäº†è§£åŸºæœ¬çš„æœºå™¨å­¦ä¹ æµç¨‹å’Œå·¥å…·ï¼Œä¸ºè¿›ä¸€æ­¥æ·±å…¥å­¦ä¹ å’Œå®è·µæ‰“ä¸‹åŸºç¡€ã€‚

  æ€»ä¹‹ï¼Œç”¨æˆ·æŒ‡å—é€‚åˆé‚£äº›å¸Œæœ›æ·±å…¥ç†è§£`scikit-learn`çš„æœºåˆ¶ã€ç®—æ³•å’Œå·¥å…·çš„ç”¨æˆ·ï¼Œè€Œæ•™ç¨‹åˆ™é€‚åˆé‚£äº›åˆšå¼€å§‹æ¥è§¦æœºå™¨å­¦ä¹ å’Œ`scikit-learn`çš„æ–°æ‰‹ç”¨æˆ·ã€‚



