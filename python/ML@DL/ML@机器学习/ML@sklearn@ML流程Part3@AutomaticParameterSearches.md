[toc]

## Automatic parameter searches

- All estimators have parameters (often called hyper-parameters in the literature) that can be tuned. 
- The generalization power of an estimator often critically depends on a few parameters. For example a [`RandomForestRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor) has a `n_estimators` parameter that determines the number of trees in the forest, and a `max_depth` parameter that determines the maximum depth of each tree. 
- Quite often, it is not clear what the exact values of these parameters should be since they depend on the data at hand.

- `Scikit-learn` provides tools to automatically **find the best parameter combinations** (via cross-validation). 
- In the following example, we randomly search over the parameter space of a random forest with a [`RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV) object. 
- When the search is over, the [`RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV) behaves as a [`RandomForestRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor) that has been **fitted with the best set of parameters**. Read more in the [User Guide](https://scikit-learn.org/stable/modules/grid_search.html#grid-search):

### eg

```python
>>> from sklearn.datasets import fetch_california_housing
>>> from sklearn.ensemble import RandomForestRegressor
>>> from sklearn.model_selection import RandomizedSearchCV
>>> from sklearn.model_selection import train_test_split
>>> from scipy.stats import randint
...
>>> X, y = fetch_california_housing(return_X_y=True)
>>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
...
>>> # define the parameter space that will be searched over
>>> param_distributions = {'n_estimators': randint(1, 5),
...                        'max_depth': randint(5, 10)}
...
>>> # now create a searchCV object and fit it to the data
>>> search = RandomizedSearchCV(estimator=RandomForestRegressor(random_state=0),
...                             n_iter=5,
...                             param_distributions=param_distributions,
...                             random_state=0)
>>> search.fit(X_train, y_train)
RandomizedSearchCV(estimator=RandomForestRegressor(random_state=0), n_iter=5,
                   param_distributions={'max_depth': ...,
                                        'n_estimators': ...},
                   random_state=0)
>>> search.best_params_
{'max_depth': 9, 'n_estimators': 4}

>>> # the search object now acts like a normal random forest estimator
>>> # with max_depth=9 and n_estimators=4
>>> search.score(X_test, y_test)
0.73...
```

- è¿™æ®µä»£ç ä½¿ç”¨äº†Scikit-learnåº“ä¸­çš„éšæœºæœç´¢ï¼ˆRandomized Searchï¼‰æ–¹æ³•æ¥å¯»æ‰¾æœ€ä½³çš„éšæœºæ£®æ—æ¨¡å‹è¶…å‚æ•°ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥ä»£ç æ‰§è¡Œäº†ä»¥ä¸‹æ­¥éª¤ï¼š

  1. ä»Scikit-learnåº“ä¸­å¯¼å…¥äº†California Housingæ•°æ®é›†ï¼Œéšæœºæ£®æ—å›å½’å™¨ï¼ˆRandomForestRegressorï¼‰ï¼Œéšæœºæœç´¢äº¤å‰éªŒè¯ï¼ˆRandomizedSearchCVï¼‰å’Œéšæœºæ•´æ•°ç”Ÿæˆå‡½æ•°ï¼ˆrandintï¼‰ç­‰å¿…è¦çš„æ¨¡å—ã€‚
  2. ä½¿ç”¨`fetch_california_housing()`å‡½æ•°ä»California Housingæ•°æ®é›†ä¸­è·å–æ•°æ®Xå’Œç›®æ ‡yï¼Œå¹¶å°†å®ƒä»¬åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚
  3. å®šä¹‰äº†ä¸€ä¸ªè¶…å‚æ•°ç©ºé—´`param_distributions`ï¼ŒåŒ…å«äº†`n_estimators`å’Œ`max_depth`ä¸¤ä¸ªè¶…å‚æ•°ã€‚å…¶ä¸­ï¼Œ`n_estimators`è¡¨ç¤ºéšæœºæ£®æ—æ¨¡å‹ä¸­æ ‘çš„æ•°é‡ï¼Œå–å€¼èŒƒå›´ä¸º1åˆ°5ä¹‹é—´çš„éšæœºæ•´æ•°ï¼›`max_depth`è¡¨ç¤ºæ¯æ£µæ ‘çš„æœ€å¤§æ·±åº¦ï¼Œå–å€¼èŒƒå›´ä¸º5åˆ°10ä¹‹é—´çš„éšæœºæ•´æ•°ã€‚
  4. åˆ›å»ºäº†ä¸€ä¸ªéšæœºæœç´¢äº¤å‰éªŒè¯å¯¹è±¡`search`ï¼Œå¹¶æŒ‡å®šäº†å®ƒçš„å‚æ•°ï¼ŒåŒ…æ‹¬è¦æœç´¢çš„è¶…å‚æ•°ç©ºé—´ã€è¦æœç´¢çš„è¿­ä»£æ¬¡æ•°ï¼ˆn_iterï¼‰ç­‰ã€‚
  5. è°ƒç”¨`fit()`å‡½æ•°å¯¹`search`å¯¹è±¡è¿›è¡Œæ‹Ÿåˆï¼Œä½¿å…¶ä»è®­ç»ƒé›†ä¸­å¯»æ‰¾æœ€ä½³çš„è¶…å‚æ•°ç»„åˆã€‚
  6. è¾“å‡º`search`å¯¹è±¡çš„æœ€ä½³è¶…å‚æ•°ç»„åˆ`search.best_params_`ã€‚
  7. æœ€åï¼Œä½¿ç”¨`search`å¯¹è±¡çš„`score()`å‡½æ•°åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶è¾“å‡ºæ¨¡å‹çš„å¾—åˆ†ã€‚

  éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œéšæœºæœç´¢æ–¹æ³•åœ¨ç»™å®šçš„è¶…å‚æ•°ç©ºé—´ä¸­éšæœºæŠ½æ ·ä¸€ç»„è¶…å‚æ•°è¿›è¡Œè®­ç»ƒï¼Œå¹¶è®¡ç®—å…¶æ€§èƒ½ã€‚å› æ­¤ï¼Œæ¯æ¬¡è¿è¡Œè¯¥ä»£ç å¯èƒ½ä¼šå¾—åˆ°ä¸åŒçš„æœ€ä½³è¶…å‚æ•°ç»„åˆå’Œæ¨¡å‹å¾—åˆ†ã€‚

- In practice, you almost always want to [search over a pipeline](https://scikit-learn.org/stable/modules/grid_search.html#composite-grid-search), instead of a single estimator. One of the main reasons is that if you apply a pre-processing step to the whole dataset without using a pipeline, and then perform any kind of cross-validation, you would be breaking the fundamental assumption of independence between training and testing data.

-  Indeed, since you pre-processed the data using the whole dataset, some information about the test sets are available to the train sets. This will lead to over-estimating the generalization power of the estimator (you can read more in this [Kaggle post](https://www.kaggle.com/alexisbcook/data-leakage)).

- Using a pipeline for cross-validation and searching will largely keep you from this common pitfall.

- å®é™…ä¸Šï¼Œä½ å‡ ä¹æ€»æ˜¯æƒ³è¦åœ¨ä¸€ä¸ªpipelineä¸Šè¿›è¡Œæœç´¢ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå•ä¸€çš„ä¼°è®¡å™¨ã€‚å…¶ä¸­ä¸€ä¸ªä¸»è¦åŸå› æ˜¯ï¼Œå¦‚æœä½ åœ¨ä¸ä½¿ç”¨pipelineçš„æƒ…å†µä¸‹å¯¹æ•´ä¸ªæ•°æ®é›†åº”ç”¨é¢„å¤„ç†æ­¥éª¤ï¼Œç„¶åæ‰§è¡Œä»»ä½•å½¢å¼çš„äº¤å‰éªŒè¯ï¼Œä½ å°†è¿åè®­ç»ƒå’Œæµ‹è¯•æ•°æ®ä¹‹é—´ç‹¬ç«‹æ€§çš„åŸºæœ¬å‡è®¾ã€‚

  å®é™…ä¸Šï¼Œç”±äºä½ ä½¿ç”¨æ•´ä¸ªæ•°æ®é›†å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œå› æ­¤ä¸€äº›å…³äºæµ‹è¯•é›†çš„ä¿¡æ¯å¯ç”¨äºè®­ç»ƒé›†ã€‚è¿™å°†å¯¼è‡´é«˜ä¼°ä¼°è®¡å™¨çš„æ³›åŒ–èƒ½åŠ›ã€‚

- ä½¿ç”¨pipelineè¿›è¡Œäº¤å‰éªŒè¯å’Œæœç´¢å°†å¤§å¤§é¿å…è¿™ç§å¸¸è§çš„é—®é¢˜ã€‚ğŸˆ


### RandomForestRegressor

- `RandomForestRegressor`æ˜¯ä¸€ç§åŸºäºéšæœºæ£®æ—çš„å›å½’æ¨¡å‹ã€‚

- å®ƒæ˜¯ä¸€ç§é›†æˆå­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡ç»„åˆå¤šä¸ªå†³ç­–æ ‘æ¥æé«˜é¢„æµ‹çš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚æ¯ä¸ªå†³ç­–æ ‘éƒ½æ˜¯ç‹¬ç«‹çš„ï¼Œå¹¶ä¸”é‡‡ç”¨éšæœºé€‰æ‹©çš„æ ·æœ¬å’Œç‰¹å¾æ¥è¿›è¡Œè®­ç»ƒã€‚

- åœ¨é¢„æµ‹æ—¶ï¼Œéšæœºæ£®æ—å°†æ‰€æœ‰å†³ç­–æ ‘çš„é¢„æµ‹ç»“æœè¿›è¡Œå¹³å‡æˆ–æŠ•ç¥¨ï¼Œä»¥å¾—åˆ°æœ€ç»ˆçš„é¢„æµ‹ç»“æœã€‚

- è¯¥æ¨¡å‹å¯ä»¥ç”¨äºè§£å†³å›å½’é—®é¢˜ï¼Œä¾‹å¦‚é¢„æµ‹æˆ¿ä»·ã€è‚¡ç¥¨ä»·æ ¼ç­‰è¿ç»­å˜é‡çš„å€¼ã€‚

- `RandomForestRegressor`ç±»æ˜¯Scikit-learnä¸­å®ç°éšæœºæ£®æ—å›å½’æ¨¡å‹çš„ç±»ã€‚å®ƒä½¿ç”¨å¤šä¸ªå†³ç­–æ ‘æ¥æ‹Ÿåˆæ•°æ®ï¼Œå¹¶ä½¿ç”¨Baggingæ–¹æ³•ï¼ˆè‡ªåŠ©é‡‡æ ·ï¼‰æ¥å‡å°‘è¿‡æ‹Ÿåˆã€‚åœ¨è®­ç»ƒæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‡å®šå†³ç­–æ ‘çš„æ•°é‡ã€æ¯ä¸ªå†³ç­–æ ‘çš„æœ€å¤§æ·±åº¦ã€æ¯ä¸ªå†³ç­–æ ‘ä½¿ç”¨çš„ç‰¹å¾æ•°é‡ç­‰å‚æ•°ã€‚

- ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨`RandomForestRegressor`ç±»æ¥è®­ç»ƒå’Œé¢„æµ‹çš„ç®€å•ä¾‹å­ï¼š

- ```python
  from sklearn.datasets import load_diabetes
  from sklearn.ensemble import RandomForestRegressor
  from sklearn.model_selection import train_test_split
  from sklearn.metrics import mean_squared_error
  
  # load data
  diabetes = load_diabetes()
  X, y = diabetes.data, diabetes.target
  
  # split data into train and test sets
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
  
  # train the model
  model = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)
  model.fit(X_train, y_train)
  
  # make predictions on test set
  y_pred = model.predict(X_test)
  
  # evaluate the model
  mse = mean_squared_error(y_test, y_pred)
  print("Mean Squared Error:", mse)
  ```

  - åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆåŠ è½½ç³–å°¿ç—…æ•°æ®é›†ï¼Œå¹¶å°†æ•°æ®é›†æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚æ¥ç€ï¼Œæˆ‘ä»¬ä½¿ç”¨`RandomForestRegressor`ç±»æ¥è®­ç»ƒæ¨¡å‹ã€‚åœ¨è®­ç»ƒæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬æŒ‡å®šäº†100ä¸ªå†³ç­–æ ‘ã€æ¯ä¸ªå†³ç­–æ ‘çš„æœ€å¤§æ·±åº¦ä¸º5ã€ä½¿ç”¨éšæœºç§å­ä¸º42ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹ï¼Œå¹¶ä½¿ç”¨å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰æ¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚


### MSE

- å‡æ–¹è¯¯å·®ï¼ˆMean Squared Errorï¼ŒMSEï¼‰æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å›å½’æ¨¡å‹æ€§èƒ½çš„æŒ‡æ ‡ã€‚å®ƒåº¦é‡äº†æ¨¡å‹é¢„æµ‹å€¼å’ŒçœŸå®å€¼ä¹‹é—´çš„å¹³å‡å·®çš„å¹³æ–¹ã€‚

- å…·ä½“åœ°ï¼Œè®¾é¢„æµ‹å€¼ä¸º $\hat{y}_i$ï¼ŒçœŸå®å€¼ä¸º $y_i$ï¼Œæ ·æœ¬æ•°é‡ä¸º $n$ï¼Œåˆ™ MSE çš„è®¡ç®—å…¬å¼ä¸ºï¼š

  $$MSE = \frac{1}{n}\sum_{i=1}^{n}(\hat{y}_i - y_i)^2$$

  MSEè¶Šå°ï¼Œè¡¨ç¤ºé¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„å·®å¼‚è¶Šå°ï¼Œæ¨¡å‹çš„æ€§èƒ½è¶Šå¥½ã€‚(åœ¨æ·±åº¦å­¦ä¹ ä½œä¸ºæŸå¤±å‡½æ•°çš„ä¸€ç§)

  MSE çš„å€¼å—åˆ°ç¦»ç¾¤å€¼çš„å½±å“è¾ƒå¤§ï¼Œå› ä¸ºå®ƒè®¡ç®—çš„æ˜¯å·®å¼‚çš„å¹³æ–¹ï¼Œè€Œç¦»ç¾¤å€¼çš„å·®å¼‚å¾€å¾€æ¯”è¾ƒå¤§ã€‚

  MSE ä¸å‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰æœ‰å¯†åˆ‡å…³ç³»ï¼ŒRMSE æ˜¯ MSE çš„å¹³æ–¹æ ¹ï¼Œç”¨äºé‡åŒ–é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„å¹³å‡å·®å¼‚ã€‚RMSE çš„å•ä½ä¸é¢„æµ‹å€¼å’ŒçœŸå®å€¼çš„å•ä½ç›¸åŒï¼Œå› æ­¤æ›´åŠ ç›´è§‚ã€‚

## pipelineäº¤å‰éªŒè¯



- å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«äº†æˆ¿å±‹çš„é¢ç§¯ã€æˆ¿é—´æ•°é‡å’Œä»·æ ¼ç­‰ç‰¹å¾ã€‚æˆ‘ä»¬æƒ³è¦ä½¿ç”¨æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰æ¥é¢„æµ‹æˆ¿å±‹ä»·æ ¼ã€‚åœ¨ä½¿ç”¨SVMä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œä¾‹å¦‚å°†ç‰¹å¾ç¼©æ”¾åˆ°ç›¸åŒçš„å°ºåº¦ä¸Šã€‚

- å¦‚æœæˆ‘ä»¬ä¸ä½¿ç”¨pipelineï¼Œè€Œæ˜¯å…ˆå¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œç¼©æ”¾ï¼Œç„¶åå°†ç¼©æ”¾åçš„æ•°æ®é›†æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå¹¶å¯¹å®ƒä»¬è¿›è¡Œäº¤å‰éªŒè¯å’Œæœç´¢ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°†ä¼šå­˜åœ¨æ•°æ®æ³„æ¼ï¼ˆdata leakageï¼‰çš„é—®é¢˜ã€‚è¿™æ˜¯å› ä¸ºï¼Œæˆ‘ä»¬åœ¨å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œç¼©æ”¾æ—¶ï¼Œå·²ç»ä½¿ç”¨äº†æµ‹è¯•é›†ä¸­çš„ä¿¡æ¯ï¼Œä»è€Œä½¿å¾—è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¹‹é—´ä¸å†æ˜¯ç‹¬ç«‹çš„ã€‚

- å…·ä½“æ¥è¯´ï¼Œå¦‚æœæˆ‘ä»¬åœ¨å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œç¼©æ”¾åå†å¯¹å…¶è¿›è¡Œæ‹†åˆ†ï¼Œé‚£ä¹ˆè®­ç»ƒé›†ä¸­çš„æŸäº›æ•°æ®å¯èƒ½å·²ç»â€œçŸ¥é“â€äº†æµ‹è¯•é›†ä¸­çš„ä¸€äº›ä¿¡æ¯ï¼Œè¿™å°†å¯¼è‡´æˆ‘ä»¬é«˜ä¼°SVMçš„æ€§èƒ½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨pipelineæ¥ç¡®ä¿åœ¨äº¤å‰éªŒè¯å’Œæœç´¢è¿‡ç¨‹ä¸­ï¼Œé¢„å¤„ç†æ­¥éª¤ä»…ä½¿ç”¨è®­ç»ƒé›†ä¸­çš„ä¿¡æ¯ï¼Œè€Œä¸æ¶‰åŠæµ‹è¯•é›†ã€‚


ä½¿ç”¨pipelineçš„æ–¹æ³•å¦‚ä¸‹ï¼š

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVR
from sklearn.model_selection import GridSearchCV

# create a pipeline with scaling and SVM
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('svm', SVR())
])

# define the parameter space for grid search
param_grid = {
    'svm__C': [0.1, 1, 10],
    'svm__gamma': [0.01, 0.1, 1],
}

# create a grid search object and fit to the data
grid_search = GridSearchCV(pipeline, param_grid, cv=5)
grid_search.fit(X_train, y_train)

# evaluate the best model on the test set
grid_search.score(X_test, y_test)
```

- åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªpipelineå¯¹è±¡ï¼Œå…¶ä¸­åŒ…å«äº†ä¸€ä¸ªStandardScalerå¯¹è±¡å’Œä¸€ä¸ªSVRå¯¹è±¡ã€‚StandardScalerå¯¹è±¡ç”¨äºå¯¹æ•°æ®è¿›è¡Œç¼©æ”¾ï¼Œè€ŒSVRå¯¹è±¡ç”¨äºè¿›è¡Œæ”¯æŒå‘é‡æœºå›å½’ã€‚ç„¶åï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªå‚æ•°ç©ºé—´param_gridï¼Œå…¶ä¸­åŒ…å«äº†SVMçš„ä¸¤ä¸ªè¶…å‚æ•°Cå’Œgammaçš„å–å€¼èŒƒå›´ã€‚æ¥ç€ï¼Œæˆ‘ä»¬ä½¿ç”¨GridSearchCVè¿›è¡Œäº¤å‰éªŒè¯å’Œæœç´¢ï¼Œå°†pipelineä½œä¸ºä¼°è®¡å™¨å¯¹è±¡ï¼Œå¹¶ä¼ å…¥å‚æ•°ç©ºé—´param_gridã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°äº†æœ€ä½³æ¨¡å‹çš„æ€§èƒ½ã€‚

- éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†Pipelineå¯¹è±¡æ¥å°†å¤šä¸ªæ­¥éª¤ç»„åˆåœ¨ä¸€èµ·ï¼Œå¹¶ç¡®ä¿åœ¨äº¤å‰éªŒè¯å’Œæœç´¢è¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ªæ­¥éª¤ä»…ä½¿ç”¨è®­ç»ƒé›†ä¸­çš„ä¿¡æ¯ï¼Œè€Œä¸æ¶‰åŠæµ‹è¯•é›†ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å¯ä»¥é¿å…æ•°æ®æ³„æ¼é—®é¢˜ï¼Œå¹¶è·å¾—æ›´å‡†ç¡®çš„æ¨¡å‹è¯„ä¼°ç»“æœã€‚

## Next steps

- We have briefly covered estimator fitting and predicting, pre-processing steps, pipelines, cross-validation tools and automatic hyper-parameter searches. This guide should give you an overview of some of the main features of the library, but there is much more to `scikit-learn`!

- Please refer to our [User Guide](https://scikit-learn.org/stable/user_guide.html#user-guide) for details on all the tools that we provide. You can also find an exhaustive list of the public API in the [API Reference](https://scikit-learn.org/stable/modules/classes.html#api-ref).

- You can also look at our numerous [examples](https://scikit-learn.org/stable/auto_examples/index.html#general-examples) that illustrate the use of `scikit-learn` in many different contexts.

- The [tutorials](https://scikit-learn.org/stable/tutorial/index.html#tutorial-menu) also contain additional learning resources.


