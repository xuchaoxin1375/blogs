[toc]

## 神经网络

- 神经网络(neural networks)方面的研究很早就已出现,今天“神经网络”已是一个相当大的、多学科交叉的学科领域.各相关学科对神经网络的**定义**多种多样
- 用得最广泛的一种定义,即“神经网络是由具有**适应性**的**简单单元组**成的**广泛并行互连的网络**,它的**组织**能够**模拟**<u>生物神经系统</u>对真实世界物体所作出的**交互反应**”[Kohonen,1988].
- 机器学习中谈论**神经网络**指的是“**神经网络学习**”,或者说,是<u>机器学习与神经网络</u>这两个**学科领域**的**交叉部分**.

### 神经元

- 神经网络中<u>最基本的成分</u>是**神经元**(neuron或unit)模型,即上述定义中的“**简单单元**”．
  - 在生物神经网络中,每个神经元与其他神经元相连，当它“兴奋”时，就会向相连的神经元发送化学物质,从而改变这些神经元内的电位;
  - 如果某神经元的**电位**超过了一个“**阈值**”(threshold或bias,区别`阈`和`阀`,阈值vs阀门)，那么它就会被激活，即“兴奋”起来,<u>向其他神经元</u>发送化学物质.

- 上述情形抽象为一直沿用至今的“M-P神经元模型”(1943年，[McCulloch and Pitts,1943])
  - ![在这里插入图片描述](https://img-blog.csdnimg.cn/6658a53d1de04a5ea63a6e423b2d1216.png)
  - $x_i$表示来第i个神经元的输入
  - $w_i$表示第i个神经元的连接权重weight
  - $\theta$表示阈值(bias或threshold)
  - $f$表示激活函数
  - y表示输出,表达式$f(\sum_{i=1}^{n}w_ix_i-\theta)$综合运用了上述值
- 在这个模型中,神经元接收到来自n个其他神经元传递过来的**输入信号**,这些输入信号通过**带权重的连接**(connection)进行传递,神经元接收到的**总输入值**将与神经元的**阈值**进行比较,然后通过“激活函数”(activation function)处理以产生神经元的输出.
- 理想中的**激活函数**(响应函数)是**阶跃函数**，它将输入值映射为输出值“0”或“1”，显然“1”对应于神经元**兴奋**，“0”对应于神经元**抑制**.
- 然而,阶跃函数具有<u>不连续、不光滑</u>等**不太好的性质**，因此实际常用`Sigmoid`函数作为激活函数.
- 典型的Sigmoid函数把可能在较大范围内变化的输入值挤压到(0,1)输出值范围内，因此有时也称为“**挤压函数**”(squashing function).
- 把许多个这样的神经元按一定的层次结构连接起来,就得到了神经网络.
- 从计算机科学的角度看，我们可以不考虑神经网络是否真的模拟了生物神经网络
- 只需将一个**神经网络**视为包含了许多参数的**数学模型**,这个模型是**若干个函数**,例如$f(\sum_{i}w_ix_i-\theta_j)$相互(嵌套)代入而得.
- 有效的**神经网络学习算法**大多以**数学证明**为支撑.

## 感知机

- 一般地,给定训练数据集,权重$w_i(i=1,2,\cdots,n)$以及阈值$\theta$可通过学习得到.
- 阈值$\theta$可看作一个固定输入为-1.0的“哑结点”(dummy node)所对应的连接权重$w_{n+1}$
- 这样,<u>权重和阈值的学习</u>就可**统一为权重的学习**.
- 感知机学习规则非常简单，对训练样例$s=(\boldsymbol{x},y)$，若当前感知机的输出为$\hat{y}$，则感知机**权重**将这样调整:
  - $w_i\leftarrow{w_i+\Delta{w_i}}$
    - $\Delta{w_i}=\eta(y-\hat{y})x_i$
    - 其中$\eta\in(0,1)$称为**学习率**(learning rate)
- 若感知机P对训练样例$s$预测正确,则$\hat{y}=y$,感知机不发生变化(因为$\Delta{w_i}=\eta\times{0}\times{x_i}=0$,否则根据错误程度调整权重
- 感知机只有输出层神经元进行了**激活处理**(只有一层**功能神经元**(functional neuron)),学习能力很有限
  - 可以用来学习`与,或,非`这类**线性可分**(linearly separable)问题(`亦或`则是非线性可分问题)
  - 若两类模式是**线性可分**的(存在一个超平面能够它们分开),则感知机的学习过程会收敛(converge),可以求得适当的**权向量**:$\boldsymbol{w}=(w_1;w_2;\cdots;w_{n+1})$
  - 否则感知机学习过程会发生震荡(fluctuation),$\boldsymbol{w}$无法稳定下来

- 要解决非线性可分问题,需考虑使用多层功能神经元.例如，简单的**两层感知机**就能解决异或问题.
  - 2层感知机的<u>输出层与输入层之间</u>的一层神经元,被称为**隐层**或**隐含层**(hidden layer),
  - ![在这里插入图片描述](https://img-blog.csdnimg.cn/4d99ef984bed4ebb851ae32feaccd46d.png)
- 隐含层和输出层**神经元**都是拥有**激活函数**的**功能神经元**.
- 更一般的,常见的神经网络有p个层的层级结构,每层神经元与下一层神经元**全互连**,神经元之间**不存在同层连接**,也不存在**跨层连接**，这样的神经网络结构通常称为“**多层前馈神经网络**”(multi-layer feedforward neuralnetworks)，
  - 前馈是指网络结构上不存环（回路）
- 其中**输入层**神经元接收**外界输入**，隐层与输出层神经元<u>对信号进行加工</u>,最终结果由输出层神经元输出;
  - 换言之,<u>输入层神经元仅是接受输入,不进行函数处理</u>
  - 隐层与输出层包含功能神经元.2层感知机通常被称为“两层网络”．为避免歧义,可称其为“**单隐层网络**”．只需包含隐层，即可称为**多层网络**.
- 神经网络的学习过程,就是根据训练数据来调整<u>神经元之间的“**连接权**”</u>(connection weight)以及每个功能神经元的阈值;
- 换言之,神经网络“学”到的东西,蕴涵在**连接权与阈值**中，



## 误差逆传播算法(反向传播算法)

- 多层网络的学习能力比单层感知机强得多．欲训练多层网络，简单感知机学习规则显然不够了,需要更强大的学习算法．
- 误差逆传播(errorBackPropagation,简称BP)算法就是其中最杰出的代表,它是最经典的神经网络学习算法.现实任务中使用神经网络时,大多是在使用BP算法进行训练.
- BP算法不仅可用于**多层前馈神经网络**,还可用于其他类型的神经网络,例如训练**递归神经网络**[Pineda, 1987].但通常说“BP网络”时,一般是指用BP算法训练的多层前馈神经网络.
- 







