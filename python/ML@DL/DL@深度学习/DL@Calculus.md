[toc]

## DL@微积分

- 在深度学习中，我们“训练”模型，不断更新它们，使它们在看到越来越多的数据时变得越来越好。

  通常情况下，变得更好意味着最小化一个**损失函数**（loss function），即一个衡量“模型有多糟糕”这个问题的分数。

  最终，我们真正关心的是生成一个模型，它能够在从未见过的数据上表现良好。

  但“训练”模型只能将模型与我们实际能看到的数据相拟合。因此，我们可以将拟合模型的任务分解为两个关键问题：

- **优化**（optimization）：用模型拟合观测数据的过程；

- **泛化**（generalization）：数学原理和实践者的智慧，能够指导我们生成出有效性超出用于训练的数据集本身的模型。

- 在深度学习中，我们通常选择对于模型参数可微的损失函数。简而言之，对于每个参数，如果我们把这个参数**增加**或**减少**一个无穷小的量，可以知道损失会以多快的速度增加或减少，