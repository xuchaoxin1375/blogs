# Speech Emotion Recognition

## Introduction

- This repository handles building and training Speech Emotion Recognition System.
- The basic idea behind this tool is to build and train/test a suited machine learning ( as well as deep learning ) algorithm that could recognize and detects human emotions from speech.
- This is useful for many industry fields such as making product recommendations, affective computing, etc.
- Check this [tutorial](https://www.thepythoncode.com/article/building-a-speech-emotion-recognizer-using-sklearn) for more information.

## Requirements

- **Python 3.6+**

### Python Packages

- **tensorflow**
- **librosa==0.6.3**
- **numpy**
- **pandas**
- **soundfile==0.9.0**
- **wave**
- **scikit-learn==0.24.2**
- **tqdm==4.28.1**
- **matplotlib==2.2.3**
- **pyaudio==0.2.11**
- **[ffmpeg](https://ffmpeg.org/) (optional)**: used if you want to add more sample audio by converting to 16000Hz sample rate and mono channel which is provided in ``convert_wavs.py``

Install these libraries by the following command:

```
pip3 install -r requirements.txt
```

### Dataset

This repository used 4 datasets (including this repo's custom dataset) which are downloaded and formatted already in `data` folder:

- [**RAVDESS**](https://zenodo.org/record/1188976) : The **R**yson **A**udio-**V**isual **D**atabase of **E**motional **S**peech and **S**ong that contains 24 actors (12 male, 12 female), vocalizing two lexically-matched statements in a neutral North American accent.
- [**TESS**](https://tspace.library.utoronto.ca/handle/1807/24487) : **T**oronto **E**motional **S**peech **S**et that was modeled on the Northwestern University Auditory Test No. 6 (NU-6; Tillman & Carhart, 1966). A set of 200 target words were spoken in the carrier phrase "Say the word _____' by two actresses (aged 26 and 64 years).
- [**EMO-DB**](http://emodb.bilderbar.info/docu/) : As a part of the DFG funded research project SE462/3-1 in 1997 and 1999 we recorded a database of emotional utterances spoken by actors. The recordings took place in the anechoic chamber of the Technical University Berlin, department of Technical Acoustics. Director of the project was Prof. Dr. W. Sendlmeier, Technical University of Berlin, Institute of Speech and Communication, department of communication science. Members of the project were mainly Felix Burkhardt, Miriam Kienast, Astrid Paeschke and Benjamin Weiss.
- **Custom** : Some unbalanced noisy dataset that is located in `data/train-custom` for training and `data/test-custom` for testing in which you can add/remove recording samples easily by converting the raw audio to 16000 sample rate, mono channel (this is provided in `create_wavs.py` script in ``convert_audio(audio_path)`` method which requires [ffmpeg](https://ffmpeg.org/) to be installed and in *PATH*) and adding the emotion to the end of audio file name separated with '_' (e.g "20190616_125714_happy.wav" will be parsed automatically as happy)

#### desc_files(csv å…ƒæ•°æ®æ–‡ä»¶)ğŸˆ

- desc_files (csv)æ–‡ä»¶å­˜å‚¨çš„æ˜¯è¯­æ–™åº“çš„è¯­éŸ³æ–‡ä»¶çš„è·¯å¾„(path),æƒ…æ„Ÿç±»åˆ«(emotion)

### Emotions available

There are 9 emotions available: "neutral", "calm", "happy" "sad", "angry", "fear", "disgust", "ps" (pleasant surprise) and "boredom".

## Feature Extraction@ç‰¹å¾æå–

Feature extraction is the main part of the speech emotion recognition system. It is basically accomplished by changing the speech waveform to a form of parametric representation at a relatively lesser data rate.

ç‰¹å¾æå–æ˜¯è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿçš„ä¸»è¦éƒ¨åˆ†ã€‚å®ƒåŸºæœ¬ä¸Šæ˜¯é€šè¿‡å°†è¯­éŸ³æ³¢å½¢è½¬æ¢ä¸ºå‚æ•°å½¢å¼çš„è¡¨ç¤ºå½¢å¼ï¼Œä»¥ç›¸å¯¹è¾ƒä½çš„æ•°æ®é€Ÿç‡å®Œæˆçš„ã€‚

â€œæ•°æ®é€Ÿç‡("Data rate")â€ï¼Œå®ƒæ˜¯æŒ‡åœ¨ç‰¹å®šæ—¶é—´å†…ä¼ è¾“çš„æ•°æ®é‡ã€‚åœ¨è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿä¸­ï¼Œæ•°æ®é€Ÿç‡æ˜¯æŒ‡æ¯ç§’é’Ÿä¼ è¾“çš„è¯­éŸ³æ•°æ®é‡ï¼Œé€šå¸¸ä»¥æ¯”ç‰¹ç‡ï¼ˆbits per secondï¼‰æˆ–åƒä½æ¯ç§’ï¼ˆkilobits per secondï¼‰ä¸ºå•ä½è¿›è¡Œåº¦é‡ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ•°æ®é€Ÿç‡è¿˜å¯ä»¥ç”¨äºæè¿°å…¶ä»–ç±»å‹çš„æ•°æ®ä¼ è¾“ï¼Œä¾‹å¦‚ç½‘ç»œä¼ è¾“æˆ–å­˜å‚¨ä»‹è´¨çš„è¯»å–é€Ÿåº¦ã€‚åœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œå®ƒé€šå¸¸æŒ‡åœ¨ç‰¹å®šæ—¶é—´å†…ä¼ è¾“æˆ–å¤„ç†çš„æ•°æ®é‡ï¼Œé€šå¸¸ä»¥æ¯”ç‰¹ç‡æˆ–å­—èŠ‚ç‡ï¼ˆbytes per secondï¼‰ä¸ºå•ä½è¿›è¡Œåº¦é‡ã€‚

ç‰¹å¾æå–çš„è¿‡ç¨‹é€šè¿‡å°†è¯­éŸ³æ³¢å½¢è½¬æ¢ä¸ºå‚æ•°å½¢å¼çš„è¡¨ç¤ºå½¢å¼ï¼Œå¯ä»¥å‡å°‘è¯­éŸ³ä¿¡å·çš„æ•°æ®é€Ÿç‡ã€‚è¿™æ˜¯å› ä¸ºï¼ŒåŸå§‹è¯­éŸ³ä¿¡å·é€šå¸¸åŒ…å«å¤§é‡å†—ä½™ä¿¡æ¯ï¼Œè€Œé€šè¿‡æå–ä¸æƒ…æ„ŸçŠ¶æ€ç›¸å…³çš„å£°å­¦ç‰¹å¾ï¼Œå¯ä»¥å‹ç¼©æ•°æ®å¹¶å‡å°‘ä¼ è¾“æ‰€éœ€çš„å¸¦å®½å’Œå­˜å‚¨ç©ºé—´ã€‚

å› æ­¤ï¼Œç‰¹å¾æå–å¯¹äºé«˜æ•ˆå¤„ç†å’Œåˆ†æå¤§é‡è¯­éŸ³æ•°æ®æ˜¯è‡³å…³é‡è¦çš„ã€‚


In this repository, we have used the most used features that are available in [librosa](https://github.com/librosa/librosa) library including:

- [MFCC](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum)
- Chromagram
- MEL Spectrogram Frequency (mel)
- Contrast
- Tonnetz (tonal centroid features)
- æœ¬é¡¹ç›®é‡‡ç”¨ä»¥ä¸‹ç‰¹å¾

* MFCCï¼šMFCCæ˜¯ä¸€ç§å¸¸ç”¨çš„å£°å­¦ç‰¹å¾ï¼Œå®ƒæ˜¯é€šè¿‡å°†è¯­éŸ³ä¿¡å·è½¬æ¢ä¸ºæ¢…å°”é¢‘ç‡å€’è°±ç³»æ•°æ¥æå–çš„ã€‚MFCCå…·æœ‰å¯¹äºè¯­éŸ³ä¿¡å·ä¸­çš„é¢‘ç‡å˜åŒ–è¾ƒä¸ºæ•æ„Ÿçš„ç‰¹ç‚¹ï¼Œå¯¹äºè¯†åˆ«ä¸åŒæƒ…æ„ŸçŠ¶æ€å…·æœ‰å¾ˆå¥½çš„åŒºåˆ†èƒ½åŠ›ã€‚
* Chromagramï¼šChromagramæ˜¯ä¸€ç§åŸºäºéŸ³é«˜çš„ç‰¹å¾ï¼Œå®ƒé€šè¿‡è®¡ç®—è¯­éŸ³ä¿¡å·ä¸­ä¸åŒçš„éŸ³é«˜åˆ†å¸ƒæ¥æå–ã€‚
* MELé¢‘è°±ï¼šMELé¢‘è°±æ˜¯ä¸€ç§åŸºäºäººè€³å¬è§‰æ¨¡å‹çš„ç‰¹å¾ï¼Œå®ƒå°†è¯­éŸ³ä¿¡å·è½¬æ¢ä¸ºæ¢…å°”é¢‘ç‡åˆ†å¸ƒï¼Œå¹¶å¯¹æ¯ä¸ªé¢‘ç‡åˆ†å¸ƒè¿›è¡Œç¦»æ•£ä½™å¼¦å˜æ¢ä»¥æå–ç‰¹å¾ã€‚MELé¢‘è°±å¯¹äºè¯­éŸ³ä¿¡å·ä¸­çš„é¢‘ç‡å˜åŒ–è¾ƒä¸ºæ•æ„Ÿï¼Œä½†ä¸MFCCç›¸æ¯”ï¼Œå®ƒåœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½å…·æœ‰æ›´å¥½çš„åŒºåˆ†èƒ½åŠ›ã€‚
* å¯¹æ¯”åº¦ï¼šå¯¹æ¯”åº¦æ˜¯ä¸€ç§åŸºäºè¯­éŸ³ä¿¡å·çš„å¼ºåº¦å˜åŒ–çš„ç‰¹å¾ï¼Œå®ƒå¯ä»¥é€šè¿‡è®¡ç®—ä¸åŒé¢‘ç‡åˆ†é‡ä¹‹é—´çš„èƒ½é‡å·®å¼‚æ¥æå–ã€‚å¯¹æ¯”åº¦å¯¹äºè¯†åˆ«è¯­éŸ³ä¿¡å·ä¸­çš„å¼ºåº¦å˜åŒ–å…·æœ‰å¾ˆå¥½çš„åŒºåˆ†èƒ½åŠ›ï¼Œä½†åœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½å¯¹äºæƒ…æ„ŸçŠ¶æ€çš„è¯†åˆ«ä¸å¤Ÿæ•æ„Ÿã€‚
* Tonnetzï¼šTonnetzæ˜¯ä¸€ç§åŸºäºéŸ³é«˜çš„ç‰¹å¾ï¼Œå®ƒé€šè¿‡è®¡ç®—ä¸åŒéŸ³é«˜ä¹‹é—´çš„è·ç¦»å’Œç»„åˆå…³ç³»æ¥æå–ã€‚åœ¨å¾‹å­¦ä¸å’Œå£°å­¦ä¸­ï¼Œè°ƒæ€§ç½‘ç»œï¼Œæˆ–æ‰˜å†…æ–¯ï¼ˆæ¥è‡ªäºå¾·è¯­â€œTonnetzâ€ï¼Œâ€œtone-networkâ€çš„æ„æ€ï¼‰æ˜¯ä¸€ç§ç”¨äºè¡¨ç¤ºè°ƒæ€§ç©ºé—´çš„ã€æ¦‚å¿µæ€§çš„éŸ³ä¹æ ¼å­å›¾ï¼Œç”±è±æ˜‚å“ˆå¾·Â·æ¬§æ‹‰äº1739å¹´æå‡ºã€‚è°ƒæ€§ç½‘ç»œçš„å„ç§å¯è§†åŒ–å½¢å¼å¯è¢«ç”¨äºè¡¨ç¤ºæ¬§æ´²å¤å…¸éŸ³ä¹çš„ä¼ ç»Ÿå’Œå£°å…³ç³»ã€‚Tonnetzåœ¨éŸ³ä¹ä¿¡æ¯æ£€ç´¢å’ŒéŸ³ä¹æƒ…æ„Ÿè¯†åˆ«ä¸­å¹¿æ³›åº”ç”¨.
* æ€»çš„æ¥è¯´ï¼Œè¿™äº›ç‰¹å¾åœ¨è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ä¸­éƒ½å…·æœ‰ä¸€å®šçš„åº”ç”¨ä»·å€¼ï¼Œä½†å…·ä½“é€‰ç”¨å“ªäº›ç‰¹å¾éœ€è¦æ ¹æ®å®é™…åº”ç”¨æƒ…å†µå’Œæ•°æ®åˆ†æç»“æœè¿›è¡Œé€‰æ‹©ã€‚åœ¨ç‰¹å¾æå–çš„è¿‡ç¨‹ä¸­ï¼Œéœ€è¦ç»¼åˆè€ƒè™‘ä¸åŒç‰¹å¾ä¹‹é—´çš„äº’è¡¥æ€§å’Œå·®å¼‚æ€§ï¼Œå¹¶å¯¹ç‰¹å¾è¿›è¡Œåˆç†çš„ç»„åˆå’Œè°ƒæ•´ï¼Œä»¥æé«˜è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«çš„å‡†ç¡®ç‡å’Œé²æ£’æ€§ã€‚

åœ¨è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ä¸­ï¼Œå¸¸ç”¨çš„ç‰¹å¾åŒ…æ‹¬ä»¥ä¸‹å‡ ç§ï¼š

1. é¢‘è°±ç‰¹å¾ï¼šé¢‘è°±ç‰¹å¾åŒ…æ‹¬å…±æŒ¯å³°é¢‘ç‡ã€è°æ³¢æ¯”ã€é¢‘è°±èƒ½é‡ç­‰ã€‚è¿™äº›ç‰¹å¾åæ˜ äº†å£°éŸ³çš„é¢‘åŸŸä¿¡æ¯ï¼Œå¯¹äºè¯†åˆ«ä¸åŒæƒ…æ„ŸçŠ¶æ€å…·æœ‰å¾ˆå¥½çš„åŒºåˆ†èƒ½åŠ›ã€‚
2. éŸµå¾‹ç‰¹å¾ï¼šéŸµå¾‹ç‰¹å¾åŒ…æ‹¬åŸºé¢‘ã€è¯­é€Ÿã€èŠ‚å¥ç­‰ã€‚è¿™äº›ç‰¹å¾åæ˜ äº†å£°éŸ³çš„æ—¶é—´å’ŒèŠ‚å¥ä¿¡æ¯ï¼Œå¯¹äºè¯†åˆ«è¯­éŸ³æƒ…æ„ŸçŠ¶æ€ä¹Ÿéå¸¸é‡è¦ã€‚
3. éçº¿æ€§ç‰¹å¾ï¼šéçº¿æ€§ç‰¹å¾åŒ…æ‹¬å£°è°ƒã€é¢¤éŸ³ã€å˜¶å“‘ç­‰ã€‚è¿™äº›ç‰¹å¾åæ˜ äº†å£°éŸ³çš„è´¨é‡å’Œè¡¨ç°å½¢å¼ï¼Œå¯¹äºè¯†åˆ«ä¸åŒæƒ…æ„ŸçŠ¶æ€ä¹Ÿæœ‰å¾ˆå¥½çš„è¯†åˆ«èƒ½åŠ›ã€‚
4. è¯­éŸ³è´¨é‡ç‰¹å¾ï¼šè¯­éŸ³è´¨é‡ç‰¹å¾åŒ…æ‹¬å™ªå£°ã€å¤±çœŸã€æ¸…æ™°åº¦ç­‰ã€‚è¿™äº›ç‰¹å¾åæ˜ äº†è¯­éŸ³ä¿¡å·çš„è´¨é‡ï¼Œå¯èƒ½å¯¹æƒ…æ„ŸçŠ¶æ€çš„è¯†åˆ«äº§ç”Ÿå½±å“ã€‚
5. æƒ…æ„Ÿè¯æ±‡ç‰¹å¾ï¼šæƒ…æ„Ÿè¯æ±‡ç‰¹å¾æ˜¯ä»è¯­éŸ³ä¿¡å·ä¸­æå–å‡ºä¸æƒ…æ„ŸçŠ¶æ€ç›¸å…³çš„è¯æ±‡ï¼Œå¯ä»¥é€šè¿‡æƒ…æ„Ÿè¯å…¸ç­‰å·¥å…·æ¥å®ç°ã€‚

è¿™äº›ç‰¹å¾åœ¨ä¸åŒæƒ…æ„ŸçŠ¶æ€ä¹‹é—´å…·æœ‰ä¸åŒçš„åŒºåˆ†èƒ½åŠ›ï¼Œå› æ­¤åœ¨è®¾è®¡è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿæ—¶éœ€è¦ç»¼åˆè€ƒè™‘å®ƒä»¬çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶æ ¹æ®å…·ä½“æƒ…å†µé€‰æ‹©åˆé€‚çš„ç‰¹å¾ç»„åˆã€‚ä¾‹å¦‚ï¼Œä¸€äº›ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºé¢‘è°±ç‰¹å¾å’ŒéŸµå¾‹ç‰¹å¾çš„ç‰¹å¾ç»„åˆå¯ä»¥å–å¾—è¾ƒå¥½çš„æƒ…æ„Ÿè¯†åˆ«æ•ˆæœï¼›è€Œå…¶ä»–ç ”ç©¶åˆ™å‘ç°ï¼Œéçº¿æ€§ç‰¹å¾å¯¹äºè¯†åˆ«æŸäº›æƒ…æ„ŸçŠ¶æ€å…·æœ‰æ›´å¥½çš„åŒºåˆ†èƒ½åŠ›ã€‚å› æ­¤ï¼Œåœ¨å®é™…åº”ç”¨ä¸­ï¼Œéœ€è¦æ ¹æ®å…·ä½“æƒ…å†µé€‰æ‹©é€‚åˆçš„ç‰¹å¾ç»„åˆï¼Œå¹¶é€šè¿‡æœºå™¨å­¦ä¹ ç®—æ³•ç­‰æ‰‹æ®µå¯¹è¯­éŸ³ä¿¡å·è¿›è¡Œåˆ†ç±»å’Œè¯†åˆ«ã€‚

## Grid Search@ç½‘æ ¼æœç´¢



- [3.2. Tuning the hyper-parameters of an estimator â€” scikit-learn 1.2.2 documentation](https://scikit-learn.org/stable/modules/grid_search.html)
- ç½‘æ ¼æœç´¢æ˜¯ä¸€ç§æœºå™¨å­¦ä¹ ä¸­çš„è¶…å‚æ•°è°ƒä¼˜æŠ€æœ¯ï¼Œå…¶ç›®çš„æ˜¯æ‰¾åˆ°æ¨¡å‹è¶…å‚æ•°çš„æœ€ä¼˜å€¼ã€‚
- è¶…å‚æ•°æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸ä¼šè¢«å­¦ä¹ çš„å‚æ•°ï¼Œä½†åœ¨è®­ç»ƒå‰éœ€è¦è®¾ç½®ï¼Œå¯ä»¥å¯¹æ¨¡å‹æ€§èƒ½äº§ç”Ÿé‡è¦å½±å“ã€‚
- Grid Search å¯ä»¥ç¿»è¯‘ä¸ºâ€œç½‘æ ¼æœç´¢â€æˆ–â€œç½‘æ ¼è°ƒå‚â€ï¼Œæ˜¯ä¸€ç§å¸¸ç”¨çš„æœºå™¨å­¦ä¹ è¶…å‚æ•°è°ƒä¼˜æ–¹æ³•ã€‚
- Grid Search çš„ç›®çš„æ˜¯é€šè¿‡éå†ç»™å®šçš„è¶…å‚æ•°ç»„åˆï¼Œæ‰¾åˆ°æœ€ä¼˜çš„è¶…å‚æ•°ç»„åˆï¼Œä»¥è·å¾—æœ€ä½³çš„æ¨¡å‹æ€§èƒ½ã€‚å®ƒåŸºäºä¸€ä¸ªé¢„å®šä¹‰çš„è¶…å‚æ•°ç½‘æ ¼ï¼ˆgridï¼‰ï¼Œå¯¹æ¯ä¸ªè¶…å‚æ•°ç»„åˆè¿›è¡Œè¯„ä¼°å’Œæ¯”è¾ƒï¼Œä»è€Œé€‰æ‹©æœ€ä½³çš„è¶…å‚æ•°ç»„åˆã€‚ç½‘æ ¼æœç´¢æ¶‰åŠ `<u>`å®šä¹‰ä¸€ä¸ªè¶…å‚æ•°å€¼çš„çŸ©é˜µ `</u>`ï¼Œå¹¶ä¸”ç³»ç»Ÿåœ°æœç´¢çŸ©é˜µä»¥å¯»æ‰¾åœ¨éªŒè¯æ•°æ®é›†ä¸Šè¡¨ç°æœ€ä½³çš„è¶…å‚æ•°ç»„åˆã€‚
- å…·ä½“æ¥è¯´ï¼ŒGrid Search å°†æ¯ä¸ªè¶…å‚æ•°çš„å–å€¼èŒƒå›´åˆ’åˆ†æˆä¸€ç»„ç¦»æ•£çš„å€¼ï¼Œç„¶åå¯¹æ‰€æœ‰å¯èƒ½çš„è¶…å‚æ•°ç»„åˆè¿›è¡Œéå†ï¼Œå¯¹æ¯ä¸ªç»„åˆè®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨äº¤å‰éªŒè¯ç­‰æ–¹æ³•è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚
- æœ€åï¼Œé€‰æ‹©å…·æœ‰æœ€ä½³æ€§èƒ½çš„è¶…å‚æ•°ç»„åˆä½œä¸ºæœ€ç»ˆæ¨¡å‹çš„è¶…å‚æ•°ã€‚
- Grid Search æ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„è°ƒå‚æ–¹æ³•ï¼Œä½†å®ƒéœ€è¦éå†æ‰€æœ‰å¯èƒ½çš„è¶…å‚æ•°ç»„åˆï¼Œå› æ­¤è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚ä¸ºäº†å‡å°‘è®¡ç®—æˆæœ¬ï¼Œå¯ä»¥ä½¿ç”¨ `<u>`éšæœºæœç´¢ï¼ˆRandom Searchï¼‰`</u>`ç­‰å…¶ä»–è°ƒå‚æ–¹æ³•ã€‚

Grid search results are already provided in `grid` folder, but if you want to tune various grid search parameters in `parameters.py`, you can run the script `grid_search.py` by:

```
python grid_search.py
```

This may take several hours to complete execution, once it is finished, best estimators are stored and pickled in `grid` folder.

## Example 1: Using 3 Emotions

The way to build and train a model for classifying 3 emotions is as shown below:

```python
from emotion_recognition import EmotionRecognizer
from sklearn.svm import SVC
# init a model, let's use SVC
my_model = SVC()
# pass my model to EmotionRecognizer instance
# and balance the dataset
rec = EmotionRecognizer(model=my_model, emotions=['sad', 'neutral', 'happy'], balance=True, verbose=0)
# train the model
rec.train()
# check the test accuracy for that model
print("Test score:", rec.test_score())
# check the train accuracy for that model
print("Train score:", rec.train_score())
```

**Output:**

```
Test score: 0.8148148148148148
Train score: 1.0
```

### Determining the best model

In order to determine the best model, you can by:

```python
# loads the best estimators from `grid` folder that was searched by GridSearchCV in `grid_search.py`,
# and set the model to the best in terms of test score, and then train it
rec.determine_best_model()
# get the determined sklearn model name
print(rec.model.__class__.__name__, "is the best")
# get the test accuracy score for the best estimator
print("Test score:", rec.test_score())
```

**Output:**

```
MLPClassifier is the best
Test Score: 0.8958333333333334
```

### Predicting

Just pass an audio path to the `rec.predict()` method as shown below:

```python
# this is a neutral speech from emo-db from the testing set
print("Prediction:", rec.predict("data/emodb/wav/15a04Nc.wav"))
# this is a sad speech from TESS from the testing set
print("Prediction:", rec.predict("data/validation/Actor_25/25_01_01_01_back_sad.wav"))
```

**Output:**

```
Prediction: neutral
Prediction: sad
```

You can pass any audio file, if it's not in the appropriate format (16000Hz and mono channel), then it'll be automatically converted, make sure you have `ffmpeg` installed in your system and added to *PATH*.

## Example 2: Using RNNs for 5 Emotions

```python
from deep_emotion_recognition import DeepEmotionRecognizer
# initialize instance
# inherited from emotion_recognition.EmotionRecognizer
# default parameters (LSTM: 128x2, Dense:128x2)
deeprec = DeepEmotionRecognizer(emotions=['angry', 'sad', 'neutral', 'ps', 'happy'], n_rnn_layers=2, n_dense_layers=2, rnn_units=128, dense_units=128)
# train the model
deeprec.train()
# get the accuracy
print(deeprec.test_score())
# predict angry audio sample
prediction = deeprec.predict('data/validation/Actor_10/03-02-05-02-02-02-10_angry.wav')
print(f"Prediction: {prediction}")
```

**Output:**

```
0.7717948717948718
Prediction: angry
```

Predicting probabilities is also possible (for classification ofc):

```python
print(deeprec.predict_proba("data/emodb/wav/16a01Wb.wav"))
```

**Output:**

```
{'angry': 0.99878675, 'sad': 0.0009922335, 'neutral': 7.959707e-06, 'ps': 0.00021298956, 'happy': 8.3598025e-08}
```

### Confusion Matrix

```python
print(deeprec.confusion_matrix(percentage=True, labeled=True))
```

**Output:**

```
              predicted_angry  predicted_sad  predicted_neutral  predicted_ps  predicted_happy
true_angry          80.769226       7.692308           3.846154      5.128205         2.564103
true_sad            12.820514      73.076920           3.846154      6.410257         3.846154
true_neutral         1.282051       1.282051          79.487183      1.282051        16.666668
true_ps             10.256411       3.846154           1.282051     79.487183         5.128205
true_happy           5.128205       8.974360           7.692308      8.974360        69.230774
```

## Example 3: Not Passing any Model and Removing the Custom Dataset

Below code initializes `EmotionRecognizer` with 3 chosen emotions while removing Custom dataset, and setting `balance` to `False`:

```python
from emotion_recognition import EmotionRecognizer
# initialize instance, this will take a bit the first time executed
# as it'll extract the features and calls determine_best_model() automatically
# to load the best performing model on the picked dataset
rec = EmotionRecognizer(emotions=["angry", "neutral", "sad"], balance=False, verbose=1, custom_db=False)
# it will be trained, so no need to train this time
# get the accuracy on the test set
print(rec.confusion_matrix())
# predict angry audio sample
prediction = rec.predict('data/validation/Actor_10/03-02-05-02-02-02-10_angry.wav')
print(f"Prediction: {prediction}")
```

**Output:**

```
[+] Best model determined: RandomForestClassifier with 93.454% test accuracy

              predicted_angry  predicted_neutral  predicted_sad
true_angry          98.275864           1.149425       0.574713
true_neutral         0.917431          88.073395      11.009174
true_sad             6.250000           1.875000      91.875000

Prediction: angry
```

You can print the number of samples on each class:

```python
rec.get_samples_by_class()
```

**Output:**

```
         train  test  total
angry      910   174   1084
neutral    650   109    759
sad        862   160   1022
total     2422   443   2865
```

In this case, the dataset is only from TESS and RAVDESS, and not balanced, you can pass `True` to `balance` on the `EmotionRecognizer` instance to balance the data.

## Algorithms Used

This repository can be used to build machine learning classifiers as well as regressors for the case of 3 emotions {'sad': 0, 'neutral': 1, 'happy': 2} and the case of 5 emotions {'angry': 1, 'sad': 2, 'neutral': 3, 'ps': 4, 'happy': 5}

### Classifiers

- SVC
- RandomForestClassifier
- GradientBoostingClassifier
- KNeighborsClassifier
- MLPClassifier
- BaggingClassifier
- Recurrent Neural Networks (Keras)

### Regressors

- SVR
- RandomForestRegressor
- GradientBoostingRegressor
- KNeighborsRegressor
- MLPRegressor
- BaggingRegressor
- Recurrent Neural Networks (Keras)

### Testing

You can test your own voice by executing the following command:

```
python test.py
```

Wait until "Please talk" prompt is appeared, then you can start talking, and the model will automatically detects your emotion when you stop (talking).

You can change emotions to predict, as well as models, type ``--help`` for more information.

```
python test.py --help
```

**Output:**

```
usage: test.py [-h] [-e EMOTIONS] [-m MODEL]

Testing emotion recognition system using your voice, please consider changing
the model and/or parameters as you wish.

optional arguments:
  -h, --help            show this help message and exit
  -e EMOTIONS, --emotions EMOTIONS
                        Emotions to recognize separated by a comma ',',
                        available emotions are "neutral", "calm", "happy"
                        "sad", "angry", "fear", "disgust", "ps" (pleasant
                        surprise) and "boredom", default is
                        "sad,neutral,happy"
  -m MODEL, --model MODEL
                        The model to use, 8 models available are: "SVC","AdaBo
                        ostClassifier","RandomForestClassifier","GradientBoost
                        ingClassifier","DecisionTreeClassifier","KNeighborsCla
                        ssifier","MLPClassifier","BaggingClassifier", default
                        is "BaggingClassifier"

```

## Plotting Histograms

This will only work if grid search is performed.

```python
from emotion_recognition import plot_histograms
# plot histograms on different classifiers
plot_histograms(classifiers=True)
```

**Output:**

<img src="images/Figure.png">
<p align="center">A Histogram shows different algorithms metric results on different data sizes as well as time consumed to train/predict.</p>
