[toc]

# 摘要

- 近年来,语音情感识别(Speech Emotion Recognition，SER)在人机交互过程中发挥着越来越重要的作用，相关领域的研究也越来越受到重视。SER的主要目的是对语音信号按照不同的情感进行分类，比如“生气”、“恐惧”、“厌恶”、“高兴”等。在过去的几年里，该领域研究者已经提出了许多有效的方法来解决SER中出现的问题，这些方法大部分是集中在一个单一的语音数据库上进行的。大多数的语音情感识别方法主要在单一语音数据库上进行训练和测试。这些训练数据和测试数据具有相同的声学条件，在这种条件下，现有的语音情感识别技术已经实现了较高的性能。然而，在实际应用中训练和测试集可能来自不同的语音情感数据库,用于训练的语料库与测试语料库之间往往存在非常大的差异,传统的SER系统存在跨库识别率较低等问题。为了提高SER系统的跨库识别性能,近年来许多新的跨库技术和方法被提出。本文献综述系统性综述了近年来跨库语音情感识别方法的研究现状与进展,分析了新兴的深度学习方法在跨库语音情感识别中的应用情况。主要从跨库语音情感识别的意义、语音数据库、语音特征、传统机器学习分类器和深度学习方法等对语音情感识别的相关文献进行分析，并对未来的研究方向进行了展望。

## 关键词

- 语音情感识别; 跨库;  语音情感；深度学习; 手工特征; 深度特征;

# 正文

## 跨库语音情感识别的研究背景和意义



情感是人类现实生活中的一个重要因素。人类情感可以被诸如手势,面部表情,身体姿态言语交流。许多物理属性也被用于人类情感识别,例如体温,心率,血压,肌肉活动,皮肤电阻[1]。 人类情感也可以通过语音交流被良好的识别。从语音中识别出人类情感的一个基础问题是从给定的说话者的一段语音信号中识别情感状态。过去的几十年里,从语音中识别情感是研究人员不断增长的研究领域,因为它在许多现实生活中的问题,例如呼叫中心对话自动回复系统,在线辅导,对话系统,疼痛识别,抑郁诊断等。

利用计算机从语音信号中自动识别出说话人的情感状态，是实现自然人机交互界面的关键前提。借助语音情感识别系统，可以提升用户操作计算机的体验。由于跨库语音情感识别涉及多个学科，因此对该领域的研究还有助于促进相关学科的共同进步，具有重要的使用价值和理论价值。跨库语音情感识别系统考虑的条件相比于单数据库训练出来的SER系统更加接近实际生活，具有更好的鲁棒性和实用性，更加适应实际生活的识别要求，跨库技术的研究使SER系统在使用的时候更加有效。

-  Paidi G, Kadiri S, Yegnanarayana B: Analysis of Emotional Speech—A Review, 2016: 205-238.

# 情感语音数据库

## 基于演员型

基于演员的情感语音数据库也称为模拟情感语音数据库。这些类型的数据库是由训练有素的专业演员创建的，如广播艺术家、戏剧艺术家，或者来自一个可以用不同情感说话的人。录音是由说话人在不同的情绪下讲同一文本所做的。记录可能在早上、下午、晚上和晚上等不同的时段进行，以考虑人类的物理言语和表现力的变化。这是完整情感范围内记录情感语音数据库的可靠方式之一。超过60 %的情感语音数据库是模拟数据库。一般来说，模拟的情感数据库比真实的情感数据库更具表现力[  ]。

-  Williams C E, Stevens K N. Emotions and Speech: Some Acoustical Correlates[J]. The Journal of the Acoustical Society of America, 1972, 52(4B): 1238-1250.

## 诱导型

由于诱导情感语音数据库更接近自然数据库，因此也被称为半自然情感数据库。这些类型的数据库是在行为人不知情的情况下，在人工情感情境下录制的。在人工情感情境创设之后，演员与说话人一起参与情感转换。这种类型的数据库比模拟数据库更加自然。但如果说话人知情，录音可能不具有表现力，那么这就是一种人为的情感情境。

## 自然型

自然情感数据库是真实的数据，有时很难识别情感。这类数据库可以从客服与客户的谈话、电视广播、医患对话、法庭、非正常情况下的驾驶舱录音等方面进行记录。在这些情况下，很难找到完整的情感范围,也存在一些版权和安全问题[ ]。

-  El Ayadi M, Kamel M S, Karray F. Survey on speech emotion recognition: Features, classification schemes, and databases[J]. Pattern Recognition, 2011, 44(3): 572-587.



## 常用情感语音数据库

- EMO-DB

这个数据集包含10个德语句子，10个人分别模仿了7种情感，其中7种情感分别是中性的，愤怒的，恐惧的，快乐的，悲伤的，厌恶的和厌烦的，总共采集了536个样本，总共耗时0.38小时。录音是在柏林技术学院技术声部的消音室进行的，取样频率为48 kHz，随后向下取样为16 kHz，每一次取样都用16比特的数字来表示。该数据库被大量的语音情感识别实验和研究所采用,是许多研究的基础。

- SAVEE

该数据集是一组多模式情绪数据集，以英国英语为基础进行录制。这个语料库中一共有480个语音和7种情绪，分别是：中性，愤怒，惊讶，快乐，悲伤，恐惧和厌恶。这些语音的录制是由4名男演员组成的。为了保证高品质的情绪表现，所有的记录都是通过10个不同的评估人员在音频、视觉和视听环境下进行的。这段录音的剧本是从传统的 TIMIT语料库中选出的。

- RAVDESS
- 这是一个多模态的关于情绪语音和歌曲的数据集。这一数据集具有性别平衡性，邀请了24位专业人员，他们用北美语调进行了语音和歌曲的创作。这些情绪语音中包括平静，恐惧，惊讶，快乐，悲伤，愤怒，厌恶。在情感方面，它包括平静，恐惧，惊讶，快乐，悲伤，愤怒，厌恶和恐惧。每一种表情都来自于两个不同的情绪强度，再加上一种中性的表达。

1.  Burkhardt F,Paeschke A,Rolfes M,et al.A database of German emotional speech.Proceedings of the 9th European Conference on Speech Communication and Technology.Lisbon:ISCA,2005.1517-1520.
2. Sugan N,Srinivas NSS,Kar N,et al.Performance comparison of different cepstral features for speech emotion recognition.Proceedings of 2018 International CET Conference on Control,Communication,and Computing(IC4).Thiruvananthapuram:IEEE,2018.266-271.
3. Ali Z,Talha M.Innovative method for unsupervised voice activity detection and classification of audio segments.IEEEAccess,2018,6:15494-15504.
4. Livingstone SR,Russo FA.The ryerson audio-visual database of emotional speech and song(RAVDESS):A dynamic,multimodal set of facial and vocal expressions in North American English.PLoS One,2018,13(5):e0196391.

# 语音情感特征



在跨库语音情感识别中，提取域不变性的情绪特征是非常关键的，域不变性特征domain invariant )的提取将直接影响到跨库语音的情感识别效果。按照所使用的样本，可以分为三种：监督、半监督和无监督。与监督学习不同，无监督学习采用完全没有标记的训练资料进行训练[]，半监督式学习是一种介于二者之间的学习方法，它的训练数据包括有标签的数据和没有标签的数据，一般认为所有的训练数据都来源于同一或类似的分布[].



1. Barlow HB.Unsupervised learning.Neural Computation,1989,1(3):295-311.
2. Zhu XJ,Goldberg AB.Introduction to Semi-supervised Learning.Synthesis Lectures on Artificial Intelligence and Machine Learning.San Rafael:Morgan&amp;Clay pool Publishers.1-84.
3.  Zhu XJ.Semi-supervised learning literature survey.Technical Report,Madison:University of WisconsinMadison,2005.

## 谱特征

当任意一个说话人发出一个语音信号时，它对声道进行过滤和控制。声道特征被用来获取频谱特征，并值得将其表示到频域[ 37 ]。谱特征可以通过傅里叶变换获得。傅里叶变换是将频域变换到时域。最先进的谱特征是梅尔频率倒谱系数( MFCC )。

- Fleischer M, Pinkert S, Mattheus W, et al. Formant frequencies and bandwidths of the vocal tract transfer function are affected by the mechanical impedance of the vocal tract wall[J]. Biomechanics and Modeling in Mechanobiology, 2015, 14(4): 719-733.

## 韵律特征

音高、时长、能量等韵律特征被认为和情绪具有良好的相关[ 44  ]。最大值、最小值、音高相似特征、方差、极差、平均值和标准差等作为良好的韵律信息源，用于识别在片段级别提取的情绪[ 47 ]。

- Lee C M ,  Narayanan S S . Toward detecting emotions in spoken dialogs[J]. IEEE, 2005(2).
- Schröder M. Emotional speech synthesis: a review[M].  2001: 561-564.

## 音质特征

音质特征被定义为个体的语音特征。音质特征是许多语音处理、说话人识别、情感识别等的中心。格式频率、带宽、声门参数、谐波噪声比、抖动和微光等特征被称为音质特征。音质与情感内容之间存在着截然不同的相互关系[ 52 ]。

-  Guidi A ,  Gentili C ,  Scilingo E P , et al. Analysis of speech features and personality traits[J]. Biomedical Signal Processing and Control, 2019, 51(MAY):1-7.

# SER分类器

对于SER来说，分类器与语音特征同等重要。可分为两类：( i )传统机器学习分类器；( ii )深度学习分类器。混合方法，即传统分类器和深度学习分类器的结合也被一些研究者使用。大量的分类器已经被用于SER的研究，但是到目前为止，很难确定哪个分类器表现最好。

## 传统的机器学习方法

传统的用于SER的ML方法是在从语音信号中提取所需特征后应用的。许多分类器已经被研究人员对SER进行了评估，以达到更好的精度。常用的传统分类器有支持向量机、高斯混合模型、隐马尔可夫模型、人工神经网络、k近邻等[ 5 ]。

## 深度学习方法

深度学习概念被描述为机器学习的子集，它从多个层面进行学习。深度学习在2015年前后对语音情感识别影响较大。近年来，深度学习方法为语音情感识别提供了令人鼓舞的结果。与传统的机器学习方法相比，深度学习方法具有功能灵活、特征自动学习范围广、可扩展性强、识别率高等优点，被认为是最适合情感识别的分类器。另一方面，传统的ML方法需要较少的数据进行训练。研究者们使用了多种深度学习方法进行情感识别。表5简要介绍了用于语音情感识别的深度学习方法的主要特点和局限性。目前的研究者正在使用深度学习方法进行情绪识别并提高准确率。使用深度学习方法的语音情感识别模型的主要类别是使用自动学习相关特征的方法、使用手工特征和使用语谱图。总体而言在2013年之后，使用深度学方法进行情绪识别的研究逐渐增多。目前，研究者们提出的SER模型大多采用深度学习方法，并在平均精度和计算成本方面取得了较好的结果。

### 近年来跨库语音情感识别的研究

2014年金赟[]等摘要针对训练样本与测试样本来自不同语音情感数据库造成特征向量空间分布不匹配的问题，采用半监督判别分析减小二者的差异。首先寻找有标签的训练样本和来自另一个库的部分无标签训练样本之间的最优投影方向。基于一致性假设即相近的点更有可能具有相同的类别，利用P近邻图对无标签训练样本相近点之间的关系进行建模，从而获得无标签样本的分布信息。在保证无标签样本间流形结构的同时，使所有训练样本类间散度和类内散度的比值达到最大，从而得到最优的投影方向。采用两组实验进行验证，第1组用eNTERFACE库训练去测试Berlin库，识别率为51.41%，第2组用Berlin库训练测试eNTERFACE库，识别率为45.76%，相比未采用半监督判别分析的识别结果分别有了13.72%和22.81%的提高，说明该算法的有效性。通过实验前后数据的可视化分析，说明利用半监督判别分析确实减小了不同库之间特征向量空间分布的不匹配问题，从而提高跨库语音情感识别率。

2016年,张昕然[]针对跨库语音情感特征优选分类，提出了带有无限成分数的t分布混合模型（iSMM）。它可以直接对多种语音情感样本进行有效的识别。与传统的高斯混合模型（GMM）相比，基于混合t分布的语音情感模型能有效处理样本特征空间中存在异常值的问题。首先，t分布混合模型对用于测试的非典型情感数据能够保持鲁棒性。其次，针对高维空间引起的数据高复杂度和训练样本不足的问题，本文将全局隐空间加入情感模型。这种方法使样本空间被划分的成分数量为无限，形成一个iSMM情感模型。此外，该模型可以自动确定最佳的成分数量，同时满足低复杂性，进而完成多种情感特征数据的分类。为验证所提出的iSMM模型对手不同情感特征分布空间的识别效果，本文在3个数据库上进行仿真实验，分别是：表演型语料库DES、EMO-DB和自发型语料库FAU。它们都是通用的语音情感数据库，且具有高维特征样本和不同的空间分布。在这种实验条件下，验证了各个模型对手特征异常值和高维数据的优选效果以及模型本身的泛化性。结果显示iSMM相比其它对比模型，保持了更稳定的识别性能。因此说明本文提出的基于无限1分布的情感模型，在处理不同来源的语音数据时具有较好的鲁棒性，且对带有离群值的高维情感特征具有良好的优选识别能力。

结合K近邻、核学习方法、特征线重心法和LDA算法，提出了用于情感识别的LDA+kernel-KNNFLC方法。首先针对过大的先验样本特征数月造成的计算量庞大问题，采用重心准则学习样本距离，改进了核学习的K近邻方法；然后加入LDA对情感特征向量优化，在避免维度冗余的情况下，更好的保证了类间情感信息识别的稳定性。对于跨库领域的研究，关注了独立数据库中不同类别间边界拟合度过高导致的识别性能差异：通过对特征空间再学习，所提出的分类方法优化了情感特征向量的类间区分度，适合于不同语料来源的情感特征分类。在包含高维全局统计特征的两个语音情感数据库上进行了仿真实验。通过降维方案、情感分类器和维度参数进行多组实验对比分析，结果表明：LDA+kernel-KNNFLC方法在同条件下识别性能有显著提升，具有相对稳定的情感类别间分类能力。

针对跨库条件下情感特征类别的改进（扩充）研究，提出了基于听觉注意模型的语谱图特征提取方法。模型模拟人耳听觉特性，能有效探测语谱图上变化的情感特征。同时，利用时频原子对模型进行改进，取得频率特性信号匹配的优势，从时域上提取情感信息。在语音情感识别技术中，由于噪声环境、说话方式和说话人特质等原因，会造成特征空间分布不匹配的情况。从语音学上分析，该问题多存在于跨数据库情感识别任务中。训练的声学模型和用于测试的语句样本之间的错位，会使语音情感识别性能急剧下降。语谱图的特征能从图像的角度对现有情感特征进行有效的补充。听觉注意机制使模型能提取跨语音数据库中的显著性特征，提高语音情感识别系统的情感辨识能力。仿真实验部分利用文章所提出的方法在跨库情感样本上进行特征提取，再通过典型的分类器进行识别。结果显示：与国际通用的标准方法相比，语谱图情感特征的识别性能提高了约9个百分点，从而验证了该方法对不同数据库具有更好的鲁棒性。

利用深度学习领域的深度信念模型，提出了基于深度信念网络的特征层融合方法。将语音频谱图中隐含的情感信息作为图像特征，与传统声学情感特征融合。研究解决了跨数据库语音情感识别中，将不同尺度上提取的情感特征相结合的技术难点。利用STBⅱ模型对语谱图进行分析，从颜色、亮度、方向三个角度出发提取语谱图特征；然后研究改进了DBN网络模型，并利用其对传统声学特征与语谱图特征进行了特征层融合，扩充了特征子集的尺度，提升了情感表征能力。通过在ABC数据库和多个中文数据库上的实验验证，特征融合后的新特征子集相比传统的语音情感特征，其跨数据库识别性能获得了明显提升。

2017年,张昕然[]等利用深度学习领域的深度信念模型，提出了基于深度信念网络的特征层融合方法。将语音频谱图中隐含的情感信息作为图像特征，与传统情感特征融合。研究解决了跨数据库语音情感识别中，将不同尺度上提取的情感特征进行融合的技术难点。利用 STB /Itti 模型对语谱图进行分析，从颜色、亮度、方向三个角度出发，提取了新的语谱图特征; 然后研究改进的 DBN 网络模型并对传统声学特征与新提取的语谱图特征进行了特征层融合，增强了特征子集的尺度，提升了情感表征能力。通过在 ABC 数据库和多个中文数据库上的实验验证，特征融合后的新特征子集相比传统的语音情感特征，其跨数据库识别结果获得了明显提升。

2019年陈颖[]针对特征映射迁移学习方法忽略特有信息的缺陷，本文工作提出了融合共性与特性的双子空间迁移学习框架，对仅利用共性的特征映射迁移学习进行改进，以提高情感识别性能。

钟琪[]在IEMOCAP英语情感数据库、CASIA汉语情感数据库、EMO-BD德语情感数据库，以中性、生气、快乐、悲伤四种情感为研究对象，了解在单语言语料库、混合语言语料库、跨语料库的语音情感识别情况。使用支持向量机（SupportVector Machine,SVM)、卷积神经网络(Convolutional Neural Networks,CNN)和长短时记忆网络（Long-Short TermMemory,LSTM)为分类器进行训练，对情感进行识别。从实验结果可以看出，不同语料库的语音情感的识别模式存在相似性，也存在相似的语言情感特性。还发现英文的中性情感和中文的悲伤情感具有良好的模型泛化性，英文的悲伤情感和中文的中性情感有较好的适应性，

2020年,陈秀珍[]提出了一种目标适应的子空间学习模型跨库语音情感识别的方法。该方法通过寻找一个投影子空间将语音特征投影到标签空间，以此建立源域与目标域之间的关系，并更有效的减少其特征分布的差异。为获得更有效的投影矩阵，将l1 与l2,1 范数作为此模型的正则项。最后，提取INTERSPEECH情感挑战赛中的IS09与IS10特征集，并在三个公开数据库（EmoDB、eNTERFACE与AFEW4.0）对此模型进行实验验证，同时与现有的跨库语音情感识别方法进行对比，结果表明此方法有效且IS09特征集优于IS10特征集。

刘雨柔[]采用能够较好地处理非线性非平稳信号的变分模态算法分解情感语音信号，将不同频率进行合成再通过伽马通滤波器，求取对数，离散余弦变换之后计算统计参数得到新的情感语音谱特征；考虑到单一特征无法全面表征情感信息，选取了表达语音基本特性的韵律特征、从混沌角度描述语音信息的非线性特征以及本文提出的新的谱特征进行特征级融合，得到全局特征。并且提出一种复合网络栈式稀疏自编码网络——核函数极限学习机，首先通过栈式稀疏自编码网络对原始特征进行无监督预训练，然后结合数据标签利用反向传播算法有监督微调，重构得到更符合大脑稀疏性且更具有区分情感信息的深度特征，最后采用人工蜂群优化的核函数极限学习机对情感进行识别分类 .

2021年汪洋[]等提出一种基于决策边界优化域自适应（decision boundary optimized domain adaptation, DBODA）的跨库语音情感识别方法。首先利用卷积神经网络进行特征处理，随后将特征送入最大化核范数及均值差异（maximum n-norm and mean discrepancy, MNMD）模块，在减小域间差异的同时，最大化目标域情感预测概率矩阵的核范数，以提升目标域样本的鉴别性，优化决策边界。在以Berlin, eNTERFACE, CASIA语音库为基准库设立的六组跨库实验中，所提方法的平均识别精度领先于其他算法1.28%~11.01%，说明模型有效降低了决策边界的样本密度，提升了预测的准确性。 

钟颖[]采用了Focal 损失函数，该函数可以使网络模型更加关注难样本的学习，从而减少对简单样本的关注。通过结合注意力机制，本文提出的网络可以增加模型对情感显著部分信息的获取。该模型在单一语料库IEMOCAP 和EmoDB 上分别达到了71.72%和90.1%的未加权精度UA。与目前已知参数量最少的模型相比，该模型的参数量降低了5 倍;为了提高跨语料库的语音情感识别模型泛化能力，采用增量学习对模型进行训练,所使用的办法在IEMOCAP、MSP-IMPROV、MES-P 三个数据库的跨语料库的语音情感识别上取得了很大的提高。 

李晓坤[]等在对现有语音情感识别方法进行研究的基础上，提出一个深度迁移网络——基于注意力机制的长短时 动 态 对 抗 适 配网络（Attention-based LSTM Dynamic Adversarial Adaptation Networks，LSTM-TF-at-DAAN）进行跨库语音情感识别。在语音情感识别领域广泛使用的 eNTERFACE 语音库和 EMO-DB语音库上进行实验，并将实验结果与采用一般迁移学习方法的实验结果进行对比，发现 LSTM-TF-at-DAAN 提升了 5.37% 的识别准确率，为深度迁移学习应用于跨库语音情感识别提供了可行性证明。

郑婉璐[]提出一种新颖的全局局部尺度对抗网络。为了获得更强判别性和泛化性的语音情感特征，网络从语音信号的时序特征进行情感信息建模。针对语音信号的时序性，提出了一种在局部、全局、融合尺度上新颖的语音情感特征的提取方法，该方法融合了手工语音特征、深度语音特征等多种尺度特征的互补优势，能够更好地同时表征不同数据库中的语音信号。同时，还提出一种基于注意力的时序信息建模网络，筛选出与情感相关的语音帧或段，以获得判别性强的语音情感表征。此外，为了消除源域数据库和目标数据库的语音特征分布差异，提出了一种分层级的差异对抗网络，在局部、全局、融合尺度特征层面上协同地消除域间差异，获得泛化性更强的语音情感特征

# 参考文献

1. P. Gangamohan, S.R. Kadiri, B. Yegnanarayana, Analysis of emotional speech-a review, Toward Robotic Socially Believable Behaving Systems-Volume I, (2016) 205-238. 



